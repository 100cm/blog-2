<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=gbk">
<title>PostgreSQL research</title>
<style type="text/css">
.blogcnt{line-height:160%;font-size:14px;text-align:left;word-wrap:break-word;}
.blogcnt *{line-height:160%;}
.blogcnt p{margin:0 0 10px;}
.blogcnt ul,.nbw-blog ol{margin:5px 0 5px 40px;padding:0}
.blogcnt em{font-style:italic;}
.blogcnt blockquote{font-size:1em;margin:auto 0 auto 35px;}
.blogcnt img{border:0;max-width:100%;}
</style>
</head>
<body style="color:#444444;">
<h1 id="blog-Title"><a href="index.html">PostgreSQL research</a></h1>
<div id="" style="padding:0 20px;">
	<h2 id="">ceph - remove A monitor (MANUAL)</h2>
	<h5 id="">2014-12-10 11:58:28&nbsp;&nbsp;&nbsp;<a href="http://blog.163.com/digoal@126/blog/static/16387704020141110112112555/" target="_blank">查看原文&gt;&gt;</a></h5>
	<div class="" id="" style="padding:0 20px;">
		<div class="blogcnt" style="width:800px;"><div>在删除mon节点前, 请务必确保删除后的mon节点可以达到健康状态, 例如从5个节点删除到4个节点, 并且4个节点里面有3个或以上mon是健康的, 这样的状态ceph storage cluster才是健康的.</div><div>删除mon节点分两种情况, 1种情况是删除一个健康的mon节点, 另一种情况是删除一个不健康的mon节点.</div><div><br></div><div>首选来看看如何删除一个健康的mon节点 :&nbsp;</div><div>连接到该mon节点, 关闭服务, 或(停止ceph_mon服务)</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon5 ~]# ps -ewf|grep mon</font></div><div><font size="2"   >root &nbsp; &nbsp; &nbsp; &nbsp; 127 &nbsp; &nbsp; &nbsp; 1 &nbsp;0 Dec09 pts/0 &nbsp; &nbsp;00:00:24 ceph-mon -i mon5 --public-addr 172.17.0.10 --mon-data /data01/ceph/mon/ceph-5</font></div><div><font size="2"   >root &nbsp; &nbsp; &nbsp; &nbsp; 202 &nbsp; &nbsp; &nbsp;28 &nbsp;0 11:24 pts/0 &nbsp; &nbsp;00:00:00 grep --color=auto mon</font></div><div><font size="2"   >[root@mon5 ~]# kill 127</font></div><div><font size="2"   >[root@mon5 ~]# ps -ewf|grep mon</font></div><div><font size="2"   >root &nbsp; &nbsp; &nbsp; &nbsp; 204 &nbsp; &nbsp; &nbsp;28 &nbsp;0 11:24 pts/0 &nbsp; &nbsp;00:00:00 grep --color=auto mon</font></div><p></p></pre></div><div><br></div><div>关闭ceph-mon进程后</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon5 ~]# ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_WARN 1 mons down, quorum 0,1,2,3 mon1,mon2,mon3,mon4</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e3: 5 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0,mon4=172.17.0.9:6789/0,mon5=172.17.0.10:6789/0}, election epoch 16, quorum 0,1,2,3 mon1,mon2,mon3,mon4</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e29: 4 osds: 4 up, 4 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v2384: 128 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 41210 MB used, 1596 GB / 1636 GB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;128 active+clean</font></div><p></p></pre></div><div><br></div><div>使用命令删除节点</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >[root@mon5 ~]# ceph mon remove mon5</font></div><div><font size="2"   >removed mon.mon5 at 172.17.0.10:6789/0, there are now 4 monitors</font></div></div><div><font size="2"   ><br></font></div><div><div><font size="2"   >[root@mon5 ~]# ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_OK</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e4: 4 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0,mon4=172.17.0.9:6789/0}, election epoch 20, quorum 0,1,2,3 mon1,mon2,mon3,mon4</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e29: 4 osds: 4 up, 4 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v2384: 128 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 41210 MB used, 1596 GB / 1636 GB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;128 active+clean</font></div></div><p></p></pre></div><div><br></div><div><span style="line-height: 28px;"   >建议修改所有mon节点的配置文件, 把删掉的节点从mon&nbsp;</span><span style="line-height: 28px;"   >members</span><span style="line-height: 28px;"   >&nbsp;配置和host配置去除 .</span></div><div><span style="line-height: 28px;"   >(修改ceph.conf只是为了好看, 其实mon节点之间是通过monmap来发现其他mon节点的, 所以配置文件不是必须的)</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >/etc/ceph/ceph.conf&nbsp;</font></div><div><font size="2"   ><br></font></div><div><div><font size="2"   >mon initial members = mon1, mon2, mon3, mon4</font></div><div><font size="2"   >mon host = 172.17.0.2, 172.17.0.3, 172.17.0.4, 172.17.0.9</font></div></div><p></p></pre></div><div><br></div><div>接下来让ceph storage cluster不健康, 关掉2个节点, 剩余2个节点.</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >[root@localhost ceph]# ssh 172.17.0.9</font></div><div><font size="2"   >root@172.17.0.9's password:&nbsp;</font></div><div><font size="2"   >Last login: Tue Dec &nbsp;9 17:26:03 2014 from 172.17.42.1</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@mon4 ~]# ps -ewf|grep mon</font></div><div><font size="2"   >root &nbsp; &nbsp; &nbsp; &nbsp; 223 &nbsp; &nbsp; &nbsp; 1 &nbsp;0 Dec09 pts/0 &nbsp; &nbsp;00:00:24 ceph-mon -i mon4 --public-addr 172.17.0.9 --mon-data /data01/ceph/mon/ceph-4</font></div><div><font size="2"   >root &nbsp; &nbsp; &nbsp; &nbsp; 278 &nbsp; &nbsp; 261 &nbsp;0 11:29 pts/1 &nbsp; &nbsp;00:00:00 grep --color=auto mon</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@mon4 ~]# kill 223</font></div></div><div><font size="2"   ><br></font></div><div><div><span style="line-height: 28px;"   ><font size="2"   >[root@mon4 ~]# ceph -s</font></span></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_WARN 1 mons down, quorum 0,1,2 mon1,mon2,mon3</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e4: 4 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0,mon4=172.17.0.9:6789/0}, election epoch 22, quorum 0,1,2 mon1,mon2,mon3</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e29: 4 osds: 4 up, 4 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v2392: 128 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 41219 MB used, 1596 GB / 1636 GB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;128 active+clean</font></div><div><font size="2"   ><br></font></div><div><div style="line-height: 28px;"   ><font size="2"   >[root@localhost ceph]# ssh 172.17.0.4</font></div><div style="line-height: 28px;"   ><font size="2"   >root@172.17.0.4's password:&nbsp;</font></div><div style="line-height: 28px;"   ><font size="2"   >Last login: Tue Dec &nbsp;9 17:26:32 2014 from 172.17.42.1</font></div><div style="line-height: 28px;"   ><font size="2"   ><br></font></div><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><font size="2"   >[root@mon3 ~]# service ceph -a stop mon.mon3</font></div><div style="line-height: 28px;"   ><font size="2"   >=== mon.mon3 ===&nbsp;</font></div><div style="line-height: 28px;"   ><font size="2"   >Stopping Ceph mon.mon3 on mon3...kill 903...done</font></div><div style="line-height: 28px;"   ><font size="2"   ><br></font></div><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><font size="2"   >[root@mon3 ~]# ceph -s</font></div><div style="line-height: 28px;"   ><font size="2"   >2014-12-10 11:33:47.004145 7f24641a0700 &nbsp;0 -- :/1001200 &gt;&gt; 172.17.0.4:6789/0 pipe(0x7f2460023190 sd=3 :0 s=1 pgs=0 cs=0 l=1 c=0x7f2460023420).fault</font></div><div style="line-height: 28px;"   ><font size="2"   >^CError connecting to cluster: InterruptedOrTimeoutError</font></div></div></div></div></div><p></p></pre></div><div><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><span style="line-height: 28px;"   >因为集群中有4个mon节点, 已经有2个被关闭了, 所以现在ceph 集群是不健康的状态, 为了达到健康, 我们可以启动这两个节点, 或从集群中把这两个节点删掉. 让集群只有2个mon节点.</span></div><div style="line-height: 28px;"   ><span style="line-height: 28px;"   ><br></span></div><div style="line-height: 28px;"   ><span style="line-height: 28px;"   >接下来要从因为mon节点选举数不够导致集群不健康的情况, 删除mon节点 :&nbsp;</span></div><div style="line-height: 28px;"   >(因为集群已经不健康了, 所以不能直接操作来删除, 必须通过dump和inject来删除不健康节点, 使ceph集群的mon选举数足够多来恢复集群健康)</div><div style="line-height: 28px;"   ><span style="line-height: 28px;"   >首选要确认剩余的健康的mon节点是哪些? 本例是mon1, mon2.</span></div><div style="line-height: 28px;"   ><span style="line-height: 28px;"   >mon3, mon4都关闭了, 并且假设不可恢复, 或者不需要他们了.</span></div><div style="line-height: 28px;"   >首先要关闭所有的要留下的mon, 本例为mon1, mon2</div><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><pre class="prettyprint"   ><p></p><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><font size="2"   >[root@mon1 tmp]# ps -efw|grep mon</font></div><div style="line-height: 28px;"   ><font size="2"   >root &nbsp; &nbsp; &nbsp; &nbsp; 560 &nbsp; &nbsp; &nbsp; 1 &nbsp;0 Dec09 ? &nbsp; &nbsp; &nbsp; &nbsp;00:01:22 /usr/bin/ceph-mon -i mon1 --pid-file /var/run/ceph/mon.mon1.pid -c /etc/ceph/ceph.conf --cluster ceph</font></div><div style="line-height: 28px;"   ><font size="2"   >root &nbsp; &nbsp; &nbsp; &nbsp;1302 &nbsp; &nbsp;1228 &nbsp;0 11:38 pts/1 &nbsp; &nbsp;00:00:00 grep --color=auto mon</font></div><div style="line-height: 28px;"   ><font size="2"   >[root@mon1 tmp]# kill 560</font></div><div style="line-height: 28px;"   ><font size="2"   ><br style="line-height: 28px;"   ></font></div></div><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><font size="2"   >[root@mon2 ~]# ps -ewf|grep mon</font></div><div style="line-height: 28px;"   ><font size="2"   >root &nbsp; &nbsp; &nbsp; &nbsp; 220 &nbsp; &nbsp; &nbsp; 1 &nbsp;0 Dec09 ? &nbsp; &nbsp; &nbsp; &nbsp;00:00:28 /usr/bin/ceph-mon -i mon2 --pid-file /var/run/ceph/mon.mon2.pid -c /etc/ceph/ceph.conf --cluster ceph</font></div><div style="line-height: 28px;"   ><font size="2"   >root &nbsp; &nbsp; &nbsp; &nbsp; 684 &nbsp; &nbsp; 665 &nbsp;0 11:39 pts/0 &nbsp; &nbsp;00:00:00 grep --color=auto mon</font></div><div style="line-height: 28px;"   ><font size="2"   >[root@mon2 ~]# kill 220</font></div></div><p></p></pre></div></div></div><div style="line-height: 28px;"   ><span style="line-height: 28px;"   >然后连接到mon节点, dump monmap, 并从dump出来的monmap中删除不需要的节点(mon3, mon4)</span></div><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><pre class="prettyprint"   ><p></p><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><span style="line-height: 28px;"   ><font size="2"   >[root@mon2 ~]# ceph-mon -i mon2 --extract-monmap /tmp/monmap</font></span></div></div><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><font size="2"   >2014-12-10 11:39:39.775643 7ff70b6af880 -1 wrote monmap to /tmp/monmap</font></div><div style="line-height: 28px;"   ><font size="2"   ><br></font></div><div style="line-height: 28px;"   ><font size="2"   >[root@mon2 ~]# monmaptool /tmp/monmap --rm mon3</font></div><div style="line-height: 28px;"   ><font size="2"   >monmaptool: monmap file /tmp/monmap</font></div><div style="line-height: 28px;"   ><font size="2"   >monmaptool: removing mon3</font></div><div style="line-height: 28px;"   ><font size="2"   >monmaptool: writing epoch 4 to /tmp/monmap (3 monitors)</font></div><div style="line-height: 28px;"   ><font size="2"   ><br></font></div><div style="line-height: 28px;"   ><font size="2"   >[root@mon2 ~]# monmaptool /tmp/monmap --rm mon4</font></div><div style="line-height: 28px;"   ><font size="2"   >monmaptool: monmap file /tmp/monmap</font></div><div style="line-height: 28px;"   ><font size="2"   >monmaptool: removing mon4</font></div><div style="line-height: 28px;"   ><font size="2"   >monmaptool: writing epoch 4 to /tmp/monmap (2 monitors)</font></div></div><p></p></pre></div></div></div></div></div><div><br></div><div>使用修改后的monmap导入当前mon节点的monmap数据中.</div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon2 ~]# ceph-mon -i mon2 --inject-monmap /tmp/monmap</font></div><div></div><p></p></pre><div><span style="line-height: 28px;"   >启动</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon2 ~]# /usr/bin/ceph-mon -i mon2 --pid-file /var/run/ceph/mon.mon2.pid -c /etc/ceph/ceph.conf --cluster ceph</font></div><div></div><p></p></pre></div><div>另一个节点也做同样的操作 :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# ceph-mon -i mon1 --extract-monmap /tmp/monmap</font></div><div><font size="2"   >2014-12-10 11:43:16.959514 7fe2231b5880 -1 wrote monmap to /tmp/monmap</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@mon1 ~]# monmaptool /tmp/monmap --rm mon3</font></div><div><font size="2"   >monmaptool: monmap file /tmp/monmap</font></div><div><font size="2"   >monmaptool: removing mon3</font></div><div><font size="2"   >monmaptool: writing epoch 4 to /tmp/monmap (3 monitors)</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@mon1 ~]# monmaptool /tmp/monmap --rm mon4</font></div><div><font size="2"   >monmaptool: monmap file /tmp/monmap</font></div><div><font size="2"   >monmaptool: removing mon4</font></div><div><font size="2"   >monmaptool: writing epoch 4 to /tmp/monmap (2 monitors)</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@mon1 ~]# ceph-mon -i mon1 --inject-monmap /tmp/monmap</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@mon1 ~]# /usr/bin/ceph-mon -i mon1 --pid-file /var/run/ceph/mon.mon1.pid -c /etc/ceph/ceph.conf --cluster ceph</font></div><p></p></pre></div><div><span style="line-height: 28px;"   >现在的ceph storage cluster只有2个mon节点, 而且都up了, 所以集群健康.</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_OK</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e5: 2 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0}, election epoch 26, quorum 0,1 mon1,mon2</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e29: 4 osds: 4 up, 4 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v2395: 128 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 41223 MB used, 1596 GB / 1636 GB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;128 active+clean</font></div><p></p></pre></div><div>如果被删除的节点以后都不需要加进来了, 建议修改一下当前剩余节点mon1, mon2的配置文件, 把他们去除.</div><div>(修改ceph.conf只是为了好看, 其实mon节点之间是通过monmap来发现其他mon节点的, 所以配置文件不是必须的)</div><div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >/etc/ceph/ceph.conf</font></div><div><font size="2"   >......</font></div><div><font size="2"   >mon initial members = mon1, mon2</font></div><div><font size="2"   >mon host = 172.17.0.2, 172.17.0.3</font></div><p></p></pre></div><div><br></div><div>删除mon的操作讲完了, 接下来可以用上一篇讲的方法把mon3, mon4, mon5加入进来.</div></div><div>因为mon3,4,5的信息都在, 所以加进来很简单.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# ceph mon add mon3 172.17.0.4</font></div><div><font size="2"   >port defaulted to 6789; added mon.mon3 at 172.17.0.4:6789/0</font></div><div><font size="2"   >[root@mon1 ~]# ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_WARN 1 mons down, quorum 0,1 mon1,mon2</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e6: 3 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0}, election epoch 28, quorum 0,1 mon1,mon2</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e29: 4 osds: 4 up, 4 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v2410: 128 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 41234 MB used, 1596 GB / 1636 GB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;128 active+clean</font></div><div><font size="2"   >[root@mon1 ~]# ssh 172.17.0.4</font></div><div><font size="2"   >root@172.17.0.4's password:&nbsp;</font></div><div><font size="2"   >Last login: Wed Dec 10 11:31:54 2014 from 172.17.42.1</font></div><div><font size="2"   >[root@mon3 ~]# service ceph start mon.mon3</font></div><div><font size="2"   >=== mon.mon3 ===&nbsp;</font></div><div><font size="2"   >Starting Ceph mon.mon3 on mon3...</font></div><div><font size="2"   >Starting ceph-create-keys on mon3...</font></div><div><font size="2"   >[root@mon3 ~]# ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_OK</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e6: 3 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0}, election epoch 30, quorum 0,1,2 mon1,mon2,mon3</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e29: 4 osds: 4 up, 4 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v2411: 128 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 41234 MB used, 1596 GB / 1636 GB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;128 active+clean</font></div><p></p></pre></div><div>但是务必保证加进来后即使未启动集群也是健康的, 所以有3个节点健康后, 可以一次加2个进来.</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >[root@mon3 ~]# ceph mon add mon4 172.17.0.9</font></div><div><font size="2"   >port defaulted to 6789; added mon.mon4 at 172.17.0.9:6789/0</font></div><div><font size="2"   >[root@mon3 ~]# ceph mon add mon5 172.17.0.10</font></div><div><font size="2"   >port defaulted to 6789; added mon.mon5 at 172.17.0.10:6789/0</font></div></div><div><font size="2"   >[root@mon4 ~]# /usr/bin/ceph-mon -i mon4 --pid-file /var/run/ceph/mon.mon4.pid -c /etc/ceph/ceph.conf --cluster ceph --mon-data /data01/ceph/mon/ceph-4</font></div><div><div><font size="2"   >[root@mon4 ~]# ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_WARN 1 mons down, quorum 0,1,2,3 mon1,mon2,mon3,mon4</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e8: 5 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0,mon4=172.17.0.9:6789/0,mon5=172.17.0.10:6789/0}, election epoch 36, quorum 0,1,2,3 mon1,mon2,mon3,mon4</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e29: 4 osds: 4 up, 4 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v2417: 128 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 41241 MB used, 1596 GB / 1636 GB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;128 active+clean</font></div></div><div><font size="2"   ><br></font></div><div><div><font size="2"   >[root@mon5 ~]# /usr/bin/ceph-mon -i mon5 --pid-file /var/run/ceph/mon.mon5.pid -c /etc/ceph/ceph.conf --cluster ceph --mon-data /data01/ceph/mon/ceph-5</font></div><div><font size="2"   >[root@mon5 ~]# ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_OK</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e8: 5 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0,mon4=172.17.0.9:6789/0,mon5=172.17.0.10:6789/0}, election epoch 38, quorum 0,1,2,3,4 mon1,mon2,mon3,mon4,mon5</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e29: 4 osds: 4 up, 4 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v2418: 128 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 41243 MB used, 1596 GB / 1636 GB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;128 active+clean</font></div></div><p></p></pre></div><div><br></div>[参考]<wbr><div>1.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="http://docs.ceph.com/docs/master/rados/operations/add-or-rm-mons/"   >http://docs.ceph.com/docs/master/rados/operations/add-or-rm-mons/</a></div><div>2.&nbsp;<a style="line-height: 28px;" target="_blank" href="http://blog.163.com/digoal@126/blog/static/163877040201411952056378/"   >http://blog.163.com/digoal@126/blog/static/163877040201411952056378/</a></div><div><br></div>
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"   ><img title="ceph - remove A monitor (MANUAL) - 德哥@Digoal - PostgreSQL research"   src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"   alt="Flag Counter"   border="0"   ></a></div>
	</div>
</div>
</body>
</html>