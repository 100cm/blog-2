<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=gbk">
<title>PostgreSQL research</title>
<style type="text/css">
.blogcnt{line-height:160%;font-size:14px;text-align:left;word-wrap:break-word;}
.blogcnt *{line-height:160%;}
.blogcnt p{margin:0 0 10px;}
.blogcnt ul,.nbw-blog ol{margin:5px 0 5px 40px;padding:0}
.blogcnt em{font-style:italic;}
.blogcnt blockquote{font-size:1em;margin:auto 0 auto 35px;}
.blogcnt img{border:0;max-width:100%;}
</style>
</head>
<body style="color:#444444;">
<h1 id="blog-Title"><a href="index.html">PostgreSQL research</a></h1>
<div id="" style="padding:0 20px;">
	<h2 id="">ceph install in CentOS 7 x64 within docker - 3</h2>
	<h5 id="">2014-12-01 16:10:51&nbsp;&nbsp;&nbsp;<a href="http://blog.163.com/digoal@126/blog/static/163877040201410274232276/" target="_blank">查看原文&gt;&gt;</a></h5>
	<div class="" id="" style="padding:0 20px;">
		<div class="blogcnt" style="width:800px;"><div><div>部署ceph cluster的第一步是部署monitor节点, 本例我们使用3台monitor. 并且monitor之间使用cephx认证.</div><div><br></div><div>INSTALL CEPH STORAGE CLUSTER</div><div>1. 安装yum-plugin-priorities &nbsp;(所有节点)</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@deploy yum.repos.d]# su - ceph</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.1&nbsp;</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sudo yum install -y yum-plugin-priorities</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.2&nbsp;</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sudo yum install -y yum-plugin-priorities</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.3&nbsp;</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sudo yum install -y yum-plugin-priorities</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.4&nbsp;</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sudo yum install -y yum-plugin-priorities</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.5&nbsp;</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sudo yum install -y yum-plugin-priorities</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.6&nbsp;</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sudo yum install -y yum-plugin-priorities</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.7&nbsp;</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sudo yum install -y yum-plugin-priorities</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.8&nbsp;</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sudo yum install -y yum-plugin-priorities</font></div><p></p></pre></div><div><br></div><div>确保 :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.1 sudo cat /etc/yum/pluginconf.d/priorities.conf</font></div><div><font size="2"   >[main]</font></div><div><font size="2"   >enabled = 1</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.2 sudo cat /etc/yum/pluginconf.d/priorities.conf</font></div><div><font size="2"   >[main]</font></div><div><font size="2"   >enabled = 1</font></div><div><font size="2"   >.......................</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.8 sudo cat /etc/yum/pluginconf.d/priorities.conf</font></div><div><font size="2"   >[main]</font></div><div><font size="2"   >enabled = 1</font></div><p></p></pre></div><div><br></div><div><span style="line-height: 28px;"   >2. 安装依赖包(所有节点)</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.1 sudo yum install -y snappy leveldb gdisk python-argparse gperftools-libs</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.2 sudo yum install -y snappy leveldb gdisk python-argparse gperftools-libs</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.3 sudo yum install -y snappy leveldb gdisk python-argparse gperftools-libs</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.4 sudo yum install -y snappy leveldb gdisk python-argparse gperftools-libs</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.5 sudo yum install -y snappy leveldb gdisk python-argparse gperftools-libs</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.6 sudo yum install -y snappy leveldb gdisk python-argparse gperftools-libs</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.7 sudo yum install -y snappy leveldb gdisk python-argparse gperftools-libs</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.8 sudo yum install -y snappy leveldb gdisk python-argparse gperftools-libs</font></div><p></p></pre></div><div><br></div><div>3. 安装ceph包(所有节点)</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.1 sudo yum install -y ceph</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.2 sudo yum install -y ceph</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.3 sudo yum install -y ceph</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.4 sudo yum install -y ceph</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.5 sudo yum install -y ceph</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.6 sudo yum install -y ceph</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.7 sudo yum install -y ceph</font></div><div><font size="2"   >[ceph@deploy ~]$ ssh 172.17.0.8 sudo yum install -y ceph</font></div><p></p></pre></div><div><br></div><div>4. 部署监控节点, (所有mon1, mon2, mon3节点执行)</div><div>创建配置文件目录</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ><span style="line-height: 21px;"   ># 一般安装完ceph rpm已经有这两个目录了.</span></font></div><div><font size="2"   >sudo mkdir -p /var/lib/ceph/mon</font></div><div><font size="2"   >sudo mkdir -p /etc/ceph</font></div><p></p></pre></div><div><br></div><div>4.1 使用uuidgen命令生成fsid, 用来唯一标识一个cluster : (mon1执行)</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Unique Identifier: The fsid is a unique identifier for the cluster, and stands for File System ID from the days when the Ceph Storage Cluster was principally for the Ceph Filesystem.&nbsp;</font></div><div><font size="2"   >Ceph now supports native interfaces, block devices, and object storage gateway interfaces too, so fsid is a bit of a misnomer.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[ceph@mon1 ~]$ uuidgen</font></div><div><font size="2"   >f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >对应配置参数</font></div><div><font size="2"   >fsid = f649b128-963c-4802-ae17-5a76f36c4c76</font></div><p></p></pre></div><div><br></div><div>4.2 选择合适的集群名, 例如, 集群名为ceph, 则配置文件名为ceph.conf :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Cluster Name: Ceph clusters have a cluster name, which is a simple string without spaces. The default cluster name is ceph, but you may specify a different cluster name. Overriding the default cluster name is especially useful when you are working with multiple clusters and you need to clearly understand which cluster your are working with.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >For example, when you run multiple clusters in a federated architecture, the cluster name (e.g., us-west, us-east) identifies the cluster for the current CLI session. Note: To identify the cluster name on the command line interface, specify the a Ceph configuration file with the cluster name (e.g., ceph.conf, us-west.conf, us-east.conf, etc.). Also see CLI usage (ceph --cluster {cluster-name}).</font></div><p></p></pre></div><div><br></div><div>4.3 确定监控节点名(`hostname -s`) :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Monitor Name: Each monitor instance within a cluster has a unique name. In common practice, the Ceph Monitor name is the host name (we recommend one Ceph Monitor per host, and no commingling of Ceph OSD Daemons with Ceph Monitors). You may retrieve the short hostname with hostname -s.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >本例监控节点名 mon1, mon2, mon3</font></div><div><font size="2"   >对应IP 172.17.0.2, 172.17.0.3, 172.17.0.4</font></div><div><font size="2"   >对应配置参数 :&nbsp;</font></div><div><font size="2"   >mon initial members = mon1, mon2, mon3</font></div><div><font size="2"   >mon host = 172.17.0.2, 172.17.0.3, 172.17.0.4</font></div><p></p></pre></div><div><br></div><div>4.4 创建监控和管理keyring.<span style="line-height: 28px;"   >&nbsp;</span><span style="line-height: 28px;"   >(mon1执行)</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Monitor Keyring: Monitors communicate with each other via a secret key. You must generate a keyring with a monitor secret and provide it when bootstrapping the initial monitor(s).</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Create a keyring for your cluster and generate a monitor secret key.</font></div><p></p></pre></div><div>操作如下</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[ceph@mon1 ~]$ sudo ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'</font></div><div><font size="2"   >creating /tmp/ceph.mon.keyring</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Administrator Keyring: To use the ceph CLI tools, you must have a client.admin user. So you must generate the admin user and keyring, and you must also add the client.admin user to the monitor keyring.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Generate an administrator keyring, generate a client.admin user and add the user to the keyring.</font></div><p></p></pre></div><div>操作如下</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[ceph@mon1 ~]$ sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow'</font></div><div><font size="2"   >creating /etc/ceph/ceph.client.admin.keyring</font></div><p></p></pre></div><div><br></div><div>4.5 合并keyring<span style="line-height: 28px;"   >&nbsp;</span><span style="line-height: 28px;"   >(mon1执行)</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Add the client.admin key to the ceph.mon.keyring.</font></div><div><font size="2"   >[ceph@mon1 ~]$ sudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring</font></div><div><font size="2"   >importing contents of /etc/ceph/ceph.client.admin.keyring into /tmp/ceph.mon.keyring</font></div><p></p></pre></div><div><br></div><div><br></div><div>4.6 生成temp monitor map.<span style="line-height: 28px;"   >&nbsp;</span><span style="line-height: 28px;"   >(mon1执行)</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Monitor Map: Bootstrapping the initial monitor(s) requires you to generate a monitor map. The monitor map requires the fsid, the cluster name (or uses the default), and at least one host name and its IP address.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Generate a monitor map using the hostname(s), host IP address(es) and the FSID. Save it as /tmp/monmap:</font></div><div><font size="2"   >[ceph@mon1 ~]$ sudo monmaptool --create --clobber --add mon1 172.17.0.2 --add mon2 172.17.0.3 --add mon3 172.17.0.4 --fsid f649b128-963c-4802-ae17-5a76f36c4c76 /tmp/monmap</font></div><div><font size="2"   >monmaptool: monmap file /tmp/monmap</font></div><div><font size="2"   >monmaptool: set fsid to f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >monmaptool: writing epoch 0 to /tmp/monmap (3 monitors)</font></div><p></p></pre></div><div><br></div><div>4.7 创建监控节点数据目录, (存储cluster maps)&nbsp;<span style="line-height: 28px;"   >&nbsp;</span><span style="line-height: 28px;"   >(mon1执行)</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Create a default data directory (or directories) on the monitor host(s).</font></div><div><font size="2"   >sudo mkdir /var/lib/ceph/mon/{cluster-name}-{hostname}</font></div><div><font size="2"   >For example:</font></div><p></p></pre></div><div>为所有监控节点创建各自的map数据目录, 在mon1节点生成后拷贝到其他mon节点.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[ceph@mon1 ~]$ sudo mkdir -p /var/lib/ceph/mon/ceph-mon1</font></div><div><span style="line-height: 28px;"   ><font size="2"   >[ceph@mon1 ~]$ sudo mkdir -p /var/lib/ceph/mon/ceph-mon2</font></span></div><div><span style="line-height: 28px;"   ><font size="2"   >[ceph@mon1 ~]$ sudo mkdir -p /var/lib/ceph/mon/ceph-mon3</font></span></div><p></p></pre></div><div><span style="line-height: 28px;"   ><br></span></div><div>4.8 使用前面创建的监控和管理key, 以及map数据, 写入数据目录.&nbsp;<span style="line-height: 28px;"   >(mon1执行)</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Populate the monitor daemon(s) with the monitor map and keyring.</font></div><div><font size="2"   >ceph-mon --mkfs -i {hostname} --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring</font></div><div><font size="2"   >For example:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[ceph@mon1 ~]$ sudo ceph-mon --mkfs -i mon1 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring</font></div><div><font size="2"   >ceph-mon: set fsid to f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >ceph-mon: created monfs at /var/lib/ceph/mon/ceph-mon1 for mon.mon1</font></div><div><font size="2"   >[ceph@mon1 ~]$ sudo ceph-mon --mkfs -i mon2 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring</font></div><div><font size="2"   >ceph-mon: set fsid to f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >ceph-mon: created monfs at /var/lib/ceph/mon/ceph-mon2 for mon.mon2</font></div><div><font size="2"   >[ceph@mon1 ~]$ sudo ceph-mon --mkfs -i mon3 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring</font></div><div><font size="2"   >ceph-mon: set fsid to f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >ceph-mon: created monfs at /var/lib/ceph/mon/ceph-mon3 for mon.mon3</font></div><p></p></pre></div><div><br></div><div>在centos中使用, 需要添加sysvinit文件, 否则无法使用service来管理服务.</div><div>当然如果不用/etc/init.d/ceph来管理, 也可以直接使用ceph-mon来管理.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Touch the done file.</font></div><div><font size="2"   >Mark that the monitor is created and ready to be started:</font></div><div><font size="2"   >[ceph@mon1 ~]$ sudo touch /var/lib/ceph/mon/ceph-mon1/done</font></div><div><font size="2"   >[ceph@mon1 ~]$ sudo touch /var/lib/ceph/mon/ceph-mon2/done</font></div><div><font size="2"   >[ceph@mon1 ~]$ sudo touch /var/lib/ceph/mon/ceph-mon3/done</font></div><div><font size="2"   >[ceph@mon1 ~]$ sudo touch /var/lib/ceph/mon/ceph-mon1/sysvinit</font></div><div><font size="2"   >[ceph@mon1 ~]$ sudo touch /var/lib/ceph/mon/ceph-mon2/sysvinit</font></div><div><font size="2"   >[ceph@mon1 ~]$ sudo touch /var/lib/ceph/mon/ceph-mon3/sysvinit</font></div><p></p></pre></div><div><br></div><div>4.9 创建ceph集群配置文件ceph.conf .&nbsp;<span style="line-height: 28px;"   >(mon1执行)</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Consider settings for a Ceph configuration file. Common settings include the following:</font></div><div><font size="2"   >[global]</font></div><div><font size="2"   >fsid = {cluster-id}</font></div><div><font size="2"   >mon initial members = {hostname}[, {hostname}]</font></div><div><font size="2"   >mon host = {ip-address}[, {ip-address}]</font></div><div><font size="2"   >public network = {network}[, {network}]</font></div><div><font size="2"   >cluster network = {network}[, {network}]</font></div><div><font size="2"   >auth cluster required = cephx</font></div><div><font size="2"   >auth service required = cephx</font></div><div><font size="2"   >auth client required = cephx</font></div><div><font size="2"   >osd journal size = {n}</font></div><div><font size="2"   >filestore xattr use omap = true</font></div><div><font size="2"   >osd pool default size = {n} &nbsp;# Write an object n times.</font></div><div><font size="2"   >osd pool default min size = {n} # Allow writing n copy in a degraded state.</font></div><div><font size="2"   >osd pool default pg num = {n}</font></div><div><font size="2"   >osd pool default pgp num = {n}</font></div><div><font size="2"   >osd crush chooseleaf type = {n}</font></div><div><font size="2"   >In the foregoing example, the [global] section of the configuration might look like this:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[ceph@mon1 ceph]$ sudo vi /etc/ceph/ceph.conf</font></div><div><font size="2"   >[global]</font></div><div><font size="2"   >fsid = f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >mon initial members = mon1, mon2, mon3</font></div><div><font size="2"   >mon host = 172.17.0.2, 172.17.0.3, 172.17.0.4</font></div><div><font size="2"   >public network = 172.17.0.0/16</font></div><div><font size="2"   >cluster network = 172.18.0.0/16, 172.19.0.0/16</font></div><div><font size="2"   >auth cluster required = cephx</font></div><div><font size="2"   >auth service required = cephx</font></div><div><font size="2"   >auth client required = cephx</font></div><div><font size="2"   >osd journal size = 1024</font></div><div><font size="2"   >filestore xattr use omap = true</font></div><div><font size="2"   >osd pool default size = 2</font></div><div><font size="2"   >osd pool default min size = 1</font></div><div><font size="2"   >osd pool default pg num = 333</font></div><div><font size="2"   >osd pool default pgp num = 333</font></div><div><font size="2"   >osd crush chooseleaf type = 1</font></div><div><font size="2"   >ms_bind_ipv6 = false</font></div><p></p></pre></div><div>注意我们使用了3张网, 1张用于client交互, 另外两张用于集群内部交互(如data replication, rebalance...)</div><div>网络配置参考</div><div><a target="_blank" rel="nofollow" href="http://docs.ceph.com/docs/master/rados/configuration/network-config-ref/"   >http://docs.ceph.com/docs/master/rados/configuration/network-config-ref/</a></div><div>ceph集群配置参考</div><div><a target="_blank" rel="nofollow" href="http://ceph.com/docs/master/rados/configuration/ceph-conf/"   >http://ceph.com/docs/master/rados/configuration/ceph-conf/</a></div><div><br></div><div>4.10 拷贝配置文件和cluster map数据目录到其他几个monitor节点.&nbsp;<span style="line-height: 28px;"   >(mon1执行)</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# scp /etc/ceph/ceph.conf 172.17.0.3:/etc/ceph/</font></div><div><font size="2"   >[root@mon1 ~]# scp /etc/ceph/ceph.conf 172.17.0.4:/etc/ceph/</font></div><div><font size="2"   >[root@mon1 ~]# scp -r /var/lib/ceph/mon/ceph-mon2 172.17.0.3:/var/lib/ceph/mon/</font></div><div><font size="2"   >[root@mon1 ~]# scp -r /var/lib/ceph/mon/ceph-mon3 172.17.0.4:/var/lib/ceph/mon/</font></div><p></p></pre></div><div><br></div><div>4.11 启动monitor后台进程.&nbsp;<span style="line-height: 28px;"   >(mon1, mon2, mon3执行)</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Start the monitor(s).</font></div><div><font size="2"   >[ceph@mon1 ]$ sudo /etc/init.d/ceph start mon.mon1</font></div><div><font size="2"   >[ceph@mon2 ~]$ sudo /etc/init.d/ceph start mon.mon2</font></div><div><font size="2"   >[ceph@mon3 ~]$ sudo /etc/init.d/ceph start mon.mon3</font></div><p></p></pre></div><div>直接启动mon的命令 :&nbsp;</div><div>例如</div><pre class="prettyprint"   ><p></p><div><font size="2"   >/usr/bin/ceph-mon -i mon1 --pid-file /var/run/ceph/mon.mon1.pid -c /etc/ceph/ceph.conf --cluster ceph</font></div><div></div><p></p></pre><div><span style="line-height: 28px;"   >支持指定数据目录</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# ceph-mon --help</font></div><div><font size="2"   >usage: ceph-mon -i monid [flags]</font></div><div><font size="2"   >&nbsp; --debug_mon n</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; debug monitor level (e.g. 10)</font></div><div><font size="2"   >&nbsp; --mkfs</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; build fresh monitor fs</font></div><div><font size="2"   >&nbsp; --force-sync</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; force a sync from another mon by wiping local data (BE CAREFUL)</font></div><div><font size="2"   >&nbsp; --yes-i-really-mean-it</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; mandatory safeguard for --force-sync</font></div><div><font size="2"   >&nbsp; --compact</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; compact the monitor store</font></div><div><font size="2"   >&nbsp; --osdmap &lt;filename&gt;</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; only used when --mkfs is provided: load the osdmap from &lt;filename&gt;</font></div><div><font size="2"   >&nbsp; --inject-monmap &lt;filename&gt;</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write the &lt;filename&gt; monmap to the local monitor store and exit</font></div><div><font size="2"   >&nbsp; --extract-monmap &lt;filename&gt;</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; extract the monmap from the local monitor store and exit</font></div><div><font size="2"   >&nbsp; --mon-data &lt;directory&gt;</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; where the mon store and keyring are located</font></div><div><font size="2"   >&nbsp; --conf/-c FILE &nbsp; &nbsp;read configuration from the given configuration file</font></div><div><font size="2"   >&nbsp; --id/-i ID &nbsp; &nbsp; &nbsp; &nbsp;set ID portion of my name</font></div><div><font size="2"   >&nbsp; --name/-n TYPE.ID set name</font></div><div><font size="2"   >&nbsp; --cluster NAME &nbsp; &nbsp;set cluster name (default: ceph)</font></div><div><font size="2"   >&nbsp; --version &nbsp; &nbsp; &nbsp; &nbsp; show version and quit</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; -d &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;run in foreground, log to stderr.</font></div><div><font size="2"   >&nbsp; -f &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;run in foreground, log to usual location.</font></div><div><font size="2"   >&nbsp; --debug_ms N &nbsp; &nbsp; &nbsp;set message debug level (e.g. 1)</font></div><p></p></pre></div><div><br></div><div>4.12 检查集群是否已经启动.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Verify that Ceph created the default pools.</font></div><div><font size="2"   >[ceph@mon1 ~]$ sudo ceph osd lspools</font></div><div><font size="2"   >0 rbd,</font></div><div><font size="2"   >[ceph@mon2 ~]$ sudo ceph osd lspools</font></div><div><font size="2"   >0 rbd,</font></div><div><font size="2"   >[ceph@mon3 ~]$ sudo ceph osd lspools</font></div><div><font size="2"   >0 rbd,</font></div><p></p></pre></div><div><br></div><div>检查集群状态, 在3个monitor节点都应返回正常信息如下当前为HEALTH_ERR状态, 因为还没有加入任何Osd :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Verify that the monitor is running.</font></div><div><font size="2"   >[ceph@mon3 ~]$ sudo ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_ERR 64 pgs stuck inactive; 64 pgs stuck unclean; no osds</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e1: 3 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0}, election epoch 6, quorum 0,1,2 mon1,mon2,mon3</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e1: 0 osds: 0 up, 0 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 kB used, 0 kB / 0 kB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 64 creating</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >You should see output that the monitor you started is up and running, and you should see a health error indicating that placement groups are stuck inactive. It should look something like this:</font></div><div><font size="2"   >Note: Once you add OSDs and start them, the placement group health errors should disappear.</font></div><p></p></pre></div><div>到此 monitor配置和启动完成.</div><div><br></div></div>[参考]<wbr><div>1.&nbsp;<a style="line-height: 28px;" target="_blank" href="http://blog.163.com/digoal@126/blog/static/163877040201410269169450/"   >http://blog.163.com/digoal@126/blog/static/163877040201410269169450/</a></div><div>2.&nbsp;<a style="line-height: 28px;" target="_blank" href="http://blog.163.com/digoal@126/blog/static/1638770402014102732027457/"   >http://blog.163.com/digoal@126/blog/static/1638770402014102732027457/</a></div><div>3.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="http://docs.ceph.com/docs/master/rados/operations/add-or-rm-mons/"   >http://docs.ceph.com/docs/master/rados/operations/add-or-rm-mons/</a></div><div>4. 网络配置</div><div><div id="id1"   style="color: rgb(62, 67, 73); font-family: Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.6000003814697px;"   ><h3 style="margin: 30px 0px 10px; padding: 5px 0px; text-transform: uppercase; font-weight: normal; font-stretch: normal; font-size: 16px; line-height: 1.6; font-family: 'Titillium Web'; color: rgb(55, 66, 74);"   >PUBLIC NETWORK<a title="Permalink to this headline" style="color: rgb(198, 15, 15); text-decoration: none; visibility: hidden; font-size: 0.8em; padding: 0px 4px;" rel="nofollow" href="http://docs.ceph.com/docs/master/rados/configuration/network-config-ref/#id1"   ></a></h3><p style="line-height: 1.5em;"   >The public network configuration allows you specifically define IP addresses and subnets for the public network. You may specifically assign static IP addresses or override&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>public</span>&nbsp;<span>network</span></tt>&nbsp;settings using the&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>public</span>&nbsp;<span>addr</span></tt>&nbsp;setting for a specific daemon.</p><p style="line-height: 1.5em;"   ><tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>public</span>&nbsp;<span>network</span></tt></p><table frame="void"   rules="none"   style="border: 0px; border-collapse: collapse; margin: 1.5em;"   ><colgroup><col><col><tbody valign="top"   ><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Description:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >The IP address and netmask of the public (front-side) network (e.g.,&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>192.168.0.0/24</span></tt>). Set in&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>[global]</span></tt>. You may specify comma-delimited subnets.</td></tr><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Type:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   ><tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>{ip-address}/{netmask}</span>&nbsp;<span>[,</span>&nbsp;<span>{ip-address}/{netmask}]</span></tt></td></tr><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Required:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >No</td></tr><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Default:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >N/A</td></tr></table><p style="line-height: 1.5em;"   ><tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>public</span>&nbsp;<span>addr</span></tt></p><table frame="void"   rules="none"   style="border: 0px; border-collapse: collapse; margin: 1.5em;"   ><colgroup><col><col><tbody valign="top"   ><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Description:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >The IP address for the public (front-side) network. Set for each daemon.</td></tr><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Type:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >IP Address</td></tr><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Required:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >No</td></tr><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Default:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >N/A</td></tr></table></div><div id="id2"   style="color: rgb(62, 67, 73); font-family: Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.6000003814697px;"   ><h3 style="margin: 30px 0px 10px; padding: 5px 0px; text-transform: uppercase; font-weight: normal; font-stretch: normal; font-size: 16px; line-height: 1.6; font-family: 'Titillium Web'; color: rgb(55, 66, 74);"   >CLUSTER NETWORK<a title="Permalink to this headline" style="color: rgb(198, 15, 15); text-decoration: none; visibility: hidden; font-size: 0.8em; padding: 0px 4px;" rel="nofollow" href="http://docs.ceph.com/docs/master/rados/configuration/network-config-ref/#id2"   ></a></h3><p style="line-height: 1.5em;"   >The cluster network configuration allows you to declare a cluster network, and specifically define IP addresses and subnets for the cluster network. You may specifically assign static IP addresses or override&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>cluster</span>&nbsp;<span>network</span></tt>&nbsp;settings using the&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>cluster</span>&nbsp;<span>addr</span></tt>&nbsp;setting for specific OSD daemons.</p><p style="line-height: 1.5em;"   ><tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>cluster</span>&nbsp;<span>network</span></tt></p><table frame="void"   rules="none"   style="border: 0px; border-collapse: collapse; margin: 1.5em;"   ><colgroup><col><col><tbody valign="top"   ><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Description:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >The IP address and netmask of the cluster (back-side) network (e.g.,&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>10.0.0.0/24</span></tt>). Set in&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>[global]</span></tt>. You may specify comma-delimited subnets.</td></tr><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Type:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   ><tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>{ip-address}/{netmask}</span>&nbsp;<span>[,</span>&nbsp;<span>{ip-address}/{netmask}]</span></tt></td></tr><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Required:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >No</td></tr><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Default:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >N/A</td></tr></table><p style="line-height: 1.5em;"   ><tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>cluster</span>&nbsp;<span>addr</span></tt></p><table frame="void"   rules="none"   style="border: 0px; border-collapse: collapse; margin: 1.5em;"   ><colgroup><col><col><tbody valign="top"   ><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Description:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >The IP address for the cluster (back-side) network. Set for each daemon.</td></tr><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Type:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Address</td></tr><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Required:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >No</td></tr><tr><th style="padding: 1px 8px 1px 5px; border: 0px !important;"   >Default:</th><td style="padding: 1px 8px 1px 5px; border: 0px !important;"   >N/A</td></tr></table></div></div><div><br></div>
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"   ><img title="ceph install in CentOS 7 x64 within docker - 3 - 德哥@Digoal - PostgreSQL research"   src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"   alt="Flag Counter"   border="0"   ></a></div>
	</div>
</div>
</body>
</html>