<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=gbk">
<title>PostgreSQL research</title>
<style type="text/css">
.blogcnt{line-height:160%;font-size:14px;text-align:left;word-wrap:break-word;}
.blogcnt *{line-height:160%;}
.blogcnt p{margin:0 0 10px;}
.blogcnt ul,.nbw-blog ol{margin:5px 0 5px 40px;padding:0}
.blogcnt em{font-style:italic;}
.blogcnt blockquote{font-size:1em;margin:auto 0 auto 35px;}
.blogcnt img{border:0;max-width:100%;}
</style>
</head>
<body style="color:#444444;">
<h1 id="blog-Title"><a href="index.html">PostgreSQL research</a></h1>
<div id="" style="padding:0 20px;">
	<h2 id="">ceph install in CentOS 7 x64 within docker - 4</h2>
	<h5 id="">2014-12-02 10:15:41&nbsp;&nbsp;&nbsp;<a href="http://blog.163.com/digoal@126/blog/static/163877040201411141846487/" target="_blank">查看原文&gt;&gt;</a></h5>
	<div class="" id="" style="padding:0 20px;">
		<div class="blogcnt" style="width:800px;"><div>前面已经部署好了monitor, 接下来要部署4台osd节点.</div><div>osd节点需要注意osd数据目录, 建议挂载到独立的硬盘, journal建议放到ssd中.</div><div>如果一个主机有多个硬盘并且没有使用lvm来管理条带的话, 建议启动多个osd daemon, 分别挂载到他们的数据目录.</div><div><pre class="prettyprint"   ><p><font size="2"   >Once you have your initial monitor(s) running, you should add OSDs. Your cluster cannot reach an active + clean state until you have enough OSDs to handle the number of copies of an object (e.g., osd pool default size = 2 requires at least two OSDs). After bootstrapping your monitor, your cluster has a default CRUSH map; however, the CRUSH map doesn’t have any Ceph OSD Daemons mapped to a Ceph Node.</font></p></pre></div><div><br></div><div><span style="line-height: 28px;"   >生成osd UUID, 每个osd进程对应一个uuid(如果单台主机要运行多个osd daemon, 那么也需要多个uuid) :&nbsp;</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@osd1 ~]# uuidgen</font></div><div><font size="2"   >854777b2-c188-4509-9df4-02f57bd17e12</font></div><div><font size="2"   >[root@osd2 ceph]# uuidgen</font></div><div><font size="2"   >d13959a4-a3fc-4589-91cd-7104fcd8dbe9</font></div><div><font size="2"   >[root@osd3 ceph]# uuidgen&nbsp;</font></div><div><font size="2"   >d28bdb68-e8ee-4ca8-be5d-7e86438e7663</font></div><div><font size="2"   >[root@osd4 ~]# uuidgen&nbsp;</font></div><div><font size="2"   >c2e86146-fedc-4486-8d2f-ead6f473e841</font></div><p></p></pre></div><div><br></div><div><span style="line-height: 28px;"   >从mon节点拷贝key文件和ceph.conf到osd节点, 没有KEY无法通讯.</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ceph]# cd /etc/ceph</font></div><div><font size="2"   >[root@mon1 ceph]# ll</font></div><div><font size="2"   >total 8</font></div><div><font size="2"   >-rw------- 1 root root 137 Nov 28 16:39 ceph.client.admin.keyring</font></div><div><font size="2"   >-rw-r--r-- 1 root root 529 Dec &nbsp;1 17:18 ceph.conf</font></div><div><font size="2"   >[root@mon1 ceph]# scp ceph.conf ceph.client.admin.keyring 172.17.0.5:/etc/ceph</font></div><div><font size="2"   >[root@mon1 ceph]# scp ceph.conf ceph.client.admin.keyring 172.17.0.6:/etc/ceph</font></div><div><font size="2"   >[root@mon1 ceph]# scp ceph.conf ceph.client.admin.keyring 172.17.0.7:/etc/ceph</font></div><div><font size="2"   >[root@mon1 ceph]# scp ceph.conf ceph.client.admin.keyring 172.17.0.8:/etc/ceph</font></div><p></p></pre></div><div><br></div><div>其中一台osd, 可以看到拷贝过来的ceph.conf和key文件.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@osd2 ceph]# cd /etc/ceph</font></div><div><font size="2"   >[root@osd2 ceph]# ll</font></div><div><font size="2"   >total 12</font></div><div><font size="2"   >-rw------- 1 root root 137 Dec &nbsp;1 17:35 ceph.client.admin.keyring</font></div><div><font size="2"   >-rw-r--r-- 1 root root 529 Dec &nbsp;1 17:35 ceph.conf</font></div><div><font size="2"   >-rwxr-xr-x 1 root root &nbsp;92 Oct 29 18:35 rbdmap</font></div><p></p></pre></div><div><br></div><div>创建osd, 返回osd numberid.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@osd1 ceph]# ceph osd create 854777b2-c188-4509-9df4-02f57bd17e12</font></div><div><font size="2"   >0</font></div><div><font size="2"   >返回0表示该osd-number=0</font></div><div><font size="2"   >[root@osd2 ceph]# ceph osd create d13959a4-a3fc-4589-91cd-7104fcd8dbe9</font></div><div><font size="2"   >1</font></div><div><font size="2"   >返回0表示该osd-number=1</font></div><div><font size="2"   >[root@osd3 ~]# ceph osd create d28bdb68-e8ee-4ca8-be5d-7e86438e7663</font></div><div><font size="2"   >2</font></div><div><font size="2"   >[root@osd4 ~]# ceph osd create c2e86146-fedc-4486-8d2f-ead6f473e841</font></div><div><font size="2"   >3</font></div><p></p></pre></div><div><br></div><div>创建osd数据目录, 注意替换osd-number :&nbsp;</div><div>使用外部目录, 然后使用软链接.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Create the default directory on your new OSD.</font></div><div><font size="2"   >ssh {new-osd-host}</font></div><div><font size="2"   >sudo mkdir /var/lib/ceph/osd/ceph-{osd-number}</font></div><div><font size="2"   ><br></font></div><div><font size="2"   ><span style="line-height: 21px;"   >[root@osd1 ceph]# mkdir /data01/ceph/osd/ceph-0<br>[root@osd2 ceph]# mkdir /data01/ceph/osd/ceph-1<br>[root@osd3 ceph]# mkdir /data01/ceph/osd/ceph-2<br>[root@osd4 ceph]# mkdir /data01/ceph/osd/ceph-3</span></font></div><div><font size="2"   ><span style="line-height: 21px;"   ><br></span></font></div><div><font size="2"   >[root@osd1 ceph]# ln -s /data01/ceph/osd/ceph-0 /var/lib/ceph/osd/<br>[root@osd2 ceph]# ln -s /data01/ceph/osd/ceph-1 /var/lib/ceph/osd/<br>[root@osd3 ceph]# ln -s /data01/ceph/osd/ceph-2 /var/lib/ceph/osd/<br>[root@osd4 ceph]# ln -s /data01/ceph/osd/ceph-3 /var/lib/ceph/osd/</font></div><p></p></pre></div><div><br></div><div>性能相关 :&nbsp;</div><div><span style="line-height: 28px;"   >如果OSD数据目录要挂载其他块设备, osd number即ceph osd create返回的值.</span></div><div>最好在初始化数据目录前完成挂载, 因为可能要读取文件系统信息.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >If the OSD is for a drive other than the OS drive, prepare it for use with Ceph, and mount it to the directory you just created:</font></div><div><font size="2"   >ssh {new-osd-host}</font></div><div><font size="2"   >sudo mkfs -t {fstype} /dev/{hdd}</font></div><div><font size="2"   >sudo mount -o user_xattr /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number}</font></div><p></p></pre></div><div><br></div><div>初始化osd 数据目录</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Initialize the OSD data directory.</font></div><div><font size="2"   >ssh {new-osd-host}</font></div><div><font size="2"   >sudo ceph-osd -i {osd-num} --mkfs --mkkey --osd-uuid [{uuid}] --cluster {cluster_name}</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@osd1 ceph]# <span style="line-height: 21px;"   >ceph-osd -i 0 --mkfs --mkjournal --mkkey --osd-uuid 854777b2-c188-4509-9df4-02f57bd17e12 --cluster ceph --osd-data=/data01/ceph/osd/ceph-0 --osd-journal=/data01/ceph/osd/ceph-0/journal</span></font></div><div><font size="2"   ><span style="line-height: 21px;"   ><br></span></font></div><div><font size="2"   >2014-12-02 08:52:13.438014 7f909cad9880 -1 journal FileJournal::_open: disabling aio for non-block journal. &nbsp;Use journal_force_aio to force use of aio anyway</font></div><div><font size="2"   >2014-12-02 08:52:13.669785 7f909cad9880 -1 journal FileJournal::_open: disabling aio for non-block journal. &nbsp;Use journal_force_aio to force use of aio anyway</font></div><div><font size="2"   >2014-12-02 08:52:13.671436 7f909cad9880 -1 filestore(/var/lib/ceph/osd/ceph-0) could not find 23c2fcde/osd_superblock/0//-1 in index: (2) No such file or directory</font></div><div><font size="2"   >2014-12-02 08:52:13.942519 7f909cad9880 -1 created object store /var/lib/ceph/osd/ceph-0 journal /var/lib/ceph/osd/ceph-0/journal for osd.0 fsid f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >2014-12-02 08:52:13.942645 7f909cad9880 -1 auth: error reading file: /var/lib/ceph/osd/ceph-0/keyring: can't open /var/lib/ceph/osd/ceph-0/keyring: (2) No such file or directory</font></div><div><font size="2"   >2014-12-02 08:52:13.942748 7f909cad9880 -1 created new key in keyring /var/lib/ceph/osd/ceph-0/keyring</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@osd2 ceph]# <span style="line-height: 21px;"   >ceph-osd -i 1 --mkfs --mkjournal --mkkey --osd-uuid d13959a4-a3fc-4589-91cd-7104fcd8dbe9 --cluster ceph --osd-data=/data01/ceph/osd/ceph-1 --osd-journal=/data01/ceph/osd/ceph-1/journal</span></font></div><div><font size="2"   >[root@osd3 ceph]# <span style="line-height: 21px;"   >ceph-osd -i 2 --mkfs --mkjournal --mkkey --osd-uuid d28bdb68-e8ee-4ca8-be5d-7e86438e7663 --cluster ceph --osd-data=/data01/ceph/osd/ceph-2 --osd-journal=/data01/ceph/osd/ceph-2/journal</span></font></div><div><font size="2"   >[root@osd4 ceph]# <span style="line-height: 21px;"   >ceph-osd -i 3 --mkfs --mkjournal --mkkey --osd-uuid c2e86146-fedc-4486-8d2f-ead6f473e841 --cluster ceph --osd-data=/data01/ceph/osd/ceph-3 --osd-journal=/data01/ceph/osd/ceph-3/journal</span></font></div><p></p></pre></div><div><br></div><div>注册OSD认证key</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Register the OSD authentication key. The value of ceph for ceph-{osd-num} in the path is the $cluster-$id. If your cluster name differs from ceph, use your cluster name instead.:</font></div><div><font size="2"   >sudo ceph auth add osd.{osd-num} osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-{osd-num}/keyring</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@osd1 ~]# <span style="line-height: 21px;"   >ceph auth add osd.0 osd 'allow *' mon 'allow profile osd' -i /data01/ceph/osd/ceph-0/keyring</span></font></div><div><font size="2"   >added key for osd.0</font></div><div><font size="2"   >[root@osd2 ~]# <span style="line-height: 21px;"   >ceph auth add osd.1 osd 'allow *' mon 'allow profile osd' -i /data01/ceph/osd/ceph-1/keyring</span></font></div><div><font size="2"   >added key for osd.1</font></div><div><font size="2"   >[root@osd3 ~]# <span style="line-height: 21px;"   >ceph auth add osd.2 osd 'allow *' mon 'allow profile osd' -i /data01/ceph/osd/ceph-2/keyring</span></font></div><div><font size="2"   >added key for osd.2</font></div><div><font size="2"   >[root@osd4 ~]# <span style="line-height: 21px;"   >ceph auth add osd.3 osd 'allow *' mon 'allow profile osd' -i /data01/ceph/osd/ceph-3/keyring</span></font></div><div><font size="2"   >added key for osd.3</font></div><p></p></pre></div><div><br></div><div>添加Ceph Node到CRUSH map.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Add your Ceph Node to the CRUSH map.</font></div><div><font size="2"   >ceph osd crush add-bucket {hostname} host</font></div><div><font size="2"   >For example:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@osd1 ~]# ceph osd crush add-bucket osd1 host</font></div><div><font size="2"   >added bucket osd1 type host to crush map</font></div><div><font size="2"   >[root@osd2 ~]# ceph osd crush add-bucket osd2 host</font></div><div><font size="2"   >added bucket osd2 type host to crush map</font></div><div><font size="2"   >[root@osd3 ~]# ceph osd crush add-bucket osd3 host</font></div><div><font size="2"   >added bucket osd3 type host to crush map</font></div><div><font size="2"   >[root@osd4 ~]# ceph osd crush add-bucket osd4 host</font></div><div><font size="2"   >added bucket osd4 type host to crush map</font></div><p></p></pre></div><div><br></div><div>添加ceph node到default root.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Place the Ceph Node under the root default.</font></div><div><font size="2"   >[root@osd1 ~]# ceph osd crush move osd1 root=default</font></div><div><font size="2"   >moved item id -2 name 'osd1' to location {root=default} in crush map</font></div><div><font size="2"   >[root@osd2 ~]# ceph osd crush move osd2 root=default</font></div><div><font size="2"   >moved item id -3 name 'osd2' to location {root=default} in crush map</font></div><div><font size="2"   >[root@osd3 ~]# ceph osd crush move osd3 root=default</font></div><div><font size="2"   >moved item id -4 name 'osd3' to location {root=default} in crush map</font></div><div><font size="2"   >[root@osd4 ~]# ceph osd crush move osd4 root=default</font></div><div><font size="2"   >moved item id -5 name 'osd4' to location {root=default} in crush map</font></div><p></p></pre></div><div><br></div><div>添加osd到crush map.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Add the OSD to the CRUSH map so that it can begin receiving data. You may also decompile the CRUSH map, add the OSD to the device list, add the host as a bucket (if it's not already in the CRUSH map), add the device as an item in the host, assign it a weight, recompile it and set it.</font></div><div><font size="2"   >ceph osd crush add osd.{osd_num}|{id-or-name} {weight} [{bucket-type}={bucket-name} ...]</font></div><div><font size="2"   >For example:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@osd1 ~]# ceph osd crush add osd.0 1.0 host=osd1</font></div><div><font size="2"   >add item id 0 name 'osd.0' weight 1 at location {host=osd1} to crush map</font></div><div><font size="2"   >[root@osd2 ~]# ceph osd crush add osd.1 1.0 host=osd2</font></div><div><font size="2"   >add item id 1 name 'osd.1' weight 1 at location {host=osd2} to crush map</font></div><div><font size="2"   >[root@osd3 ~]# ceph osd crush add osd.2 1.0 host=osd3</font></div><div><font size="2"   >[root@osd4 ~]# ceph osd crush add osd.3 1.0 host=osd4</font></div><p></p></pre></div><div><br></div><div>启动osd服务, 使用CentOS的话, 需要添加sysvinit文件到<span style="line-height: 28px;"   >/var/lib/ceph/osd/{cluster-name}-{osd-num}/目录.</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >After you add an OSD to Ceph, the OSD is in your configuration. However, it is not yet running. The OSD is down and in. You must start your new OSD before it can begin receiving data.</font></div><div><font size="2"   >For Debian/CentOS/RHEL, use sysvinit:</font></div><div><font size="2"   >sudo touch /var/lib/ceph/osd/{cluster-name}-{osd-num}/sysvinit</font></div><div><font size="2"   >sudo /etc/init.d/ceph start osd.1</font></div><p></p></pre></div><div><br></div><div>添加sysvinit文件, 启动服务.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@osd1 ceph]# touch /var/lib/ceph/osd/ceph-0/sysvinit</font></div><div><font size="2"   >[root@osd1 ceph]# service ceph start osd.0</font></div><div><font size="2"   >=== osd.0 ===&nbsp;</font></div><div><font size="2"   >create-or-move updated item name 'osd.0' weight 0.01 at location {host=osd1,root=default} to crush map</font></div><div><font size="2"   >Starting Ceph osd.0 on osd1...</font></div><div><font size="2"   >starting osd.0 at :/0 osd_data /var/lib/ceph/osd/ceph-0 /var/lib/ceph/osd/ceph-0/journal</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@osd2 ~]# touch /var/lib/ceph/osd/ceph-1/sysvinit</font></div><div><font size="2"   >[root@osd2 ~]# service ceph start osd.1</font></div><div><font size="2"   >=== osd.1 ===&nbsp;</font></div><div><font size="2"   >create-or-move updated item name 'osd.1' weight 0.01 at location {host=osd2,root=default} to crush map</font></div><div><font size="2"   >Starting Ceph osd.1 on osd2...</font></div><div><font size="2"   >starting osd.1 at :/0 osd_data /var/lib/ceph/osd/ceph-1 /var/lib/ceph/osd/ceph-1/journal</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@osd3 ~]# touch /var/lib/ceph/osd/ceph-2/sysvinit</font></div><div><font size="2"   >[root@osd3 ~]# service ceph start osd.2</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@osd4 ~]# touch /var/lib/ceph/osd/ceph-3/sysvinit</font></div><div><font size="2"   >[root@osd4 ~]# service ceph start osd.3</font></div><p></p></pre></div><div>部署osd结束.</div><div><br></div><div>或者你可以直接使用ceph-osd命令启动osd</div><div>例如</div><pre class="prettyprint"   ><p></p><div><font size="2"   >/usr/bin/ceph-osd -i 1 --pid-file /var/run/ceph/osd.1.pid -c /etc/ceph/ceph.conf --cluster ceph</font></div><div></div><p></p></pre><div><span style="line-height: 28px;"   >这种方式支持指定数据目录, journal文件.</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@osd2 ~]# ceph-osd -h</font></div><div><font size="2"   >&nbsp; --conf/-c FILE &nbsp; &nbsp;read configuration from the given configuration file</font></div><div><font size="2"   >&nbsp; --id/-i ID &nbsp; &nbsp; &nbsp; &nbsp;set ID portion of my name</font></div><div><font size="2"   >&nbsp; --name/-n TYPE.ID set name</font></div><div><font size="2"   >&nbsp; --cluster NAME &nbsp; &nbsp;set cluster name (default: ceph)</font></div><div><font size="2"   >&nbsp; --version &nbsp; &nbsp; &nbsp; &nbsp; show version and quit</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; -d &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;run in foreground, log to stderr.</font></div><div><font size="2"   >&nbsp; -f &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;run in foreground, log to usual location.</font></div><div><font size="2"   >&nbsp; --debug_ms N &nbsp; &nbsp; &nbsp;set message debug level (e.g. 1)</font></div><div><font size="2"   >2014-12-09 17:08:50.233420 7ff77aff6880 -1 usage: ceph-osd -i osdid [--osd-data=path] [--osd-journal=path] [--mkfs] [--mkjournal] [--convert-filestore]</font></div><div><font size="2"   >2014-12-09 17:08:50.233423 7ff77aff6880 -1 &nbsp; &nbsp;--debug_osd N &nbsp; set debug level (e.g. 10)</font></div><p></p></pre></div><div><br></div><div>启动第二台osd后, ceph 集群正常了, HEALTH_OK.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@osd2 ~]# ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_OK</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e1: 3 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0}, election epoch 12, quorum 0,1,2 mon1,mon2,mon3</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e13: 2 osds: 2 up, 2 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v24: 64 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3213 MB used, 15632 MB / 19902 MB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 64 active+clean</font></div><p></p></pre></div><div><br></div><div>查看osd tree.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@osd2 ~]# ceph osd tree</font></div><div><font size="2"   ># id &nbsp; &nbsp;weight &nbsp;type name &nbsp; &nbsp; &nbsp; up/down reweight</font></div><div><font size="2"   >-1 &nbsp; &nbsp; &nbsp;2 &nbsp; &nbsp; &nbsp; root default</font></div><div><font size="2"   >-2 &nbsp; &nbsp; &nbsp;1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; host osd1</font></div><div><font size="2"   >0 &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; osd.0 &nbsp; up &nbsp; &nbsp; &nbsp;1</font></div><div><font size="2"   >-3 &nbsp; &nbsp; &nbsp;1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; host osd2</font></div><div><font size="2"   >1 &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; osd.1 &nbsp; up &nbsp; &nbsp; &nbsp;1</font></div><p></p></pre></div><div><br></div><div>4台osd添加结束, 会有一个告警&nbsp;<span style="line-height: 28px;"   >too few pgs per osd (16 &lt; min 20)</span><span style="line-height: 28px;"   >.</span></div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >[root@osd4 ~]# ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_WARN too few pgs per osd (16 &lt; min 20)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e1: 3 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0}, election epoch 12, quorum 0,1,2 mon1,mon2,mon3</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e27: 4 osds: 4 up, 4 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v59: 64 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6432 MB used, 31260 MB / 39805 MB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 64 active+clean</font></div></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@osd4 ~]# ceph osd tree</font></div><div><font size="2"   ># id &nbsp; &nbsp;weight &nbsp;type name &nbsp; &nbsp; &nbsp; up/down reweight</font></div><div><font size="2"   >-1 &nbsp; &nbsp; &nbsp;4 &nbsp; &nbsp; &nbsp; root default</font></div><div><font size="2"   >-2 &nbsp; &nbsp; &nbsp;1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; host osd1</font></div><div><font size="2"   >0 &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; osd.0 &nbsp; up &nbsp; &nbsp; &nbsp;1</font></div><div><font size="2"   >-3 &nbsp; &nbsp; &nbsp;1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; host osd2</font></div><div><font size="2"   >1 &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; osd.1 &nbsp; up &nbsp; &nbsp; &nbsp;1</font></div><div><font size="2"   >-4 &nbsp; &nbsp; &nbsp;1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; host osd3</font></div><div><font size="2"   >2 &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; osd.2 &nbsp; up &nbsp; &nbsp; &nbsp;1</font></div><div><font size="2"   >-5 &nbsp; &nbsp; &nbsp;1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; host osd4</font></div><div><font size="2"   >3 &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; osd.3 &nbsp; up &nbsp; &nbsp; &nbsp;1</font></div><p></p></pre></div><div><br></div><div>解决办法, 需要修改<span style="line-height: 28px;"   >pg_num</span><span style="line-height: 28px;"   >&nbsp;,&nbsp;</span><span style="line-height: 28px;"   >pgp_num</span><span style="line-height: 28px;"   >&nbsp;.</span></div><div>先要获取pool name, 如下, 返回pool name : rbd.&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# ceph osd pool stats&nbsp;</font></div><div><font size="2"   >pool rbd id 0</font></div><div><font size="2"   >&nbsp; nothing is going on</font></div><p></p></pre></div><div>修改pool对应的pg_num和pgp_num.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# ceph osd pool set rbd pg_num 128</font></div><div><font size="2"   >[root@mon1 ~]# ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_WARN pool rbd pg_num 128 &gt; pgp_num 64</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e1: 3 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0}, election epoch 12, quorum 0,1,2 mon1,mon2,mon3</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e29: 4 osds: 4 up, 4 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v117: 128 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6434 MB used, 31258 MB / 39805 MB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;128 active+clean</font></div><p></p></pre></div><div>同时还需要调整pgp_num</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# ceph osd pool set rbd pgp_num 128</font></div><div><font size="2"   >set pool 0 pgp_num to 128</font></div><div><font size="2"   >[root@mon1 ~]# ceph -s</font></div><div><font size="2"   >&nbsp; &nbsp; cluster f649b128-963c-4802-ae17-5a76f36c4c76</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;health HEALTH_OK</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;monmap e1: 3 mons at {mon1=172.17.0.2:6789/0,mon2=172.17.0.3:6789/0,mon3=172.17.0.4:6789/0}, election epoch 12, quorum 0,1,2 mon1,mon2,mon3</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp;osdmap e32: 4 osds: 4 up, 4 in</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; pgmap v130: 128 pgs, 1 pools, 0 bytes data, 0 objects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6434 MB used, 31258 MB / 39805 MB avail</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;128 active+clean</font></div><p></p></pre></div><div><br></div><div>需要注意, pg_num只能增加, 不能缩小.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@mon1 ~]# ceph osd pool set rbd pg_num 64</font></div><div><font size="2"   >Error EEXIST: specified pg_num 64 &lt;= current 128</font></div><p></p></pre></div><div><br></div><div>ceph osd数据目录的结构如下(包含journal, 数据等) :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@osd1 ceph-0]# pwd</font></div><div><font size="2"   >/var/lib/ceph/osd/ceph-0</font></div><div><font size="2"   >[root@osd1 ceph-0]# ll</font></div><div><font size="2"   >total 1048616</font></div><div><font size="2"   >-rw-r--r-- &nbsp;1 root root &nbsp; &nbsp; &nbsp; &nbsp; 37 Dec &nbsp;2 08:52 ceph_fsid</font></div><div><font size="2"   >drwxr-xr-x 68 root root &nbsp; &nbsp; &nbsp; 4096 Dec &nbsp;2 09:00 current</font></div><div><font size="2"   >-rw-r--r-- &nbsp;1 root root &nbsp; &nbsp; &nbsp; &nbsp; 37 Dec &nbsp;2 08:52 fsid</font></div><div><font size="2"   >-rw-r--r-- &nbsp;1 root root 1073741824 Dec &nbsp;2 09:00 journal</font></div><div><font size="2"   >-rw------- &nbsp;1 root root &nbsp; &nbsp; &nbsp; &nbsp; 56 Dec &nbsp;2 08:52 keyring</font></div><div><font size="2"   >-rw-r--r-- &nbsp;1 root root &nbsp; &nbsp; &nbsp; &nbsp; 21 Dec &nbsp;2 08:52 magic</font></div><div><font size="2"   >-rw-r--r-- &nbsp;1 root root &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6 Dec &nbsp;2 08:52 ready</font></div><div><font size="2"   >-rw-r--r-- &nbsp;1 root root &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4 Dec &nbsp;2 08:52 store_version</font></div><div><font size="2"   >-rw-r--r-- &nbsp;1 root root &nbsp; &nbsp; &nbsp; &nbsp; 53 Dec &nbsp;2 08:52 superblock</font></div><div><font size="2"   >-rw-r--r-- &nbsp;1 root root &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 Dec &nbsp;2 08:59 sysvinit</font></div><div><font size="2"   >-rw-r--r-- &nbsp;1 root root &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2 Dec &nbsp;2 08:52 whoami</font></div><p></p></pre></div><div><div><br></div><div><div>[参考]</div><div>1.&nbsp;<a target="_blank" rel="nofollow" href="http://ceph.com/docs/master/install/manual-deployment/"   >http://ceph.com/docs/master/install/manual-deployment/</a><br><wbr><div>2.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="http://ceph.com/docs/master/rados/operations/placement-groups/"   >http://ceph.com/docs/master/rados/operations/placement-groups/</a></div><div>3.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="http://ceph.com/docs/master/rados/operations/pools/#createpool"   >http://ceph.com/docs/master/rados/operations/pools/#createpool</a></div><div>4. ceph help, ceph osd help</div><div><br></div></div>
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"   ><img title="ceph install in CentOS 7 x64 within docker - 4 - 德哥@Digoal - PostgreSQL research"   src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"   alt="Flag Counter"   border="0"   ></a><br></div></div></div>
	</div>
</div>
</body>
</html>