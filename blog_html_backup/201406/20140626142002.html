<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=gbk">
<title>PostgreSQL research</title>
<style type="text/css">
.blogcnt{line-height:160%;font-size:14px;text-align:left;word-wrap:break-word;}
.blogcnt *{line-height:160%;}
.blogcnt p{margin:0 0 10px;}
.blogcnt ul,.nbw-blog ol{margin:5px 0 5px 40px;padding:0}
.blogcnt em{font-style:italic;}
.blogcnt blockquote{font-size:1em;margin:auto 0 auto 35px;}
.blogcnt img{border:0;max-width:100%;}
</style>
</head>
<body style="color:#444444;">
<h1 id="blog-Title"><a href="index.html">PostgreSQL research</a></h1>
<div id="" style="padding:0 20px;">
	<h2 id="">ZFS (sync, async) R/W IOPS / throughput performance tuning</h2>
	<h5 id="">2014-06-26 14:20:02&nbsp;&nbsp;&nbsp;<a href="http://blog.163.com/digoal@126/blog/static/1638770402014526992910/" target="_blank">查看原文&gt;&gt;</a></h5>
	<div class="" id="" style="padding:0 20px;">
		<div class="blogcnt" style="width:800px;"><div>本文讨论一下zfs读写IOPS或吞吐量的优化技巧, (读写操作分同步和异步两种情况).</div><div><br></div><div>影响性能的因素</div><div>1. 底层设备的性能直接影响同步读写 iops, throughput. 异步读写和cache(arc, l2arc) 设备或配置有关.</div><div><br></div><div>2. vdev 的冗余选择影响iops, through.</div><div>因为ZPOOL的IO是均分到各vdevs的, 所以vdev越多, IO和吞吐能力越好.</div><div>vdev本身的话, 写性能 mirror &gt; raidz1 &gt; raidz2 &gt; raidz3 ,&nbsp;</div><div>读性能看实际存储的盘数量决定. (raidz1(3) = raidz2(4) = raidz3(5) &gt; mirror(n))</div><div><br></div><div>3. 底层设备的IO对齐影响IOPS.&nbsp;</div><div>在创建zpool 时需指定ashift, 而且以后都无法更改.&nbsp;</div><div>建议同一个vdev底层设备的sector一致, 如果不一致的话, 建议取最大的扇区作为ashift. 或者将不一致的块设备分到不同的vdev里面.</div><div>例如sda sdb的sector=512, sdc sdd的sector=4K</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >zpool create -o ashift=9 zp1 mirror sda sdb</font></div><div><font size="2"   >zpool add -o ashift=12 zp1 mirror sdc sdd</font></div><div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp;ashift</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Pool &nbsp;sector &nbsp;size exponent, to the power of 2 (internally referred to as "ashift"). I/O operations will be</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;aligned to the specified size boundaries. <font color="#ff0000"   >Additionally, the minimum (disk) write size will be &nbsp;set &nbsp;to &nbsp;the</font></font></div><div><font size="2"   ><font color="#ff0000"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;specified &nbsp;size, &nbsp;so &nbsp;this &nbsp;represents a space vs. performance trade-off. </font>The typical case for setting this</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;property is when performance is important and the underlying disks use 4KiB sectors but report 512B sectors</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;to the OS (for compatibility reasons); in that case, set ashift=12 (which is 1&lt;&lt;12 = 4096).</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;For &nbsp;optimal &nbsp;performance, &nbsp;the &nbsp;pool sector size should be <font color="#ff0000"   >greater than or equal to </font>the sector size of the</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;underlying disks. Since the property cannot be changed after pool creation, if in a given &nbsp;pool, &nbsp;you &nbsp;ever</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;want to use drives that report 4KiB sectors, you must set ashift=12 at pool creation time.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Keep in mind is that the ashift is vdev specific and is not a pool global. &nbsp;This means that when adding new</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;vdevs to an existing pool you may need to specify the ashift.</font></div></div><p></p></pre></div><div>这里有一个工具收录了一些常见设备的扇区大小.</div><div><a target="_blank" rel="nofollow" href="https://github.com/zfsonlinux/zfs/blob/master/cmd/zpool/zpool_vdev.c#L108"   >https://github.com/zfsonlinux/zfs/blob/master/cmd/zpool/zpool_vdev.c#L108</a></div><div>如果不清楚底层设备的扇区大小, 为了对齐可以设置为13(8K).</div><div>例如</div><div><pre class="prettyprint"   ><p></p><div><span style="line-height: 28px;"   ><font size="2"   ># zpool create -o ashift=13 zp1 scsi-36c81f660eb17fb001b2c5fec6553ff5e</font></span></div><div><font size="2"   ># zpool create -o ashift=9&nbsp;<span style="line-height: 28px;"   >zp2 scsi-36c81f660eb17fb001b2c5ff465cff3ed</span></font></div><div><font size="2"   ># zfs create -o mountpoint=/data01 zp1/data01</font></div><div><font size="2"   ># zfs create -o mountpoint=/data02 zp2/data02</font></div><div><font size="2"   ><br></font></div><div><div><font size="2"   ># date +%F%T; dd if=/dev/zero of=/data01/test.img bs=1024K count=8192 oflag=sync,noatime,nonblock; date +%F%T;</font></div><div><font size="2"   >2014-06-2609:57:35</font></div><div><font size="2"   >8192+0 records in</font></div><div><font size="2"   >8192+0 records out</font></div><div><font size="2"   >8589934592 bytes (8.6 GB) copied, 46.4277 s, 185 MB/s</font></div><div><font size="2"   >2014-06-2609:58:22</font></div></div><div><font size="2"   ><br></font></div><div><div><font size="2"   ># date +%F%T; dd if=/dev/zero of=/data02/test.img bs=1024K count=8192 oflag=sync,noatime,nonblock; date +%F%T;</font></div><div><font size="2"   >2014-06-2609:58:32</font></div><div><font size="2"   >8192+0 records in</font></div><div><font size="2"   >8192+0 records out</font></div><div><font size="2"   >8589934592 bytes (8.6 GB) copied, 43.9984 s, 195 MB/s</font></div><div><font size="2"   >2014-06-2609:59:16</font></div></div><div><font size="2"   ><br></font></div><div><div><font size="2"   ># zpool list</font></div><div><font size="2"   >NAME &nbsp; SIZE &nbsp;ALLOC &nbsp; FREE &nbsp; &nbsp;CAP &nbsp;DEDUP &nbsp;HEALTH &nbsp;ALTROOT</font></div><div><font size="2"   >zp1 &nbsp; 3.62T &nbsp;8.01G &nbsp;3.62T &nbsp; &nbsp; 0% &nbsp;1.00x &nbsp;ONLINE &nbsp;-</font></div><div><font size="2"   >zp2 &nbsp; 3.62T &nbsp;8.00G &nbsp;3.62T &nbsp; &nbsp; 0% &nbsp;1.00x &nbsp;ONLINE &nbsp;-</font></div></div><p></p></pre></div><div>大文件看不出区别, 小文件的话, 如果文件小于ashift设置的大小, 那么就等于浪费空间, 同时降低了小文件的写效率. 增加cache占用等.&nbsp;</div><div><br></div><div>4. 底层设备的模式, 建议JBOD或passthrough, 绕过RAID卡的控制.</div><div><br></div><div>5. zfs 参数直接影响iops和吞吐量.</div><div>5.1&nbsp;</div><div>对于数据库类型的应用, 大文件, 离散的小数据集访问, 选择recordsize 大于或等于数据库的块大小比较好. 例如PostgreSQL 8K的block_size, 建议zfs recordsize大于等于8KB. 一般不建议调整recordsize, 使用默认的128K就能满足大多数需求.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp;recordsize=size</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Specifies a suggested block size for files in the file system. This property is &nbsp;designed &nbsp;solely &nbsp;for &nbsp;use</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;with &nbsp;database &nbsp;workloads &nbsp;that &nbsp;access &nbsp;files &nbsp;in &nbsp;fixed-size records. ZFS automatically tunes block sizes</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;according to internal algorithms optimized for typical access patterns.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;For databases that create very large files but access them in small random chunks, these algorithms may &nbsp;be</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;suboptimal. &nbsp;<font color="#ff0000"   >Specifying a recordsize greater than or equal to the record size of the database can result in</font></font></div><div><font size="2"   ><font color="#ff0000"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;significant performance gains.</font> Use of this property for general purpose file systems is &nbsp;strongly &nbsp;discour-</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;aged, and may adversely affect performance.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;The &nbsp;size &nbsp;specified &nbsp;must &nbsp;be &nbsp;a &nbsp;power &nbsp;of two greater than or equal to 512 and less than or equal to 128</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Kbytes.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Changing the file system’s recordsize affects only files created afterward; existing files are &nbsp;unaffected.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;This property can also be referred to by its shortened column name, recsize.</font></div><p></p></pre></div><div>测试 :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># zpool create -o ashift=12 zp1 scsi-36c81f660eb17fb001b2c5fec6553ff5e</font></div><div><font size="2"   ># zfs create -o mountpoint=/data01 -o recordsize=8K -o atime=off zp1/data01</font></div><div><font size="2"   ># zfs create -o mountpoint=/data02 -o recordsize=128K -o atime=off zp1/data02</font></div><div><span style="line-height: 28px;"   ><font size="2"   ># zfs create -o mountpoint=/data03 -o recordsize=512 -o atime=off zp1/data03</font></span></div><div><font size="2"   ><span style="line-height: 21px;"   >关闭数据缓存, 不影响结果.</span></font></div><div><font size="2"   ># zfs set primarycache=metadata zp1/data01</font></div><div><font size="2"   ># zfs set primarycache=metadata zp1/data02</font></div><div><span style="line-height: 28px;"   ><font size="2"   ># zfs set primarycache=metadata zp1/data03</font></span></div><div><font size="2"   ># mkdir -p /data01/pgdata</font></div><div><font size="2"   ># mkdir -p /data02/pgdata</font></div><div><span style="line-height: 28px;"   ><font size="2"   ># mkdir -p /data03/pgdata</font></span></div><div><font size="2"   ># chown postgres:postgres /data0*/pgdata</font></div><p></p></pre></div><div>pg_test_fsync 测试结果, 512最差, 8K和128K差不多.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >512</font></div><div><div><font size="2"   >Compare file sync methods using two 8kB writes:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div></div><div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 252.052 ops/sec &nbsp; &nbsp;3967 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 248.701 ops/sec &nbsp; &nbsp;4021 usecs/op</font></div></div><div><div><font size="2"   >Non-Sync'ed 8kB writes:</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7615.510 ops/sec &nbsp; &nbsp; 131 usecs/op</font></div></div><div><font size="2"   >8K</font></div><div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 329.874 ops/sec &nbsp; &nbsp;3031 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 329.008 ops/sec &nbsp; &nbsp;3039 usecs/op</font></div></div><div><div><font size="2"   >Non-Sync'ed 8kB writes:</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 83849.214 ops/sec &nbsp; &nbsp; &nbsp;12 usecs/op</font></div></div><div><font size="2"   >128K</font></div><div><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 329.207 ops/sec &nbsp; &nbsp;3038 usecs/op</font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 328.739 ops/sec &nbsp; &nbsp;3042 usecs/op</font></div></div><div style="line-height: 28px;"   ><div style="line-height: 28px;"   ><font size="2"   >Non-Sync'ed 8kB writes:</font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 76100.311 ops/sec &nbsp; &nbsp; &nbsp;13 usecs/op</font></div></div></div><p></p></pre></div><div><br></div><div>5.2&nbsp;</div><div>压缩效率和压缩比不能兼得, 一般推荐LZ4, 压缩效率和压缩比折中.&nbsp;</div><div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp;compression=on | off | lzjb | gzip | gzip-N | zle | lz4</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Controls the compression algorithm used for this dataset. The lzjb compression algorithm is &nbsp;optimized &nbsp;for</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;performance &nbsp;while &nbsp;providing &nbsp;decent data compression. Setting compression to on uses the lzjb compression</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;algorithm.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;The gzip compression algorithm uses the same compression as the gzip(1) command. You can specify &nbsp;the &nbsp;gzip</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;level &nbsp;by using the value gzip-N where N is an integer from 1 (fastest) to 9 (best compression ratio). Cur-</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;rently, gzip is equivalent to gzip-6 (which is also the default for gzip(1)).</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;The zle (zero-length encoding) compression algorithm is a fast and simple algorithm to &nbsp;eliminate &nbsp;runs &nbsp;of</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;zeroes.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;The lz4 compression algorithm is a high-performance replacement for the lzjb algorithm. It features signif-</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;icantly faster compression and decompression, as well as a moderately higher compression ratio &nbsp;than &nbsp;lzjb,</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;but &nbsp;can &nbsp;only &nbsp;be &nbsp;used &nbsp;on &nbsp;pools with the lz4_compress feature set to enabled. See zpool-features(5) for</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;details on ZFS feature flags and the lz4_compress feature.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;This property can also be referred to by its shortened column name compress. Changing this property affects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;only newly-written data.</font></div><p></p></pre></div><div>测试, 开启压缩和不开启压缩, 效率差不多.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># zfs set compression=lz4 zp1/data02</font></div><div><div><font size="2"   ># date +%F%T; dd if=/dev/zero of=/data02/test.img ibs=1024K obs=8K count=100 oflag=nonblock,sync,noatime; date +%F%T</font></div><div><font size="2"   >2014-06-2610:59:16</font></div><div><font size="2"   >100+0 records in</font></div><div><font size="2"   >12800+0 records out</font></div><div><font size="2"   >104857600 bytes (105 MB) copied, 38.9054 s, 2.7 MB/s</font></div><div><font size="2"   >2014-06-2610:59:55</font></div></div><div><font size="2"   ><br></font></div><div><font size="2"   ># zfs set compression=off zp1/data02</font></div><div><div><font size="2"   ># date +%F%T; dd if=/dev/zero of=/data02/test.img ibs=1024K obs=8K count=100 oflag=nonblock,sync,noatime; date +%F%T</font></div><div><font size="2"   >2014-06-2611:00:08</font></div><div><font size="2"   >100+0 records in</font></div><div><font size="2"   >12800+0 records out</font></div><div><font size="2"   >104857600 bytes (105 MB) copied, 38.8295 s, 2.7 MB/s</font></div><div><font size="2"   >2014-06-2611:00:46</font></div></div><p></p></pre></div><div>开启压缩后, 需要注意一些ZFS的内核参数, l2arc可能会不能缓存压缩后的buffer. 看设置.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># modinfo zfs|grep compre</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_sync_pass_dont_compress:Don't compress starting in this pass (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_mdcomp_disable:Disable meta data compression (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; l2arc_nocompress:Skip compressing L2ARC buffers (int)</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >zio.c:int zfs_sync_pass_dont_compress = 5; /* don't compress starting in this pass */<br>zio.c:          if (pass &gt;= zfs_sync_pass_dont_compress)<br>zio.c:module_param(zfs_sync_pass_dont_compress, int, 0644);<br>zio.c:MODULE_PARM_DESC(zfs_sync_pass_dont_compress,</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >static int<br>zio_write_bp_init(zio_t *zio)<br>{</font></div><div><font size="2"   >                if (pass &gt;= zfs_sync_pass_dont_compress)<br>                        compress = ZIO_COMPRESS_OFF;</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >arc.c</font></div><div><font size="2"   >int l2arc_nocompress = B_FALSE;                 /* don't compress bufs */</font></div><p></p></pre></div><div><br></div><div>5.3&nbsp;</div><div>文件的拷贝份数, 一般不建议设置, 除非你的vdev以及底层块设备都没有使用任何冗余措施. 同样影响文件写的IOPS.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp;copies=1 | 2 | 3</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Controls &nbsp;the &nbsp;number of copies of data stored for this dataset. These copies are in addition to any redun-</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dancy provided by the pool, for example, mirroring or RAID-Z. The copies are stored on different disks, &nbsp;if</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;possible. &nbsp;The &nbsp;space &nbsp;used &nbsp;by multiple copies is charged to the associated file and dataset, changing the</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;used property and counting against quotas and reservations.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Changing this property only affects newly-written data. Therefore, set this property at &nbsp;file &nbsp;system &nbsp;cre-</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ation time by using the -o copies=N option.</font></div><p></p></pre></div></div><div>5.4&nbsp;</div><div>数据块校验, &nbsp;对IOPS有一定的影响, 但是非常不建议关闭.&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp;checksum=on | off | fletcher2,| fletcher4 | sha256</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Controls the checksum used to verify data integrity. The default value is on, which &nbsp;automatically &nbsp;selects</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;an appropriate algorithm (currently, fletcher4, but this may change in future releases). The value off dis-</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ables integrity checking on user data. Disabling checksums is NOT a recommended practice.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Changing this property affects only newly-written data.</font></div><p></p></pre></div><div><span style="line-height: 28px;"   >5.5 是否更新文件的访问时间戳, 一般建议关闭. 除非应用程序需要用到文件的访问时间戳.</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp;atime=on | off</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Controls &nbsp;whether the access time for files is updated when they are read. Turning this property off avoids</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;producing write traffic when reading files and can result in significant performance gains, though it might</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;confuse mailers and other similar utilities. The default value is on. &nbsp;See also relatime below.</font></div><p></p></pre></div><div><span style="line-height: 28px;"   >5.6&nbsp;</span></div><div><span style="line-height: 28px;"   >主缓存(ARC)配置,&nbsp;</span></div><div><span style="line-height: 28px;"   >all表示所有数据均使用ARC, none表示不使用ARC, 相当于没有缓存. metadata表示只有元数据使用缓存.</span></div><div><span style="line-height: 28px;"   >开启缓存可以极大的提高读性能, 写性能则会有一定下降(差异并不大).</span></div><div>主要影响的还是读性能, 如果关闭arc, 读的性能会非常的差.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp;primarycache=all | none | metadata</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Controls what is cached in the primary cache (ARC). If this property is set to all, then both user data and</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;metadata is cached. If this property is set to none, then neither user data nor metadata is cached. If this</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;property is set to metadata, then only metadata is cached. The default value is all.</font></div><p></p></pre></div><div>缓存的使用限制可以通过zfs内核参数来调整.</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >/sys/module/zfs/parameters/zfs_arc_grow_retry:5</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_arc_max:0</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_arc_memory_throttle_disable:1</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_arc_meta_limit:0</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_arc_meta_prune:1048576</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_arc_min:0</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_arc_min_prefetch_lifespan:1000</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_arc_p_aggressive_disable:1</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_arc_p_dampener_disable:1</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_arc_shrink_shift:5</font></div></div><div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_arc_min:Min arc size (ulong)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_arc_max:Max arc size (ulong)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_arc_meta_limit:Meta limit for arc size (ulong)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_arc_meta_prune:Bytes of meta data to prune (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_arc_grow_retry:Seconds before growing arc size (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_arc_p_aggressive_disable:disable aggressive arc_p grow (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_arc_p_dampener_disable:disable arc_p adapt dampener (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_arc_shrink_shift:log2(fraction of arc to reclaim) (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_arc_memory_throttle_disable:disable memory throttle (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_arc_min_prefetch_lifespan:Min life of prefetch block (int)</font></div></div><p></p></pre></div><div>脏数据的内存使用限制内核参数</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   ># modinfo zfs|grep dirty</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_vdev_async_write_active_max_dirty_percent:Async write concurrency max threshold (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_vdev_async_write_active_min_dirty_percent:Async write concurrency min threshold (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_dirty_data_max_percent:percent of ram can be dirty (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_dirty_data_max_max_percent:zfs_dirty_data_max upper bound as % of RAM (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_delay_min_dirty_percent:transaction delay threshold (int)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_dirty_data_max:determines the dirty space limit (ulong)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_dirty_data_max_max:zfs_dirty_data_max upper bound in bytes (ulong)</font></div><div><font size="2"   >parm: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zfs_dirty_data_sync:sync txg when this much dirty data (ulong)</font></div></div><div><div><font size="2"   ># grep ".*" /sys/module/zfs/parameters/*|grep dirty</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_delay_min_dirty_percent:60</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_dirty_data_max:3361508147</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_dirty_data_max_max:8403770368</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_dirty_data_max_max_percent:25</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_dirty_data_max_percent:10</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_dirty_data_sync:67108864</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_vdev_async_write_active_max_dirty_percent:60</font></div><div><font size="2"   >/sys/module/zfs/parameters/zfs_vdev_async_write_active_min_dirty_percent:30</font></div></div><p></p></pre></div><div>测试,&nbsp;<span style="line-height: 28px;"   >异步读写, primarycache=metadata的写入速度要快一点, 一般在cache填满后cache=metadata和cache=all速度达到一致.</span></div><div><span style="line-height: 28px;"   >zpool块设备越多, 差别越明显. 通过zpool iostat -v 1来查看.</span></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># zpool create -o ashift=12 -o autoreplace=on zp1 scsi-36c81f660eb17fb001b2c5fec6553ff5e scsi-36c81f660eb17fb001b2c5ff465cff3ed scsi-36c81f660eb17fb001b2c5ffa662f3df2 scsi-36c81f660eb17fb001b2c5fff66848a6c scsi-36c81f660eb17fb001b2c600466cb5810 scsi-36c81f660eb17fb001b2c60096714bcf2 scsi-36c81f660eb17fb001b2c600e6761a9bd scsi-36c81f660eb17fb001b2c601267a63fcc scsi-36c81f660eb17fb001b2c601867f2c341 &nbsp;scsi-36c81f660eb17fb001b2c601e685414b5 scsi-36c81f660eb17fb001b2c602368a21621 scsi-36c81f660eb17fb001b2c602a690a4ed8</font></div><div><font size="2"   ><br></font></div><div><font size="2"   ># zfs create -o mountpoint=/data01 -o atime=off -o primarycache=metadata zp1/data01</font></div><div><div><font size="2"   ># dd if=/dev/zero of=/data01/test.img bs=1024K count=819200</font></div><div><font size="2"   >^C185116+0 records in</font></div><div><font size="2"   >185116+0 records out</font></div><div><font size="2"   >194108194816 bytes (194 GB) copied, 113.589 s, 1.7 GB/s</font></div></div><div><font size="2"   ><br></font></div><div><font size="2"   ># zfs destroy zp1/data01</font></div><div><font size="2"   ># zfs create -o mountpoint=/data01 -o atime=off -o primarycache=all zp1/data01</font></div><div><div><font size="2"   ># dd if=/dev/zero of=/data01/test.img bs=1024K count=819200</font></div><div><font size="2"   >^C147262+0 records in</font></div><div><font size="2"   >147262+0 records out</font></div><div><font size="2"   >154415398912 bytes (154 GB) copied, 90.1703 s, 1.7 GB/s</font></div></div><p></p></pre></div><div>读性能测试, 关闭arc后, 性能非常差, 目前还不清楚是否可以通过调整zfs内核参数来提高直接的块设备的读性能.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># zfs set primarycache=metadata zp1/data01</font></div><div><font size="2"   ># cp /data01/test.img /data01/test.img1</font></div><div><div><font size="2"   ># zpool iostat -v 1</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;capacity &nbsp; &nbsp; operations &nbsp; &nbsp;bandwidth</font></div><div><font size="2"   >pool &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;alloc &nbsp; free &nbsp; read &nbsp;write &nbsp; read &nbsp;write</font></div><div><font size="2"   >---------------------------------------- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;-----</font></div><div><font size="2"   >zp1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 80.5G &nbsp;43.4T &nbsp; &nbsp;289 &nbsp; &nbsp;592 &nbsp;35.9M &nbsp;64.5M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c5fec6553ff5e &nbsp;6.72G &nbsp;3.62T &nbsp; &nbsp; 23 &nbsp; &nbsp; 44 &nbsp;3.00M &nbsp;5.49M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c5ff465cff3ed &nbsp;6.69G &nbsp;3.62T &nbsp; &nbsp; 24 &nbsp; &nbsp; 44 &nbsp;3.12M &nbsp;5.49M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c5ffa662f3df2 &nbsp;6.71G &nbsp;3.62T &nbsp; &nbsp; 24 &nbsp; &nbsp; 49 &nbsp;3.00M &nbsp;5.76M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c5fff66848a6c &nbsp;6.72G &nbsp;3.62T &nbsp; &nbsp; 23 &nbsp; &nbsp; 44 &nbsp;3.00M &nbsp;5.01M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c600466cb5810 &nbsp;6.70G &nbsp;3.62T &nbsp; &nbsp; 24 &nbsp; &nbsp; 62 &nbsp;3.12M &nbsp;5.54M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c60096714bcf2 &nbsp;6.69G &nbsp;3.62T &nbsp; &nbsp; 21 &nbsp; &nbsp; 54 &nbsp;2.75M &nbsp;5.15M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c600e6761a9bd &nbsp;6.71G &nbsp;3.62T &nbsp; &nbsp; 27 &nbsp; &nbsp; 53 &nbsp;3.37M &nbsp;5.35M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c601267a63fcc &nbsp;6.71G &nbsp;3.62T &nbsp; &nbsp; 21 &nbsp; &nbsp; 46 &nbsp;2.75M &nbsp;4.90M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c601867f2c341 &nbsp;6.68G &nbsp;3.62T &nbsp; &nbsp; 22 &nbsp; &nbsp; 46 &nbsp;2.87M &nbsp;5.02M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c601e685414b5 &nbsp;6.74G &nbsp;3.62T &nbsp; &nbsp; 25 &nbsp; &nbsp; 54 &nbsp;3.24M &nbsp;5.90M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c602368a21621 &nbsp;6.71G &nbsp;3.62T &nbsp; &nbsp; 23 &nbsp; &nbsp; 43 &nbsp;3.00M &nbsp;5.49M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c602a690a4ed8 &nbsp;6.69G &nbsp;3.62T &nbsp; &nbsp; 21 &nbsp; &nbsp; 42 &nbsp;2.75M &nbsp;5.37M</font></div><div><font size="2"   >cache &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;-</font></div><div><font size="2"   >&nbsp; pcie-shannon-6819246149b014-part1 &nbsp; &nbsp; &nbsp; 5.14M &nbsp; 800G &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;1 &nbsp; &nbsp; &nbsp;0 &nbsp;68.9K</font></div><div><font size="2"   >---------------------------------------- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;-----</font></div></div><p></p></pre></div><div>开启arc后, 读性能提升, 注意看读的iops增加到300+, 开启arc前只有20+</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># zfs set primarycache=all zp1/data01</font></div><div><div><font size="2"   ># cp /data01/test.img /data01/test.img1</font></div><div><font size="2"   >cp: overwrite `/data01/test.img1'? y</font></div></div><div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;capacity &nbsp; &nbsp; operations &nbsp; &nbsp;bandwidth</font></div><div><font size="2"   >pool &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;alloc &nbsp; free &nbsp; read &nbsp;write &nbsp; read &nbsp;write</font></div><div><font size="2"   >---------------------------------------- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;-----</font></div><div><font size="2"   >zp1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 82.8G &nbsp;43.4T &nbsp;3.54K &nbsp;4.01K &nbsp; 449M &nbsp; 476M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c5fec6553ff5e &nbsp;6.91G &nbsp;3.62T &nbsp; &nbsp;318 &nbsp; &nbsp;318 &nbsp;39.6M &nbsp;39.6M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c5ff465cff3ed &nbsp;6.89G &nbsp;3.62T &nbsp; &nbsp;286 &nbsp; &nbsp;328 &nbsp;35.6M &nbsp;40.2M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c5ffa662f3df2 &nbsp;6.91G &nbsp;3.62T &nbsp; &nbsp;304 &nbsp; &nbsp;335 &nbsp;37.9M &nbsp;39.6M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c5fff66848a6c &nbsp;6.92G &nbsp;3.62T &nbsp; &nbsp;299 &nbsp; &nbsp;335 &nbsp;37.3M &nbsp;40.3M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c600466cb5810 &nbsp;6.89G &nbsp;3.62T &nbsp; &nbsp;288 &nbsp; &nbsp;322 &nbsp;35.5M &nbsp;37.1M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c60096714bcf2 &nbsp;6.89G &nbsp;3.62T &nbsp; &nbsp;300 &nbsp; &nbsp;337 &nbsp;37.3M &nbsp;39.4M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c600e6761a9bd &nbsp;6.90G &nbsp;3.62T &nbsp; &nbsp;305 &nbsp; &nbsp;330 &nbsp;37.9M &nbsp;39.0M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c601267a63fcc &nbsp;6.90G &nbsp;3.62T &nbsp; &nbsp;294 &nbsp; &nbsp;343 &nbsp;36.8M &nbsp;40.1M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c601867f2c341 &nbsp;6.88G &nbsp;3.62T &nbsp; &nbsp;300 &nbsp; &nbsp;373 &nbsp;36.8M &nbsp;39.5M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c601e685414b5 &nbsp;6.94G &nbsp;3.62T &nbsp; &nbsp;321 &nbsp; &nbsp;374 &nbsp;39.7M &nbsp;40.4M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c602368a21621 &nbsp;6.90G &nbsp;3.62T &nbsp; &nbsp;292 &nbsp; &nbsp;365 &nbsp;36.4M &nbsp;39.6M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c602a690a4ed8 &nbsp;6.89G &nbsp;3.62T &nbsp; &nbsp;308 &nbsp; &nbsp;339 &nbsp;38.2M &nbsp;41.2M</font></div><div><font size="2"   >cache &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;-</font></div><div><font size="2"   >&nbsp; pcie-shannon-6819246149b014-part1 &nbsp; &nbsp; &nbsp; &nbsp;454M &nbsp; 800G &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp;649 &nbsp; &nbsp; &nbsp;0 &nbsp;79.5M</font></div><div><font size="2"   >---------------------------------------- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;-----</font></div></div><p></p></pre></div><div><br></div><div>5.7&nbsp;</div><div>二级缓存(L2ARC)配置, 即zpool 中的cache设备.</div><div>如果要使用L2ARC的话, 建议使用SSD作为L2ARC.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp;secondarycache=all | none | metadata</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Controls what is cached in the secondary cache (L2ARC). If this property is set to all, then both user data</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and metadata is cached. If this property is set to none, then neither user data nor metadata is cached. &nbsp;If</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;this property is set to metadata, then only metadata is cached. The default value is all.</font></div><p></p></pre></div><div>l2arc的数据从arc的mru, mfu表取到, 所以arc如果关闭的话, l2arc也不会有缓存数据.&nbsp;</div><div>所以如果要使用l2arc的话, 务必同时打开arc和l2arc.</div><div>l2arc里面不存储脏数据, 所以对于活跃数据频繁变更的业务, L2ARC几乎没什么用处.</div><div><br></div><div><div>5.8&nbsp;</div><div>数据块去重配置, 对于大多数场景没有什么效果, 而且如果数据集很大的话需要耗费大量的内存. 同时影响IOPS和吞吐量.</div><div>一般不建议开启.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp;dedup=on | off | verify | sha256[,verify]</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Controls &nbsp;whether &nbsp;deduplication is in effect for a dataset. The default value is off. The default checksum</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;used for deduplication is sha256 (subject to change). When dedup is enabled, the dedup &nbsp;checksum &nbsp;algorithm</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;overrides the checksum property. Setting the value to verify is equivalent to specifying sha256,verify.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If &nbsp;the &nbsp;property &nbsp;is set to verify, then, whenever two blocks have the same signature, ZFS will do a byte-</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for-byte comparison with the existing block to ensure that the contents are identical.</font></div><p></p></pre></div></div><div><br></div><div><div>5.9&nbsp;</div><div>ZIL的使用配置, 对同步写请求来说, &nbsp;latency表示使用ZIL设备, throughput表示不使用zil设备(非常不推荐).</div><div>如果使用PostgreSQL数据库, 并且使用异步事务提交的话, 是否使用zil关系都不大.</div><div>zil要求IOPS能力很好的设备, 才能达到好的同步写请求iops.&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp;logbias = latency | throughput</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Provide &nbsp;a hint to ZFS about handling of synchronous requests in this dataset. If logbias is set to latency</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(the default), ZFS will use pool log devices (if configured) to handle the requests at low latency. If log-</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;bias &nbsp;is &nbsp;set &nbsp;to &nbsp;throughput, ZFS will not use configured pool log devices. ZFS will instead optimize syn-</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;chronous operations for global pool throughput and efficient use of resources.</font></div><p></p></pre></div></div><div>首先我们测试一个有SSD zil设备的, 普通机械硬盘12块组成的一个ZPOOL的fsync场景性能.</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   ># zfs get all|grep logbias</font></div><div><font size="2"   >zp1 &nbsp; &nbsp; &nbsp; &nbsp; logbias &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; latency &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default</font></div><div><font size="2"   >zp1/data01 &nbsp;logbias &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; latency &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default</font></div></div><div><font size="2"   ><br></font></div><div><div><font size="2"   >&gt; pg_test_fsync -f /data01/pgdata/1</font></div><div><font size="2"   >5 seconds per test</font></div><div><font size="2"   >O_DIRECT supported on this platform for open_datasync and open_sync.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare file sync methods using one 8kB write:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7285.416 ops/sec &nbsp; &nbsp; 137 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7359.841 ops/sec &nbsp; &nbsp; 136 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >* This file system and its mount options do not support direct</font></div><div><font size="2"   >I/O, e.g. ext4 in journaled mode.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare file sync methods using two 8kB writes:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5396.851 ops/sec &nbsp; &nbsp; 185 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4323.672 ops/sec &nbsp; &nbsp; 231 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >* This file system and its mount options do not support direct</font></div><div><font size="2"   >I/O, e.g. ext4 in journaled mode.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare open_sync with different write sizes:</font></div><div><font size="2"   >(This is designed to compare the cost of writing 16kB</font></div><div><font size="2"   >in different write open_sync sizes.)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1 * 16kB open_sync write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2 * &nbsp;8kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4 * &nbsp;4kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8 * &nbsp;2kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; 16 * &nbsp;1kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Test if fsync on non-write file descriptor is honored:</font></div><div><font size="2"   >(If the times are similar, fsync() can sync data written</font></div><div><font size="2"   >on a different descriptor.)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write, fsync, close &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5859.650 ops/sec &nbsp; &nbsp; 171 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write, close, fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6626.115 ops/sec &nbsp; &nbsp; 151 usecs/op</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Non-Sync'ed 8kB writes:</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 82388.939 ops/sec &nbsp; &nbsp; &nbsp;12 usecs/op</font></div></div><p></p></pre></div><div>注意此时ZIL所在的SSD硬盘的利用率没有到100%, 处于一个比较低的水平.</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >avg-cpu: &nbsp;%user &nbsp; %nice %system %iowait &nbsp;%steal &nbsp; %idle</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; 12.39 &nbsp; &nbsp;5.75 &nbsp; &nbsp;0.00 &nbsp; 81.86</font></div></div><div><font size="2"   >dfa &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 7401.00 &nbsp; &nbsp; 0.00 177624.00 &nbsp; &nbsp;24.00 &nbsp; &nbsp; 0.24 &nbsp; &nbsp;0.03 &nbsp; 0.03 &nbsp;24.10</font></div><p></p></pre></div><div>使用zpool iostat看到fsync调用使用了zil设备.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># zpool iostat -v 1</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;capacity &nbsp; &nbsp; operations &nbsp; &nbsp;bandwidth</font></div><div><font size="2"   >pool &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;alloc &nbsp; free &nbsp; read &nbsp;write &nbsp; read &nbsp;write</font></div><div><font size="2"   >---------------------------------------- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;-----</font></div><div><font size="2"   >zp1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;160G &nbsp;43.3T &nbsp; &nbsp; &nbsp;0 &nbsp;7.23K &nbsp; &nbsp; &nbsp;0 &nbsp;86.7M</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c5fec6553ff5e &nbsp;13.4G &nbsp;3.61T &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c5ff465cff3ed &nbsp;13.4G &nbsp;3.61T &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c5ffa662f3df2 &nbsp;13.3G &nbsp;3.61T &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c5fff66848a6c &nbsp;13.4G &nbsp;3.61T &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c600466cb5810 &nbsp;13.3G &nbsp;3.61T &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c60096714bcf2 &nbsp;13.3G &nbsp;3.61T &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c600e6761a9bd &nbsp;13.3G &nbsp;3.61T &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c601267a63fcc &nbsp;13.3G &nbsp;3.61T &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c601867f2c341 &nbsp;13.3G &nbsp;3.61T &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c601e685414b5 &nbsp;13.4G &nbsp;3.61T &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c602368a21621 &nbsp;13.3G &nbsp;3.61T &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >&nbsp; scsi-36c81f660eb17fb001b2c602a690a4ed8 &nbsp;13.3G &nbsp;3.61T &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >logs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;-</font></div><div><font size="2"   >&nbsp; pcie-shannon-6819246149b014-part2 &nbsp; &nbsp; &nbsp; &nbsp;976M &nbsp;1.03G &nbsp; &nbsp; &nbsp;0 &nbsp;7.23K &nbsp; &nbsp; &nbsp;0 &nbsp;86.7M</font></div><div><font size="2"   >cache &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp;-</font></div><div><font size="2"   >&nbsp; pcie-shannon-6819246149b014-part1 &nbsp; &nbsp; &nbsp; 2.03M &nbsp; 800G &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0 &nbsp; &nbsp; &nbsp;0</font></div><div><font size="2"   >---------------------------------------- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;----- &nbsp;-----</font></div><p></p></pre></div><div>接下来把这个zfs的logbias改成throughput, 也就是不使用zil设备, fsync的性能马上下降了.</div><div>这里实际上VDEV块设备的iops利用率还不到20%, FreeBSD下面没有问题, 这是ZFSonLinux的一个问题, 已提交brian, 得到的回复如下.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Thanks,</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >I've opened a new issue so we can track this.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >https://github.com/zfsonlinux/zfs/issues/2431</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >The next step is somebody is going to have to profile the Linux case to&nbsp;</font></div><div><font size="2"   >see what's going on. &nbsp;It seems like we're blocking somewhere in the&nbsp;</font></div><div><font size="2"   >stack unnecessarily. &nbsp;Unfortunately, all the developers are swamped so&nbsp;</font></div><div><font size="2"   >I'm not sure when someone will get a chance to look at this. &nbsp;If your&nbsp;</font></div><div><font size="2"   >interested in getting some additional profiling data I'd suggest&nbsp;</font></div><div><font size="2"   >starting with getting a call graph of fsync() using ftrace. &nbsp;That should&nbsp;</font></div><div><font size="2"   >show us where the time is going.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >http://lwn.net/Articles/370423/</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Thanks,</font></div><div><font size="2"   >Brian</font></div><p></p></pre></div><div><br></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># zfs set logbias=throughput zp1/data01</font></div><div><div><font size="2"   >&gt; pg_test_fsync -f /data01/pgdata/1</font></div><div><font size="2"   >5 seconds per test</font></div><div><font size="2"   >O_DIRECT supported on this platform for open_datasync and open_sync.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare file sync methods using one 8kB write:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 330.846 ops/sec &nbsp; &nbsp;3023 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 329.942 ops/sec &nbsp; &nbsp;3031 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >* This file system and its mount options do not support direct</font></div><div><font size="2"   >I/O, e.g. ext4 in journaled mode.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare file sync methods using two 8kB writes:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 329.407 ops/sec &nbsp; &nbsp;3036 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 329.606 ops/sec &nbsp; &nbsp;3034 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >* This file system and its mount options do not support direct</font></div><div><font size="2"   >I/O, e.g. ext4 in journaled mode.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare open_sync with different write sizes:</font></div><div><font size="2"   >(This is designed to compare the cost of writing 16kB</font></div><div><font size="2"   >in different write open_sync sizes.)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1 * 16kB open_sync write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2 * &nbsp;8kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4 * &nbsp;4kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8 * &nbsp;2kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; 16 * &nbsp;1kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Test if fsync on non-write file descriptor is honored:</font></div><div><font size="2"   >(If the times are similar, fsync() can sync data written</font></div><div><font size="2"   >on a different descriptor.)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write, fsync, close &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 324.344 ops/sec &nbsp; &nbsp;3083 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write, close, fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 329.272 ops/sec &nbsp; &nbsp;3037 usecs/op</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Non-Sync'ed 8kB writes:</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 84914.324 ops/sec &nbsp; &nbsp; &nbsp;12 usecs/op</font></div></div><p></p></pre></div><div>如果直接用SSD建立ZPOOL, 它的fsync性能如何呢? 和前面一个VDEVS使用机械硬盘+ZIL SSD性能基本一致.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># zpool destroy zp1</font></div><div><font size="2"   ># zpool create -o ashift=12 zp1 pcie-shannon-6819246149b014-part1</font></div><div><font size="2"   ># zfs create -o mountpoint=/data01 zp1/data01</font></div><div><font size="2"   ># mkdir /data01/pgdata</font></div><div><font size="2"   ># chown postgres:postgres /data01/pgdata</font></div><div><div><font size="2"   >&gt; pg_test_fsync -f /data01/pgdata/1</font></div><div><font size="2"   >5 seconds per test</font></div><div><font size="2"   >O_DIRECT supported on this platform for open_datasync and open_sync.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare file sync methods using one 8kB write:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6604.779 ops/sec &nbsp; &nbsp; 151 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7086.614 ops/sec &nbsp; &nbsp; 141 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >* This file system and its mount options do not support direct</font></div><div><font size="2"   >I/O, e.g. ext4 in journaled mode.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare file sync methods using two 8kB writes:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5760.927 ops/sec &nbsp; &nbsp; 174 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5677.560 ops/sec &nbsp; &nbsp; 176 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >* This file system and its mount options do not support direct</font></div><div><font size="2"   >I/O, e.g. ext4 in journaled mode.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare open_sync with different write sizes:</font></div><div><font size="2"   >(This is designed to compare the cost of writing 16kB</font></div><div><font size="2"   >in different write open_sync sizes.)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1 * 16kB open_sync write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2 * &nbsp;8kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4 * &nbsp;4kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8 * &nbsp;2kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; 16 * &nbsp;1kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Test if fsync on non-write file descriptor is honored:</font></div><div><font size="2"   >(If the times are similar, fsync() can sync data written</font></div><div><font size="2"   >on a different descriptor.)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write, fsync, close &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6561.159 ops/sec &nbsp; &nbsp; 152 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write, close, fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6530.990 ops/sec &nbsp; &nbsp; 153 usecs/op</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Non-Sync'ed 8kB writes:</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 81261.194 ops/sec &nbsp; &nbsp; &nbsp;12 usecs/op</font></div></div><p></p></pre></div><div>如果不使用ZFS, 直接使用EXT4的话, 性能如何呢?</div><div>此时底层块设备的利用率明显提升.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># mkfs.ext4 /dev/disk/by-id/pcie-shannon-6819246149b014-part2</font></div><div><font size="2"   ># mount /dev/disk/by-id/pcie-shannon-6819246149b014-part2 /mnt</font></div><div><font size="2"   ># chmod 777 /mnt</font></div><div><div><font size="2"   >ing one 8kB write:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 38533.583 ops/sec &nbsp; &nbsp; &nbsp;26 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 29027.342 ops/sec &nbsp; &nbsp; &nbsp;34 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 26695.490 ops/sec &nbsp; &nbsp; &nbsp;37 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 43047.350 ops/sec &nbsp; &nbsp; &nbsp;23 usecs/op</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare file sync methods using two 8kB writes:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 23826.738 ops/sec &nbsp; &nbsp; &nbsp;42 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 31193.925 ops/sec &nbsp; &nbsp; &nbsp;32 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 29445.494 ops/sec &nbsp; &nbsp; &nbsp;34 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 22241.529 ops/sec &nbsp; &nbsp; &nbsp;45 usecs/op</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare open_sync with different write sizes:</font></div><div><font size="2"   >(This is designed to compare the cost of writing 16kB</font></div><div><font size="2"   >in different write open_sync sizes.)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1 * 16kB open_sync write &nbsp; &nbsp; &nbsp; 34597.675 ops/sec &nbsp; &nbsp; &nbsp;29 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2 * &nbsp;8kB open_sync writes &nbsp; &nbsp; &nbsp;22051.151 ops/sec &nbsp; &nbsp; &nbsp;45 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4 * &nbsp;4kB open_sync writes &nbsp; &nbsp; &nbsp;11751.948 ops/sec &nbsp; &nbsp; &nbsp;85 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8 * &nbsp;2kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp;804.951 ops/sec &nbsp; &nbsp;1242 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; 16 * &nbsp;1kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp;403.788 ops/sec &nbsp; &nbsp;2477 usecs/op</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Test if fsync on non-write file descriptor is honored:</font></div><div><font size="2"   >(If the times are similar, fsync() can sync data written</font></div><div><font size="2"   >on a different descriptor.)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write, fsync, close &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 18227.669 ops/sec &nbsp; &nbsp; &nbsp;55 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write, close, fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 18158.735 ops/sec &nbsp; &nbsp; &nbsp;55 usecs/op</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Non-Sync'ed 8kB writes:</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 288696.375 ops/sec &nbsp; &nbsp; &nbsp; 3 usecs/op</font></div></div><div><font size="2"   ><span style="line-height: 21px;"   >iostat看到此时的SSD设备利用率提高.</span></font></div><div><font size="2"   >dfa               0.00     0.00    0.00 55244.00     0.00 441952.00     8.00     1.30    0.02   0.01  78.10</font></div><p></p></pre></div><div>ZVOL+EXT4的性能&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># zfs create -V 10G zp1/data02</font></div><div><font size="2"   ># mkfs.ext4 /dev/zd0</font></div><div><font size="2"   ># mount /dev/zd0 /tmp</font></div><div><font size="2"   ># chmod 777 /tmp</font></div><div><font size="2"   >结果也不理想</font></div><div><font size="2"   ><div>&gt; pg_test_fsync -f /tmp/1</div><div>5 seconds per test</div><div>O_DIRECT supported on this platform for open_datasync and open_sync.</div><div><br></div><div>Compare file sync methods using one 8kB write:</div><div>(in wal_sync_method preference order, except fdatasync</div><div>is Linux's default)</div><div>&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5221.004 ops/sec &nbsp; &nbsp; 192 usecs/op</div><div>&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4770.779 ops/sec &nbsp; &nbsp; 210 usecs/op</div><div>&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2523.113 ops/sec &nbsp; &nbsp; 396 usecs/op</div><div>&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</div><div>&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5527.120 ops/sec &nbsp; &nbsp; 181 usecs/op</div><div><br></div><div>Compare file sync methods using two 8kB writes:</div><div>(in wal_sync_method preference order, except fdatasync</div><div>is Linux's default)</div><div>&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2740.871 ops/sec &nbsp; &nbsp; 365 usecs/op</div><div>&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3774.486 ops/sec &nbsp; &nbsp; 265 usecs/op</div><div>&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1927.523 ops/sec &nbsp; &nbsp; 519 usecs/op</div><div>&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</div><div>&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2747.225 ops/sec &nbsp; &nbsp; 364 usecs/op</div><div><br></div><div>Compare open_sync with different write sizes:</div><div>(This is designed to compare the cost of writing 16kB</div><div>in different write open_sync sizes.)</div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1 * 16kB open_sync write &nbsp; &nbsp; &nbsp; &nbsp;4751.333 ops/sec &nbsp; &nbsp; 210 usecs/op</div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2 * &nbsp;8kB open_sync writes &nbsp; &nbsp; &nbsp; 2729.912 ops/sec &nbsp; &nbsp; 366 usecs/op</div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4 * &nbsp;4kB open_sync writes &nbsp; &nbsp; &nbsp; 1387.512 ops/sec &nbsp; &nbsp; 721 usecs/op</div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8 * &nbsp;2kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp;734.417 ops/sec &nbsp; &nbsp;1362 usecs/op</div><div>&nbsp; &nbsp; &nbsp; &nbsp; 16 * &nbsp;1kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp;364.665 ops/sec &nbsp; &nbsp;2742 usecs/op</div><div><br></div><div>Test if fsync on non-write file descriptor is honored:</div><div>(If the times are similar, fsync() can sync data written</div><div>on a different descriptor.)</div><div>&nbsp; &nbsp; &nbsp; &nbsp; write, fsync, close &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3134.067 ops/sec &nbsp; &nbsp; 319 usecs/op</div><div>&nbsp; &nbsp; &nbsp; &nbsp; write, close, fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3486.530 ops/sec &nbsp; &nbsp; 287 usecs/op</div><div><br></div><div>Non-Sync'ed 8kB writes:</div><div>&nbsp; &nbsp; &nbsp; &nbsp; write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 293944.412 ops/sec &nbsp; &nbsp; &nbsp; 3 usecs/op</div></font></div><p></p></pre></div><div>对比以上几种情况, ZFS没有发挥出底层设备的FSYNC能力, 而直接使用块设备+ext4有明显改善, 不知道是不是zfs在Linux下的效率问题, 还是需要调整某些ZFS内核参数? 后面我使用FreeBSD进行一下测试看看是不是有同样的情况.</div><div>FreeBSD的性能很好, 基本达到块设备的瓶颈. 如下 :&nbsp;</div><div><a target="_blank" href="http://blog.163.com/digoal@126/blog/static/16387704020145264116819/"   >http://blog.163.com/digoal@126/blog/static/16387704020145264116819/</a></div><div><br></div><div>5.10 同步接口调用的操作. 不推荐关闭, 关闭可能导致异常后数据丢失. 因为某些应用程序如数据库的一些操作, 希望调用fsync后数据确实写入了非易失存储. 而关闭sync的话, 显然和应用程序的期望实际不符.&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp;sync=standard | always | disabled</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Controls &nbsp;the &nbsp;behavior &nbsp;of &nbsp;synchronous &nbsp;requests &nbsp;(e.g. fsync, O_DSYNC). &nbsp;standard is the POSIX specified</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;behavior of ensuring all synchronous requests are written to stable storage and all devices are flushed &nbsp;to</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ensure &nbsp;data &nbsp;is &nbsp;not &nbsp;cached &nbsp;by device controllers (this is the default). always causes every file system</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transaction to be written and flushed before its system call returns. This has a large performance penalty.</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;disabled disables synchronous requests. File system transactions are only committed to stable storage peri-</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;odically. This option will give the highest performance. &nbsp;However, it is very dangerous &nbsp;as &nbsp;ZFS &nbsp;would &nbsp;be</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ignoring &nbsp;the &nbsp;synchronous &nbsp;transaction &nbsp;demands &nbsp;of applications such as databases or NFS. &nbsp;Administrators</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;should only use this option when the risks are understood.</font></div><p></p></pre></div><div>下面测试一下关闭sync后的性能, 虽然我们非常不建议这么做, 但是提供一下测试结果.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># zfs set sync=disabled zp1/data01</font></div><div><div><font size="2"   ># zfs get all|grep cache</font></div><div><font size="2"   >zp1 &nbsp; &nbsp; &nbsp; &nbsp; primarycache &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;all &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default</font></div><div><font size="2"   >zp1 &nbsp; &nbsp; &nbsp; &nbsp; secondarycache &nbsp; &nbsp; &nbsp; &nbsp;all &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default</font></div><div><font size="2"   >zp1/data01 &nbsp;primarycache &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;all &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default</font></div><div><font size="2"   >zp1/data01 &nbsp;secondarycache &nbsp; &nbsp; &nbsp; &nbsp;all &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default</font></div></div><div><div><font size="2"   >&gt; pg_test_fsync -f /data01/pgdata/1</font></div><div><font size="2"   >5 seconds per test</font></div><div><font size="2"   >O_DIRECT supported on this platform for open_datasync and open_sync.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare file sync methods using one 8kB write:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 109380.512 ops/sec &nbsp; &nbsp; &nbsp; 9 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 115186.570 ops/sec &nbsp; &nbsp; &nbsp; 9 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >* This file system and its mount options do not support direct</font></div><div><font size="2"   >I/O, e.g. ext4 in journaled mode.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare file sync methods using two 8kB writes:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 60158.540 ops/sec &nbsp; &nbsp; &nbsp;17 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 60352.231 ops/sec &nbsp; &nbsp; &nbsp;17 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >* This file system and its mount options do not support direct</font></div><div><font size="2"   >I/O, e.g. ext4 in journaled mode.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare open_sync with different write sizes:</font></div><div><font size="2"   >(This is designed to compare the cost of writing 16kB</font></div><div><font size="2"   >in different write open_sync sizes.)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1 * 16kB open_sync write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2 * &nbsp;8kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4 * &nbsp;4kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8 * &nbsp;2kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; 16 * &nbsp;1kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Test if fsync on non-write file descriptor is honored:</font></div><div><font size="2"   >(If the times are similar, fsync() can sync data written</font></div><div><font size="2"   >on a different descriptor.)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write, fsync, close &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 75829.757 ops/sec &nbsp; &nbsp; &nbsp;13 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write, close, fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 75501.094 ops/sec &nbsp; &nbsp; &nbsp;13 usecs/op</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Non-Sync'ed 8kB writes:</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 94328.592 ops/sec &nbsp; &nbsp; &nbsp;11 usecs/op</font></div></div><p></p></pre></div><div>关闭sync后, 其实和cache没有什么关系, 即使同时关闭cache性能依旧彪悍.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   ># zfs set primarycache=none zp1/data01</font></div><div><div><font size="2"   >&gt; pg_test_fsync -f /data01/pgdata/1</font></div><div><font size="2"   >5 seconds per test</font></div><div><font size="2"   >O_DIRECT supported on this platform for open_datasync and open_sync.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare file sync methods using one 8kB write:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 115321.769 ops/sec &nbsp; &nbsp; &nbsp; 9 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 115119.262 ops/sec &nbsp; &nbsp; &nbsp; 9 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >* This file system and its mount options do not support direct</font></div><div><font size="2"   >I/O, e.g. ext4 in journaled mode.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare file sync methods using two 8kB writes:</font></div><div><font size="2"   >(in wal_sync_method preference order, except fdatasync</font></div><div><font size="2"   >is Linux's default)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_datasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fdatasync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 60296.171 ops/sec &nbsp; &nbsp; &nbsp;17 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 60201.468 ops/sec &nbsp; &nbsp; &nbsp;17 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; fsync_writethrough &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; open_sync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >* This file system and its mount options do not support direct</font></div><div><font size="2"   >I/O, e.g. ext4 in journaled mode.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Compare open_sync with different write sizes:</font></div><div><font size="2"   >(This is designed to compare the cost of writing 16kB</font></div><div><font size="2"   >in different write open_sync sizes.)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1 * 16kB open_sync write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2 * &nbsp;8kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4 * &nbsp;4kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8 * &nbsp;2kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; 16 * &nbsp;1kB open_sync writes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n/a*</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Test if fsync on non-write file descriptor is honored:</font></div><div><font size="2"   >(If the times are similar, fsync() can sync data written</font></div><div><font size="2"   >on a different descriptor.)</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write, fsync, close &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 75542.879 ops/sec &nbsp; &nbsp; &nbsp;13 usecs/op</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write, close, fsync &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 75654.249 ops/sec &nbsp; &nbsp; &nbsp;13 usecs/op</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Non-Sync'ed 8kB writes:</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; write &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 95557.532 ops/sec &nbsp; &nbsp; &nbsp;10 usecs/op</font></div></div><p></p></pre></div><div><br></div><div>6. zfs模块内核参数也会极大的影响性能.</div><div>参见</div><div><a target="_blank" href="http://blog.163.com/digoal@126/blog/static/16387704020145253599111/"   >http://blog.163.com/digoal@126/blog/static/16387704020145253599111/</a></div><div><br></div>[参考]<wbr><div>1. zfs source</div><div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >2. man zpool</div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >3. man zfs</div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >4. man zdb</div></div><div>5.&nbsp;<a style="line-height: 28px;" target="_blank" href="http://blog.163.com/digoal@126/blog/static/1638770402014525103556357/"   >http://blog.163.com/digoal@126/blog/static/1638770402014525103556357/</a></div><div>6.&nbsp;<a style="line-height: 28px;" target="_blank" href="http://blog.163.com/digoal@126/blog/static/1638770402014525111238683/"   >http://blog.163.com/digoal@126/blog/static/1638770402014525111238683/</a></div><div>7.&nbsp;<a style="line-height: 28px;" target="_blank" href="http://blog.163.com/digoal@126/blog/static/16387704020145253599111/"   >http://blog.163.com/digoal@126/blog/static/16387704020145253599111/</a></div><div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >8.&nbsp;<a style="line-height: 28px; text-decoration: none; color: rgb(85, 108, 136);" target="_blank" rel="nofollow" href="http://fixunix.com/solaris-rss/579853-choosing-stripsize-lun-recordsize-zfs-postgresql.html"   >http://fixunix.com/solaris-rss/579853-choosing-stripsize-lun-recordsize-zfs-postgresql.html</a></div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >9.&nbsp;<a style="line-height: 28px; text-decoration: none; color: rgb(85, 108, 136);" target="_blank" rel="nofollow" href="https://github.com/zfsonlinux/zfs/blob/master/cmd/zpool/zpool_vdev.c#L108"   >https://github.com/zfsonlinux/zfs/blob/master/cmd/zpool/zpool_vdev.c#L108</a></div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >10.&nbsp;<a style="line-height: 28px; text-decoration: none; color: rgb(85, 108, 136);" target="_blank" rel="nofollow" href="http://blog.delphix.com/matt/2014/06/06/zfs-stripe-width/"   >http://blog.delphix.com/matt/2014/06/06/zfs-stripe-width/</a></div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >11.&nbsp;<a style="line-height: 28px; text-decoration: none; color: rgb(85, 108, 136);" target="_blank" rel="nofollow" href="http://open-zfs.org/wiki/Performance_tuning"   >http://open-zfs.org/wiki/Performance_tuning</a></div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >12.&nbsp;<a style="line-height: 28px; text-decoration: none; color: rgb(85, 108, 136);" target="_blank" rel="nofollow" href="https://pthree.org/2013/01/03/zfs-administration-part-xvii-best-practices-and-caveats/"   >https://pthree.org/2013/01/03/zfs-administration-part-xvii-best-practices-and-caveats/</a></div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >13.&nbsp;<a style="line-height: 28px; text-decoration: none; color: rgb(85, 108, 136);" target="_blank" rel="nofollow" href="http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide#Memory_and_Dynamic_Reconfiguration_Recommendations"   >http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide#Memory_and_Dynamic_Reconfiguration_Recommendations</a></div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >14.&nbsp;<a style="line-height: 28px; text-decoration: none; color: rgb(85, 108, 136);" target="_blank" rel="nofollow" href="http://www.solarisinternals.com/wiki/index.php/ZFS_Evil_Tuning_Guide#Tuning_ZFS_for_Database_Performance"   >http://www.solarisinternals.com/wiki/index.php/ZFS_Evil_Tuning_Guide#Tuning_ZFS_for_Database_Performance</a></div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >15.&nbsp;<a style="line-height: 28px; text-decoration: none; color: rgb(85, 108, 136);" target="_blank" rel="nofollow" href="http://www.solarisinternals.com/wiki/index.php/ZFS_for_Databases"   >http://www.solarisinternals.com/wiki/index.php/ZFS_for_Databases</a></div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >16.&nbsp;<a style="line-height: 28px; text-decoration: none; color: rgb(85, 108, 136);" target="_blank" rel="nofollow" href="https://blogs.oracle.com/roch/entry/dedup_performance_considerations1"   >https://blogs.oracle.com/roch/entry/dedup_performance_considerations1</a></div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >17.&nbsp;<a style="line-height: 28px; text-decoration: none; color: rgb(85, 108, 136);" target="_blank" rel="nofollow" href="https://wiki.freebsd.org/ZFSTuningGuide"   >https://wiki.freebsd.org/ZFSTuningGuide</a></div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >18.&nbsp;<a style="line-height: 28px; text-decoration: none; color: rgb(85, 108, 136);" target="_blank" rel="nofollow" href="https://blogs.oracle.com/roch/entry/proper_alignment_for_extra_performance"   >https://blogs.oracle.com/roch/entry/proper_alignment_for_extra_performance</a></div></div><div style="line-height: 28px; color: rgb(51, 51, 51); font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, u5b8bu4f53;"   >19.&nbsp;<a style="font-family: 'Hiragino Sans GB W3', 'Hiragino Sans GB', Arial, Helvetica, simsun, &#23435;&#20307;; line-height: 28px;" target="_blank" href="http://blog.163.com/digoal@126/blog/static/16387704020145264116819/"   >http://blog.163.com/digoal@126/blog/static/16387704020145264116819/</a></div><div><br></div>
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"   ><img title="ZFS (sync, async) R/W IOPS / throughput performance tuning - 德哥@Digoal - PostgreSQL"   src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"   alt="Flag Counter"   border="0"   ></a></div>
	</div>
	<h3>评论</h3>
	<div class="" id="" style="padding:0 20px;">
			<div id="">
				<h5 id="">piyongcai - 2014-08-21 17:29:34</h5>
				<div><P>感谢德哥，有没有测试过postgresql在压缩的ZFS上的性能，最好能喝ext3，ext4，xfs等比较一下。</P>
<P>我找到一篇文章可以参考</P>
<P>http://www.07net01.com/program/329634.html</P></div>
			</div>
			<div style="padding-left:40px;">
				<h5 id="">德哥@Digoal 回复 piyongcai - 2014-08-21 17:29:34</h5>
				<div style="width:600px;">ZFS在Linux下性能一般, 而且L2ARC只存储clean数据, 没有脏数据, 所以对于DML频繁的数据库, ZFS需要较大的内存来支撑, L2ARC比较有利于数据库中有大量常驻不变的活跃数据.&nbsp;</div>
			</div>
			<div style="padding-left:40px;">
				<h5 id="">德哥@Digoal 回复 piyongcai - 2014-08-21 17:29:34</h5>
				<div style="width:600px;">如果只考虑性能的话, 建议上SSD用flashcache+xfs.<div>zfs压缩方面, 建议选择lz4或者gzip6</div></div>
			</div>
			<div style="padding-left:40px;">
				<h5 id="">piyongcai 回复 德哥@Digoal - 2014-08-21 17:29:34</h5>
				<div style="width:600px;"><P>谢谢。</P>
<P>其实我现在处于是想将现有的业务迁移到pg环境中，想选择一个好的操作系统和文件系统。</P>
<P>但是网上公说公有理婆说婆有理，眼花缭乱，不知如何选择。</P>
<P>&nbsp;</P>
<P>在大多数的测试中，结果基本上是胜出，zfs一文不值，但是我找到的少有的文章中，zfs有不错的性能，再加上如下两篇文档，就不知怎么办了。</P>
<P>&nbsp;</P>
<P>1、启用压缩后的zfs性能比ext3高4倍以上</P>
<P>http://www.07net01.com/program/329634.html</P>
<P>2、有raid阵列，都将<SPAN style=""   >async</SPAN><SPAN style=""  >和<SPAN  >noatime关闭，zfs性能比ext4要好5倍以上</SPAN></SPAN></P>
<P>http://www.oschina.net/translate/postgresql-comparative-benchmark-between-freebsd-and-linux?print</P>
<P>&nbsp;</P>
<P>望德哥给予指点支持。</P>
<P>谢谢</P></div>
			</div>
			<div style="padding-left:40px;">
				<h5 id="">德哥@Digoal 回复 piyongcai - 2014-08-21 17:29:34</h5>
				<div style="width:600px;">先了解一下ZFS的原理, 搞清楚什么场景该如何调整.<div>PG要获得好的性能, 读cache, 写XLOG异步.</div><div>读cache方面, ZFS可能不能很好的使用pgfincore这类插件, 而其他文件系统如EXT4则很方便.&nbsp;</div><div>写异步方面, 各类文件系统都差不多, ZFS没有directio, 所以会比较吃亏一点, 还有注意ZFS已经有了full page write保护, 所以可以关闭PG的full page write以.</div><div>另外ZFS和zpool都有大量的参数, 可以优化.</div><div>总的来说, zfs的fsync写性能肯定是不如其他文件系统如EXT4 XFS的, 但是PG关闭sync就没什么差别了.</div><div>读的话, 如果使用了L2ARC可以提高随机IOPS读, 但是对于频繁变更又频繁读的效果可能不明显, 因为L2ARC中的数据在ARC变脏后直接丢弃的.</div></div>
			</div>
			<div style="padding-left:40px;">
				<h5 id="">德哥@Digoal 回复 piyongcai - 2014-08-21 17:29:34</h5>
				<div style="width:600px;">建议选CentOS 6 x64+xfs或ext4.<div>如果有非常高的IOPS要求, 建议用SSD, 至于要不要用flash cache就看你们的应用场景了.</div></div>
			</div>
	</div>
</div>
</body>
</html>