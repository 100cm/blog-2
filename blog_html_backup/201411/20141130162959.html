<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=gbk">
<title>PostgreSQL research</title>
<style type="text/css">
.blogcnt{line-height:160%;font-size:14px;text-align:left;word-wrap:break-word;}
.blogcnt *{line-height:160%;}
.blogcnt p{margin:0 0 10px;}
.blogcnt ul,.nbw-blog ol{margin:5px 0 5px 40px;padding:0}
.blogcnt em{font-style:italic;}
.blogcnt blockquote{font-size:1em;margin:auto 0 auto 35px;}
.blogcnt img{border:0;max-width:100%;}
</style>
</head>
<body style="color:#444444;">
<h1 id="blog-Title"><a href="index.html">PostgreSQL research</a></h1>
<div id="" style="padding:0 20px;">
	<h2 id="">ceph GLOSSARY</h2>
	<h5 id="">2014-11-30 16:29:59&nbsp;&nbsp;&nbsp;<a href="http://blog.163.com/digoal@126/blog/static/163877040201410304244205/" target="_blank">查看原文&gt;&gt;</a></h5>
	<div class="" id="" style="padding:0 20px;">
		<div class="blogcnt" style="width:800px;"><div>ceph文档里术语较多, 为了方便理解, 最好先了解一下ceph的术语.</div><div>以下摘自ceph doc, 少了PG.</div><div><br></div><div>PG placement group</div><div>&nbsp; &nbsp; &nbsp;PG, 存储 object 的逻辑组. PG存储在OSD中. OSD包含journal和data. 写完journal后返回ack确认数据安全性.</div><div>&nbsp; &nbsp; &nbsp;一般journal使用SSD来存储, 需要高的响应速度(类型postgresql xlog)</div><div>&nbsp; &nbsp; &nbsp;<span style="color: rgb(62, 67, 73); font-family: Helvetica, Arial, sans-serif; font-size: 14px; line-height: 21px;"   >Ceph stores a client’s data as objects within storage pools. Using the CRUSH algorithm, Ceph calculates which placement group should contain the object, and further calculates which Ceph OSD Daemon should store the placement group. The CRUSH algorithm enables the Ceph Storage Cluster to scale, rebalance, and recover dynamically.</span></div><div><h1 style="margin: 0px 0px 10px; padding: 5px 0px; text-transform: uppercase; font-weight: normal; font-stretch: normal; font-size: 20px; line-height: 2; font-family: 'Titillium Web'; color: rgb(55, 66, 74); border-top-width: 20px; border-top-style: solid; border-top-color: white;"   >CEPH GLOSSARY<a title="Permalink to this headline" style="color: rgb(198, 15, 15); text-decoration: none; visibility: hidden; font-size: 0.8em; padding: 0px 4px;" rel="nofollow" href="http://ceph.com/docs/master/glossary/#ceph-glossary"   ></a></h1><p style="line-height: 1.5em; color: rgb(62, 67, 73); font-family: Helvetica, Arial, sans-serif; font-size: 14px;"   >Ceph is growing rapidly. As firms deploy Ceph, the technical terms such as “RADOS”, “RBD,” “RGW” and so forth require corresponding marketing terms that explain what each component does. The terms in this glossary are intended to complement the existing technical terminology.</p><p style="line-height: 1.5em; color: rgb(62, 67, 73); font-family: Helvetica, Arial, sans-serif; font-size: 14px;"   >Sometimes more than one term applies to a definition. Generally, the first term reflects a term consistent with Ceph’s marketing, and secondary terms reflect either technical terms or legacy ways of referring to Ceph systems.</p><dl style="margin-bottom: 15px; color: rgb(62, 67, 73); font-family: Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.6000003814697px;"   ><dt id="term-ceph-project"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >Ceph Project</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The aggregate term for the people, software, mission and infrastructure of Ceph.</dd><dt id="term-cephx"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >cephx</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The Ceph authentication protocol. Cephx operates like Kerberos, but it has no single point of failure.</dd><dt id="term-ceph"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-ceph-platform"   ></span>Ceph<br>Ceph Platform</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >All Ceph software, which includes any piece of code hosted at&nbsp;<a style="color: rgb(240, 92, 86); text-decoration: none;" rel="nofollow" href="http://github.com/ceph"   >http://github.com/ceph</a>.</dd><dt id="term-ceph-system"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-ceph-stack"   ></span>Ceph System<br>Ceph Stack</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >A collection of two or more components of Ceph.</dd><dt id="term-ceph-node"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-node"   ></span><span id="term-host"   ></span>Ceph Node<br>Node<br>Host</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >Any single machine or server in a Ceph System.</dd><dt id="term-ceph-storage-cluster"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-ceph-object-store"   ></span><span id="term-rados"   ></span><span id="term-rados-cluster"   ></span><span id="term-reliable-autonomic-distributed-object-store"   ></span>Ceph Storage Cluster<br>Ceph Object Store<br>RADOS<br>RADOS Cluster<br>Reliable Autonomic Distributed Object Store</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The core set of storage software which stores the user’s data (MON+OSD).</dd><dt id="term-ceph-cluster-map"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-cluster-map"   ></span>Ceph Cluster Map<br>cluster map</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The set of maps comprising the monitor map, OSD map, PG map, MDS map and CRUSH map. See&nbsp;<a style="color: rgb(240, 92, 86); text-decoration: none;" rel="nofollow" href="http://ceph.com/docs/master/architecture#cluster-map"   >Cluster Map</a>&nbsp;for details.</dd><dt id="term-ceph-object-storage"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >Ceph Object Storage</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The object storage “product”, service or capabilities, which consists essentially of a Ceph Storage Cluster and a Ceph Object Gateway.</dd><dt id="term-ceph-object-gateway"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-rados-gateway"   ></span><span id="term-rgw"   ></span>Ceph Object Gateway<br>RADOS Gateway<br>RGW</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The S3/Swift gateway component of Ceph.</dd><dt id="term-ceph-block-device"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-rbd"   ></span>Ceph Block Device<br>RBD</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The block storage component of Ceph.</dd><dt id="term-ceph-block-storage"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >Ceph Block Storage</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The block storage “product,” service or capabilities when used in conjunction with&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>librbd</span></tt>, a hypervisor such as QEMU or Xen, and a hypervisor abstraction layer such as&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>libvirt</span></tt>.</dd><dt id="term-ceph-filesystem"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-cephfs"   ></span><span id="term-ceph-fs"   ></span>Ceph Filesystem<br>CephFS<br>Ceph FS</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The POSIX filesystem components of Ceph.</dd><dt id="term-cloud-platforms"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-cloud-stacks"   ></span>Cloud Platforms<br>Cloud Stacks</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >Third party cloud provisioning platforms such as OpenStack, CloudStack, OpenNebula, ProxMox, etc.</dd><dt id="term-object-storage-device"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-osd"   ></span>Object Storage Device<br>OSD</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >A physical or logical storage unit (<em>e.g.</em>, LUN). Sometimes, Ceph users use the term “OSD” to refer to&nbsp;<a style="color: rgb(240, 92, 86); text-decoration: none;" rel="nofollow" href="http://ceph.com/docs/master/glossary/#term-ceph-osd-daemon"   ><em>Ceph OSD Daemon</em></a>, though the proper term is “Ceph OSD”.</dd><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >Object Storage Devices.</dd><dt id="term-ceph-osd-daemon"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-ceph-osd"   ></span>Ceph OSD Daemon<br>Ceph OSD</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The Ceph OSD software, which interacts with a logical disk (<a style="color: rgb(240, 92, 86); text-decoration: none;" rel="nofollow" href="http://ceph.com/docs/master/glossary/#term-osd"   ><em>OSD</em></a>). Sometimes, Ceph users use the term “OSD” to refer to “Ceph OSD Daemon”, though the proper term is “Ceph OSD”.</dd><dt id="term-ceph-monitor"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-mon"   ></span>Ceph Monitor<br>MON</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The Ceph monitor software.</dd><dt id="term-ceph-metadata-server"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-mds"   ></span>Ceph Metadata Server<br>MDS</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The Ceph metadata software.</dd><dt id="term-ceph-clients"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-ceph-client"   ></span>Ceph Clients<br>Ceph Client</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The collection of Ceph components which can access a Ceph Storage Cluster. These include the Ceph Object Gateway, the Ceph Block Device, the Ceph Filesystem, and their corresponding libraries, kernel modules, and FUSEs.</dd><dt id="term-ceph-kernel-modules"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >Ceph Kernel Modules</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The collection of kernel modules which can be used to interact with the Ceph System (e.g,.&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>ceph.ko</span></tt>,&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>rbd.ko</span></tt>).</dd><dt id="term-ceph-client-libraries"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >Ceph Client Libraries</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The collection of libraries that can be used to interact with components of the Ceph System.</dd><dt id="term-ceph-release"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >Ceph Release</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >Any distinct numbered version of Ceph.</dd><dt id="term-ceph-point-release"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >Ceph Point Release</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >Any ad-hoc release that includes only bug or security fixes.</dd><dt id="term-ceph-interim-release"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >Ceph Interim Release</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >Versions of Ceph that have not yet been put through quality assurance testing, but may contain new features.</dd><dt id="term-ceph-release-candidate"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >Ceph Release Candidate</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >A major version of Ceph that has undergone initial quality assurance testing and is ready for beta testers.</dd><dt id="term-ceph-stable-release"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >Ceph Stable Release</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >A major version of Ceph where all features from the preceding interim releases have been put through quality assurance testing successfully.</dd><dt id="term-ceph-test-framework"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-teuthology"   ></span>Ceph Test Framework<br>Teuthology</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >The collection of software that performs scripted tests on Ceph.</dd><dt id="term-crush"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >CRUSH</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >Controlled Replication Under Scalable Hashing. It is the algorithm Ceph uses to compute object storage locations.</dd><dt id="term-ruleset"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   >ruleset</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >A set of CRUSH data placement rules that applies to a particular pool(s).</dd><dt id="term-pool"   style="font-weight: bold; font-size: 1em; padding-top: 20px;"   ><span id="term-pools"   ></span>Pool<br>Pools</dt><dd style="margin-top: 3px; margin-bottom: 10px; margin-left: 30px; line-height: 1.5em;"   >Pools are logical partitions for storing objects.</dd></dl></div><div><br></div><div><h4 style="margin: 30px 0px 10px; padding: 5px 0px; text-transform: uppercase; font-weight: normal; font-stretch: normal; font-size: 14px; line-height: 19.6000003814697px; font-family: Helvetica, Arial, sans-serif; color: rgb(55, 66, 74);"   >CLUSTER MAP<a title="Permalink to this headline" style="color: rgb(198, 15, 15); text-decoration: none; visibility: hidden; font-size: 0.8em; padding: 0px 4px;" rel="nofollow" href="http://docs.ceph.com/docs/master/architecture/#cluster-map"   ></a></h4><p style="line-height: 1.5em; color: rgb(62, 67, 73); font-family: Helvetica, Arial, sans-serif; font-size: 14px;"   >Ceph depends upon Ceph Clients and Ceph OSD Daemons having knowledge of the cluster topology, which is inclusive of 5 maps collectively referred to as the “Cluster Map”:</p><ol style="list-style-position: initial; list-style-image: initial; color: rgb(62, 67, 73); font-family: Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.6000003814697px;"   ><li style="line-height: 1.5em;"   ><strong>The Monitor Map:</strong>&nbsp;Contains the cluster&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>fsid</span></tt>, the position, name address and port of each monitor. It also indicates the current epoch, when the map was created, and the last time it changed. To view a monitor map, execute&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>ceph</span>&nbsp;<span>mon</span>&nbsp;<span>dump</span></tt>.</li><li style="line-height: 1.5em;"   ><strong>The OSD Map:</strong>&nbsp;Contains the cluster&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>fsid</span></tt>, when the map was created and last modified, a list of pools, replica sizes, PG numbers, a list of OSDs and their status (e.g.,&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>up</span></tt>,&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>in</span></tt>). To view an OSD map, execute&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>ceph</span>&nbsp;<span>osd</span>&nbsp;<span>dump</span></tt>.</li><li style="line-height: 1.5em;"   ><strong>The PG Map:</strong>&nbsp;Contains the PG version, its time stamp, the last OSD map epoch, the full ratios, and details on each placement group such as the PG ID, the&nbsp;<cite>Up Set</cite>, the&nbsp;<cite>Acting Set</cite>, the state of the PG (e.g.,&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>active</span>&nbsp;<span>+</span>&nbsp;<span>clean</span></tt>), and data usage statistics for each pool.</li><li style="line-height: 1.5em;"   ><strong>The CRUSH Map:</strong>&nbsp;Contains a list of storage devices, the failure domain hierarchy (e.g., device, host, rack, row, room, etc.), and rules for traversing the hierarchy when storing data. To view a CRUSH map, execute&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>ceph</span>&nbsp;<span>osd</span>&nbsp;<span>getcrushmap</span>&nbsp;<span>-o</span>&nbsp;<span>{filename}</span></tt>; then, decompile it by executing&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>crushtool</span>&nbsp;<span>-d</span>&nbsp;<span>{comp-crushmap-filename}</span>&nbsp;<span>-o</span>&nbsp;<span>{decomp-crushmap-filename}</span></tt>. You can view the decompiled map in a text editor or with&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>cat</span></tt>.</li><li style="line-height: 1.5em;"   ><strong>The MDS Map:</strong>&nbsp;Contains the current MDS map epoch, when the map was created, and the last time it changed. It also contains the pool for storing metadata, a list of metadata servers, and which metadata servers are&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>up</span></tt>&nbsp;and&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>in</span></tt>. To view an MDS map, execute&nbsp;<tt style="color: rgb(34, 34, 34); font-size: 15px; background-color: rgb(236, 240, 243);"   ><span>ceph</span>&nbsp;<span>mds</span>&nbsp;<span>dump</span></tt>.</li></ol><p style="line-height: 1.5em; color: rgb(62, 67, 73); font-family: Helvetica, Arial, sans-serif; font-size: 14px;"   >Each map maintains an iterative history of its operating state changes. Ceph Monitors maintain a master copy of the cluster map including the cluster members, state, changes, and the overall health of the Ceph Storage Cluster.</p></div><div><br></div>[参考]<wbr><div>1.&nbsp;http://docs.ceph.com/docs/master/architecture/#cluster-map</div><div>2.&nbsp;http://ceph.com/</div>
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"   ><img title="ceph GLOSSARY - 德哥@Digoal - PostgreSQL research"   src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"   alt="Flag Counter"   border="0"   ></a></div>
	</div>
</div>
</body>
</html>