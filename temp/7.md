## PostgreSQL\GPDB 毫秒级海量时空数据透视 典型案例分享       
                    
### 作者                    
digoal                    
                    
### 日期                    
2017-06-29                   
                    
### 标签                    
PostgreSQL , GIS , 时空数据 , 数据透视 , bitmapAnd , bitmapOr , multi-index , 分区 , brin , geohash cluster    
                    
----                    
                    
## 背景       
随着移动终端的普及，现在有越来越多的业务数据会包含空间数据，例如手机用户的FEED信息、物联网、车联网、气象传感器的数据、动物的溯源数据，一系列跟踪数据。

这些数据具备这几个维度的属性：

1、空间

2、时间

3、业务属性，例如温度、湿度、消费额、油耗、等。

数据透视是企业BI、分析师、运营非常关心的，以往的透视可能是时间结合业务维度的，现在会加入空间的透视（例如某个点附近，在某个时间段的透视；或者某个省的数据透视；或者北纬度附近的数据透视等）。

数据实时透视的一个关键点是预计算、实时计算、流式计算。下面有一个案例：

[《PostgreSQL\GPDB 毫秒级海量多维数据透视 案例分享》](../201706/20170625_01.md)  

以上案例的数据中不包含空间维度，本文将介绍包含 "空间、时间、业务" 等多个维度数据透视的数据库设计和DEMO。
  
## 一、场景介绍
我们以一个场景为例，讲解时空数据透视。

在滴滴、出租车、公交车、大巴、危化品车辆上安装了传感器，这些传感器的位置是跟着汽车流动的，同时这些传感器会不断的生成数据并上报数据。

同时还有一些静止的传感器，也会上传并上报数据。

数据结构有3种，根据不同的上报模式，对应不同的结构。

1、单条上报模式

```
table
(
  sid int,       -- 传感器ID
  pos geometry,  -- 传感器位置
  ts timestamp,  -- 时间
  val1 int,      -- 传感器探测到的属性1值
  val2 float,    -- 传感器探测到的属性2值 
  val3 text      -- 传感器探测到的属性3值
  ...... 
)
```

2、批量上报模式，聚合后的明细(一条记录包含多个VALUE)

```
table
(
  sid int,         -- 传感器ID
  pos geometry[],  -- 传感器位置数组
  ts timestamp[],  -- 时间数组
  val1 int[],      -- 传感器探测到的属性1值数组
  val2 float[],    -- 传感器探测到的属性2值数组
  val3 text[]      -- 传感器探测到的属性3值数组
  ...... 
)
```

3、批量上报模式，JSON打包的明细

```
table
(
  sid int,         -- 传感器ID
  info jsonb       -- 批量打包数据 {k1: {pos:val, ts:val, val1:val, val2:val, ...}, k2: {}, k3: {}, ....}
)
```

## 二、架构设计

数据透视架构设计，分为两种，一种比较暴力，实时裸算，需要更多的计算能力，但是比较适合无法建模的透视需求。

另一种，对于可以建模的透视，则可以通过预处理的方式，使用更低的成本，提高透视的响应速度。

如下

一、实时架构

实时模式，指数据写入后，用户根据需求查询，统计结果。实时模式的查询响应速度取决于集群的计算能力，通常为了达到高速响应，需要大量的投入。

![pic](20170629_01_pic_003.jpg)  

实时计算除了提高计算能力，通常还可以通过任意列索引来提高透视的响应速度，例如：

[《多字段，任意组合条件查询(无需建模) - 毫秒级实时圈人 最佳实践》](../201706/20170607_02.md)  

二、预处理架构

预处理的方法较多，例如流式计算、T+N调度，lambda调度。

![pic](20170629_01_pic_004.jpg)  

1、流处理

流计算可以选择PostgreSQL的pipelineDB插件（预计7月份插件化），支持TTL，滑动窗口，估值统计，以及PG内置的聚合统计函数等。性能也非常不错，单机可以达到100万行/s的处理速度。

[《流计算风云再起 - PostgreSQL携PipelineDB力挺IoT》](../201612/20161220_01.md)  

数据来源实时写入pipelinedb进行流式计算，流式计算的结果（例如统计维度为天，TTL设置为7天，每天将前天的统计结果写入报表库RDS PG或者HDB PG），数据来源的明细数据如果要留底，则可以将其写入 HDB PG或OSS。

这种架构设计，对于可以建模的透视，可以做到毫秒级的响应。对于无法建模的透视需求（需要使用明细进行实时的计算），同样可以使用HDB PG的并行计算能力，得到较快速度的响应。

![pic](20170629_01_pic_005.jpg)  

2、T+n 调度

T+n 调度，实际上是一种常见的报表系统的做法，例如在凌晨将明细数据导入到数据库或者OSS中，根据建模，生成报表。

案例如下：

[《PostgreSQL\GPDB 多维数据透视典型案例分享》](../201706/20170625_01.md) 

![pic](20170629_01_pic_006.jpg)  

3、lambda 调度

T+N调度，只是将流计算节点更换成HDB PG或者RDS PG，通过FUNCIONT和任务调度的方式，增量的对建模数据进行统计和合并统计结果。

案例如下：

[《(流式、lambda、触发器)实时处理大比拼 - 物联网(IoT)\金融,时序处理最佳实践》](../201705/20170518_01.md)  

## 三、分区规则设计
分区规则指数据在某一个数据节点内的分区规则，分区规则应考虑到数据的查询方式，例如经常按时间、空间范围搜索或查询，所以我们有两个分区维度。

PostgreSQL, HDB都支持多级分区，因此可以在这两个维度上进行多级分区。

```
   [ PARTITION BY partition_type (column)
       [ SUBPARTITION BY partition_type (column) ] 
          [ SUBPARTITION TEMPLATE ( template_spec ) ]
       [...]
    ( partition_spec ) 
        | [ SUBPARTITION BY partition_type (column) ]
          [...]
    ( partition_spec 
      [ ( subpartition_spec 
           [(...)] 
         ) ] 
    )


where partition_type is:
    LIST
  | RANGE


where partition_specification is:
partition_element [, ...]

and partition_element is:
   DEFAULT PARTITION name
  | [PARTITION name] VALUES (list_value [,...] )
  | [PARTITION name] 
     START ([datatype] 'start_value') [INCLUSIVE | EXCLUSIVE]
     [ END ([datatype] 'end_value') [INCLUSIVE | EXCLUSIVE] ]
     [ EVERY ([datatype] [number | INTERVAL] 'interval_value') ]
  | [PARTITION name] 
     END ([datatype] 'end_value') [INCLUSIVE | EXCLUSIVE]
     [ EVERY ([datatype] [number | INTERVAL] 'interval_value') ]
[ WITH ( partition_storage_parameter=value [, ... ] ) ]
[column_reference_storage_directive [, ...] ]


where subpartition_spec or template_spec is:
subpartition_element [, ...]
and subpartition_element is:
   DEFAULT SUBPARTITION name
  | [SUBPARTITION name] VALUES (list_value [,...] )
  | [SUBPARTITION name] 
     START ([datatype] 'start_value') [INCLUSIVE | EXCLUSIVE]
     [ END ([datatype] 'end_value') [INCLUSIVE | EXCLUSIVE] ]
     [ EVERY ([datatype] [number | INTERVAL] 'interval_value') ]
  | [SUBPARTITION name] 
     END ([datatype] 'end_value') [INCLUSIVE | EXCLUSIVE]
     [ EVERY ([datatype] [number | INTERVAL] 'interval_value') ]
[ WITH ( partition_storage_parameter=value [, ... ] ) ]
[column_reference_storage_directive [, ...] ]
[ TABLESPACE tablespace ]
```

1、时间范围，例如每天一个分区。

2、GEOHASH范围，geohash是经纬坐标的编码值，代表一个BOX，编码长度决定了它的精度(BOX的大小)，相邻的BOX的编码PREFIX相同。因此使用geohash进行范围编码是可行的。

例如用户需要搜索某个时间段的数据，可以使用分区规则，挑选出对应的分区进行查询，从而缩小搜索的范围。

用户需要搜索某个空间范围的数据，通过GEOHASH范围分区，同样可以挑选出对应的分区进行查询，从而缩小搜索的范围。

![pic](20170629_01_pic_007.jpg)  

HDB不仅仅支持geohash同时支持geometry，在geometry上可以建立GiST空间索引，使用空间索引可以支持KNN检索（精准检索，与BOX无关）。    

## 四、分布规则设计
分布规则指数据在多个数据节点层面的分布，不要与分区规则一致。我们可以选择随机或者业务相关字段作为分布规则，同时需要考虑数据的倾斜。

关于分区和分布列的选择，可以参考这篇文档：

[《Greenplum 最佳实践 - 数据分布黄金法则 - 分布列与分区的选择》](../201607/20160719_02.md)  

1、随机分布，数据将随机写入不同的数据节点，保证了数据的均匀性。但是查询时，需要调用所有数据节点进行查询。如果是JOIN，还会涉及数据的重分布。

2、业务ID，按业务ID来分布，当按业务ID进行查询时，只需要调用对应业务ID所在数据节点，但是请务必考虑到数据倾斜，例如某个业务ID的数据特别多，那么可能导致分布不均匀。

PS：（一致性HASH解决了分布不均匀的问题。）

![pic](20170629_01_pic_008.jpg)  

## 五、预计算设计
对于可以建模的透视需求，预计算是可以大幅度提升透视响应时间的手段。



## 预计算 - 固定时间

## 预计算 - 滑动时间窗口

五维空间

![pic](20170629_01_pic_001.jpg)  

## 预计算 - 固定行政区

## 预计算 - 滑动空间窗口(点辐射)

图


某段时间，某个维度，某个空间区间的数据统计

所有时间段，某个空间区间，某个维度的数据统计


## 六、透视设计

## 透视需求 - 固定时间

## 透视需求 - 滑动窗口

## 透视需求 - 固定行政区

## 透视需求 - 点辐射(类似滑动)



## 七、结合OSS

![pic](20170629_01_pic_002.jpg)  


## 八、流计算，lambda

## 九、小结

为什么不使用复合索引？

核心


## 参考
http://docs.pipelinedb.com/sliding-windows.html

[《PostgreSQL\GPDB 多维数据透视典型案例分享》](../201706/20170625_01.md) 

[《(流式、lambda、触发器)实时处理大比拼 - 物联网(IoT)\金融,时序处理最佳实践》](../201705/20170518_01.md)  

