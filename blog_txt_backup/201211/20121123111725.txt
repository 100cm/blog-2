PostgreSQL research

PostgreSQL report LOG stats by csvlog and file_fdw

2012-11-23 11:17:25   查看原文>>

当管理的数据库非常多的时候, 要了解数据库打印的日志都是一件非常痛苦的事情.
怎么样减轻这种痛苦呢, 以下以file_fdw为例, 通过分析日志的error_severity计数, 找出你应该重点关注的数据库集群.
注意要读文件必须是超级用户.
另外要解释以下, file_fdw是服务端接口, 所以读的是服务端的文件, 而不是客户端的文件, 所以可以通过客户端远程连过去进行统计.
另外就是csvlog的格式需要统一一下.

PostgreSQL 9.2.1 为例 : 
日志格式以及rotate等.

digoal=# select name,setting from pg_settings where name ~ 'log';
            name             |            setting             
-----------------------------+--------------------------------
 log_destination             | csvlog
 log_directory               | pg_log
 log_file_mode               | 0600
 log_filename                | postgresql-%Y-%m-%d_%H%M%S.log
 log_rotation_age            | 1440
 log_rotation_size           | 10240
 log_statement               | ddl
 log_timezone                | PRC
 log_truncate_on_rotation    | on
 logging_collector           | on


创建file_fdw extension : 

digoal=# create extension file_fdw;
CREATE EXTENSION
digoal=# CREATE SERVER pglog FOREIGN DATA WRAPPER file_fdw;
CREATE SERVER


列出pg_log下的文件名, 这里用到的是pg_ls_dir函数 : 

digoal=# select filename from (select pg_ls_dir(setting) as filename from pg_settings where name='log_directory') t where t.filename ~ '.*2012-11-22.*csv$';
             filename             
----------------------------------
 postgresql-2012-11-22_000000.csv
(1 row)


创建pglog外部表 : 

digoal=# CREATE FOREIGN TABLE pglog (
  log_time timestamp(3) with time zone,
  user_name text,
  database_name text,
  process_id integer,
  connection_from text,
  session_id text,
  session_line_num bigint,
  command_tag text,
  session_start_time timestamp with time zone,
  virtual_transaction_id text,
  transaction_id bigint,
  error_severity text,
  sql_state_code text,
  message text,
  detail text,
  hint text,
  internal_query text,
  internal_query_pos integer,
  context text,
  query text,
  query_pos integer,
  location text,
  application_name text
) SERVER pglog
OPTIONS ( filename 'pg_log/postgresql-2012-11-22_000000.csv', format 'csv' );
CREATE FOREIGN TABLE



这样就可以检索这个表了 : 

digoal=# select * from pglog limit 1;
-[ RECORD 1 ]----------+---------------------------
log_time               | 2012-11-22 14:00:52.735+08
user_name              | 
database_name          | 
process_id             | 4923
connection_from        | 
session_id             | 50ac4200.133b
session_line_num       | 618
command_tag            | 
session_start_time     | 2012-11-22 00:52:48+08
virtual_transaction_id | 
transaction_id         | 0
error_severity         | LOG
sql_state_code         | 00000
message                | checkpoint starting: time
detail                 | 
hint                   | 
internal_query         | 
internal_query_pos     | 
context                | 
query                  | 
query_pos              | 
location               | 
application_name       | 



但是这样还不够, 我们需要稍微智能一些, 因为1天可能产生很多个csv文件, 所以用函数来建外部表是比较方便快捷的 : 
根据日期创建对应的外部表 : 
外部表的格式也进行统一如 : pglog_$yyyymmdd_$x

create or replace function create_pglog_table (i_date date) returns void as $BODY$
declare
v_path text;
v_filename text;
v_suffix int;
begin
v_suffix := 0;
select setting into v_path from pg_settings where name = 'log_directory';
for v_filename in execute $$select filename from (select pg_ls_dir(setting) as filename from pg_settings where name='log_directory') t where t.filename ~ '.*$$||i_date::text||$$.*csv'$$ loop
execute $$CREATE FOREIGN TABLE IF NOT EXISTS pglog_$$||to_char(i_date,'yyyymmdd')||'_'||v_suffix||$$ (
  log_time timestamp(3) with time zone,
  user_name text,
  database_name text,
  process_id integer,
  connection_from text,
  session_id text,
  session_line_num bigint,
  command_tag text,
  session_start_time timestamp with time zone,
  virtual_transaction_id text,
  transaction_id bigint,
  error_severity text,
  sql_state_code text,
  message text,
  detail text,
  hint text,
  internal_query text,
  internal_query_pos integer,
  context text,
  query text,
  query_pos integer,
  location text,
  application_name text
) SERVER pglog
OPTIONS ( filename '$$||v_path||'/'||v_filename||$$', format 'csv' )$$;
raise notice 'created foreign table: %', $$pglog_$$||to_char(i_date,'yyyymmdd')||'_'||v_suffix ;
v_suffix := v_suffix + 1;
end loop;
return;
end;
$BODY$ language plpgsql;



外部表统计完就可以删掉了, 否则一堆的外部表看着也不舒服.
根据日期进行删除 : 

create or replace function drop_pglog_table (i_date date) returns void as $BODY$
declare
v_tablename text;
v_schema text;
v_sql text;
begin
for v_tablename in execute $$select relname from pg_class 
where oid in (select ftrelid from pg_foreign_table) 
and relname ~ 'pglog_$$||to_char(i_date,'yyyymmdd')||$$'$$ loop
select t2.nspname into v_schema from pg_class t1, pg_namespace t2 
where t1.oid in (select ftrelid from pg_foreign_table) 
and t1.relnamespace=t2.oid
and t1.relname=v_tablename;
v_sql := 'DROP FOREIGN TABLE IF EXISTS '||v_schema||'.'||v_tablename ;
execute v_sql;
raise notice 'droped foreign table: %', v_schema||'.'||v_tablename;
end loop;
end;
$BODY$ language plpgsql;



这个是统计函数, 用来统计error_severity的各个维度的计数 : 

create or replace function error_stats
(
IN i_date date,
OUT o_error_severity text,
OUT o_count bigint
)
returns setof record as $BODY$
declare
v_tablename text;
v_schema text;
v_sql text;
begin
for v_tablename in execute $$select relname from pg_class 
where oid in (select ftrelid from pg_foreign_table) 
and relname ~ 'pglog_$$||to_char(i_date,'yyyymmdd')||$$'$$ loop
select t2.nspname into v_schema from pg_class t1, pg_namespace t2 
where t1.oid in (select ftrelid from pg_foreign_table) 
and t1.relnamespace=t2.oid
and t1.relname=v_tablename;
v_sql := 'select error_severity,count(*) from '||v_schema||'.'||v_tablename||' group by error_severity';
return query execute v_sql;
end loop;
return;
end;
$BODY$ language plpgsql;


创建2012-11-19那天的csvlog外部表 : 

digoal=# select * from create_pglog_table('2012-11-19');
NOTICE:  created foreign table: pglog_20121119_0
NOTICE:  created foreign table: pglog_20121119_1
NOTICE:  created foreign table: pglog_20121119_2
NOTICE:  created foreign table: pglog_20121119_3
NOTICE:  created foreign table: pglog_20121119_4
NOTICE:  created foreign table: pglog_20121119_5
NOTICE:  created foreign table: pglog_20121119_6
NOTICE:  created foreign table: pglog_20121119_7
 create_pglog_table 
--------------------
(1 row)


统计2012-11-19那天的日志 : 
这里请务必在连接是使用IP地址和端口, 不要使用socket或者127.0.0.1, 否则inet_server_addr出来的信息会不对.

digoal=# select '2012-11-19'::date as stat_date,inet_server_addr(),inet_server_port(),o_error_severity,sum(o_count) from error_stats('2012-11-19') group by inet_server_addr(),inet_server_port(),o_error_severity order by o_error_severity;
 stat_date  | inet_server_addr | inet_server_port | o_error_severity |  sum   
------------+------------------+------------------+------------------+--------
 2012-11-19 | 172.16.3.150     |             9200 | ERROR            |      9
 2012-11-19 | 172.16.3.150     |             9200 | LOG              |    861
 2012-11-19 | 172.16.3.150     |             9200 | WARNING          | 224165
(3 rows)


有了这个信息基础, 我们可以创建一个统计信息库, 设计一个统计表, 将每个数据库每天的这些信息插入到这个统计表中, 然后我们可以定一个阈值, 例如将1天的ERROR超过多少条的数据库展示出来. 对于这种数据库进行重点的关注.

digoal=# create table pglog_stats
(stat_date date, server_addr inet, server_port int, error_severity text, cnt int, 
primary key(stat_date,server_addr,server_port,error_severity));



删表 : 

digoal=# select * from drop_pglog_table('2012-11-19');
NOTICE:  droped foreign table: public.pglog_20121119_0
NOTICE:  droped foreign table: public.pglog_20121119_1
NOTICE:  droped foreign table: public.pglog_20121119_2
NOTICE:  droped foreign table: public.pglog_20121119_3
NOTICE:  droped foreign table: public.pglog_20121119_4
NOTICE:  droped foreign table: public.pglog_20121119_5
NOTICE:  droped foreign table: public.pglog_20121119_6
NOTICE:  droped foreign table: public.pglog_20121119_7
 drop_pglog_table 
------------------
(1 row)


批量创建和删除 :

digoal=# select create_pglog_table(t.i_date) from (select date(generate_series('2012-11-01', '2012-11-23', interval '1 day')) as i_date) t;
digoal=# select drop_pglog_table(t.i_date) from (select date(generate_series('2012-11-01', '2012-11-23', interval '1 day')) as i_date) t;


