PostgreSQL research

PostgreSQL Share-disk HA configure Best Practices with CentOS or RHEL Cluster Suite

2014-05-28 22:16:37   查看原文>>

接上一篇BLOG
http://blog.163.com/digoal@126/blog/static/1638770402014428985260/
本文讲一下PostgreSQL共享存储的HA配置.
本文环境基于上一篇BLOG的配置, 但是已经加了共享存储, 换了主机. 

首先要配置multipath
CentOS 5.x可以使用scsi_id得到wwn

# for i in `cat /proc/partitions | awk '{print $4}' |grep sd`; do echo "### $i: `scsi_id -g -u -s /block/$i`"; done
### sda: 36001438005de97860000b00000d20000
### sdb: 36001438005de97860000b00000d60000
### sdc: 36001438005de97860000b00000d20000
### sdd: 36001438005de97860000b00000d60000
### sde: 36001438005de97860000b00000d20000
### sdf: 36001438005de97860000b00000d60000
### sdg: 36001438005de97860000b00000d20000
### sdh: 36001438005de97860000b00000d60000


然后根据这些ID编写/etc/multipath.conf
CentOS 6的话, 不需要这么麻烦, 只要启动multipathd进程, 就可以得到wwn, 然后编写multipath.conf

[root@172_16_13_204 ~]# multipath -ll
mpathc (36001438005de97860000b00000ce0000) dm-1 HP,HSV400
size=350G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='round-robin 0' prio=130 status=active
| |- 1:0:1:2 sde 8:64  active ready running
| `- 2:0:1:2 sdi 8:128 active ready running
`-+- policy='round-robin 0' prio=10 status=enabled
  |- 1:0:0:2 sdc 8:32  active ready running
  `- 2:0:0:2 sdg 8:96  active ready running
mpathb (36001438005de97860000b00000ca0000) dm-0 HP,HSV400
size=350G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='round-robin 0' prio=130 status=active
| |- 1:0:1:1 sdd 8:48  active ready running
| `- 2:0:1:1 sdh 8:112 active ready running
`-+- policy='round-robin 0' prio=10 status=enabled
  |- 1:0:0:1 sdb 8:16  active ready running
  `- 2:0:0:1 sdf 8:80  active ready running


如果您的环境使用了其他多路径软件, 则不需要配置multipathd, 例如EMC的PowerPath.
编写multipath.conf, 两个节点都需要配置, 注意如果两个主机的本地硬盘命名规则不一样的话, blacklist也需要注意.

[root@172_16_13_204 ~]# vi /etc/multipath.conf
# multipath.conf written by anaconda

defaults {
         udev_dir                 /dev
         polling_interval         10
         path_grouping_policy     failover
         getuid_callout           "/sbin/scsi_id -g -u -s /block/%n"
         path_checker             readsector0
         rr_min_io                100
         rr_weight                priorities
         failback                 immediate
         no_path_retry            fail
         user_friendly_names      yes
         flush_on_last_del        yes
}
blacklist {
         devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*"
         devnode "^hd[a-z]"
         devnode "^cciss!c[0-9]d[0-9]*"
}
multipaths {
        multipath {
                wwid "36001438005de97860000b00000ca0000"
                alias    e06_eva_vd4
        }
        multipath {
                wwid "36001438005de97860000b00000ce0000"
                alias    e06_eva_vd3
        }
}


重启multipathd 服务, 加入自动启动

[root@172_16_13_204 ~]# service multipathd restart
ok
Stopping multipathd daemon: [  OK  ]
Starting multipathd daemon: [  OK  ]
[root@172_16_13_204 ~]# multipath -ll
e06_eva_vd4 (36001438005de97860000b00000ca0000) dm-0 HP,HSV400
size=350G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='round-robin 0' prio=130 status=active
| |- 1:0:1:1 sdd 8:48  active ready running
| `- 2:0:1:1 sdh 8:112 active ready running
`-+- policy='round-robin 0' prio=10 status=enabled
  |- 1:0:0:1 sdb 8:16  active ready running
  `- 2:0:0:1 sdf 8:80  active ready running
e06_eva_vd3 (36001438005de97860000b00000ce0000) dm-1 HP,HSV400
size=350G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='round-robin 0' prio=130 status=active
| |- 1:0:1:2 sde 8:64  active ready running
| `- 2:0:1:2 sdi 8:128 active ready running
`-+- policy='round-robin 0' prio=10 status=enabled
  |- 1:0:0:2 sdc 8:32  active ready running
  `- 2:0:0:2 sdg 8:96  active ready running

chkconfig multipathd on



接下来要参考以下配置HA-lvm.
https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Cluster_Administration/ap-ha-halvm-CA.html

在任意节点, 创建pv

[root@172_16_13_204 ~]# pvcreate /dev/mapper/e06_eva_vd3
  Physical volume "/dev/mapper/e06_eva_vd3" successfully created
[root@172_16_13_204 ~]# pvcreate /dev/mapper/e06_eva_vd4
  Physical volume "/dev/mapper/e06_eva_vd4" successfully created



修改lvm锁, 2台主机都需要修改.

# vi /etc/lvm/lvm.conf
locking_type = 3



2台主机都启动clvmd服务. (必须在cman启动后才能启动clvmd, 大多数依赖集群的服务都需要先启动cman后启动, 如果要关闭cman服务, 也需要先关闭依赖cman的服务, 例如clvmd, rgmanager)

chkconfig clvmd on
service clvmd start



在任意节点创建集群卷组.

# vgcreate -cy shared_vg /dev/mapper/e06_eva_vd3 /dev/mapper/e06_eva_vd4


在其他节点扫描pv和vg

[root@172_16_13_204 ~]# pvs
  PV                      VG        Fmt  Attr PSize   PFree  
  /dev/mapper/e06_eva_vd3 shared_vg lvm2 a--  350.00g 350.00g
  /dev/mapper/e06_eva_vd4 shared_vg lvm2 a--  350.00g 350.00g
[root@172_16_13_204 ~]# vgs
  VG        #PV #LV #SN Attr   VSize   VFree  
  shared_vg   2   0   0 wz--nc 699.99g 699.99g



在任意节点创建逻辑卷

[root@172_16_13_203 ~]# lvcreate -L 100G -n lv01 shared_vg
  Logical volume "lv01" created
[root@172_16_13_203 ~]# lvs
  LV   VG        Attr       LSize   Pool Origin Data%  Move Log Cpy%Sync Convert
  lv01 shared_vg -wi-a----- 100.00g   



在任意节点创建集群文件系统, 如果要多个节点同时mount一个文件系统的话, 需要多个journal区域.
本例使用2个journal区域.
所以在格式化文件系统的时候需要设置好.

[root@172_16_13_203 ~]# mkfs.gfs2 -p lock_dlm -t yumdemo:lv01 -j 2 /dev/mapper/shared_vg-lv01 
This will destroy any data on /dev/mapper/shared_vg-lv01.
It appears to contain: symbolic link to `../dm-2'

Are you sure you want to proceed? [y/n] y

Device:                    /dev/mapper/shared_vg-lv01
Blocksize:                 4096
Device Size                100.00 GB (26214400 blocks)
Filesystem Size:           100.00 GB (26214398 blocks)
Journals:                  2
Resource Groups:           400
Locking Protocol:          "lock_dlm"
Lock Table:                "yumdemo:lv01"
UUID:                      7c1531b1-e1e5-b746-a2ec-c1244bbb51eb



加载文件系统

[root@172_16_13_204 ~]# mkdir /data01
[root@172_16_13_204 ~]# mount.gfs2 -o lockproto=lock_dlm /dev/mapper/shared_vg-lv01 /data01
/dev/mapper/shared_vg-lv01 on /data01 type gfs2 (rw,relatime,lockproto=lock_dlm,hostdata=jid=0)

[root@172_16_13_203 ~]# mkdir /data01
[root@172_16_13_203 ~]# mount.gfs2 -o lockproto=lock_dlm /dev/mapper/shared_vg-lv01 /data01
/dev/mapper/shared_vg-lv01 on /data01 type gfs2 (rw,relatime,lockproto=lock_dlm,hostdata=jid=1)



接下来要把文件系统卸载掉

[root@172_16_13_204 ~]# umount /data01


因为多个节点都要使用这个共享存储, 所以可以把它作为全局资源, 添加到独立的service, 并且设置为启动cman时自动启动.
当然不自动启动也没有问题. 可以和VIP的资源放一起, 跟随指定的service一起启动.
查看系统支持的资源和对应的配置选项.

[root@172_16_13_203 ~]# ccs -f /etc/cluster/cluster.conf --lsresourceopts
service - Defines a service (resource group).
ASEHAagent - Sybase ASE Failover Instance
SAPDatabase - Manages any SAP database (based on Oracle, MaxDB, or DB2)
SAPInstance - SAP instance resource agent
apache - Defines an Apache web server
clusterfs - Defines a cluster file system mount.
fs - Defines a file system mount.
ip - This is an IP address.
lvm - LVM Failover script
mysql - Defines a MySQL database server
named - Defines an instance of named server
netfs - Defines an NFS/CIFS file system mount.
nfsclient - Defines an NFS client.
nfsexport - This defines an NFS export.
nfsserver - This defines an NFS server resource.
openldap - Defines an Open LDAP server
oracledb - Oracle 10g/11g Failover Instance
orainstance - Oracle 10g Failover Instance
oralistener - Oracle 10g Listener Instance
postgres-8 - Defines a PostgreSQL server
samba - Dynamic smbd/nmbd resource agent
script - LSB-compliant init script as a clustered resource.
tomcat-6 - Defines a Tomcat server
vm - Defines a Virtual Machine
[root@172_16_13_203 ~]# ccs -f /etc/cluster/cluster.conf --lsresourceopts lvm
lvm - LVM Failover script
  Required Options:
    name: Name
    vg_name: Volume group name
  Optional Options:
    lv_name: Logical Volume name (optional).
    self_fence: Fence the node if it is not able to clean up LVM tags
    nfslock: Enable NFS lock workarounds
    __independent_subtree: Treat this and all children as an independent subtree.
    __enforce_timeouts: Consider a timeout for operations as fatal.
    __max_failures: Maximum number of failures before returning a failure to a status check.
    __failure_expire_time: Amount of time before a failure is forgotten.
    __max_restarts: Maximum number restarts for an independent subtree before giving up.
    __restart_expire_time: Amount of time before a failure is forgotten for an independent subtree.
[root@172_16_13_204 ~]# ccs -f /etc/cluster/cluster.conf --lsresourceopts fs
fs - Defines a file system mount.
  Required Options:
    name: File System Name
    mountpoint: Mount Point
    device: Device or Label
  Optional Options:
    fstype: File system type
    force_unmount: Force Unmount
    quick_status: Quick/brief status checks.
    self_fence: Seppuku Unmount
    nfslock: Enable NFS lock workarounds
    nfsrestart: Enable NFS daemon and lockd workaround
    fsid: NFS File system ID
    force_fsck: Force fsck support
    options: Mount Options
    use_findmnt: Utilize findmnt to detect if and where filesystems are mounted
    __independent_subtree: Treat this and all children as an independent subtree.
    __enforce_timeouts: Consider a timeout for operations as fatal.
    __max_failures: Maximum number of failures before returning a failure to a status check.
    __failure_expire_time: Amount of time before a failure is forgotten.
    __max_restarts: Maximum number restarts for an independent subtree before giving up.
    __restart_expire_time: Amount of time before a failure is forgotten for an independent subtree.



把2个节点的服务停了再开始配置/etc/cluster/cluster.conf

clusvcadmin -d yumdemo_pg
service rgmanager stop
service clvmd stop
service cman stop



接下来我们需要做的是把LVM添加进资源, GFS2文件系统添加进资源.

[root@172_16_13_203 cluster]# ccs -f /etc/cluster/cluster.conf --addresource lvm name=lvm vg_name=shared_vg lv_name=lv01
[root@172_16_13_203 cluster]# ccs -f /etc/cluster/cluster.conf --addresource fs name="fs" mountpoint="/data01" device="/dev/mapper/shared_vg-lv01" options="lockproto=lock_dlm"



把这两个资源添加到yumdemo_pg服务中.

[root@172_16_13_203 cluster]# ccs -f /etc/cluster/cluster.conf --addsubservice yumdemo_pg lvm ref="LVM"
[root@172_16_13_203 cluster]# ccs -f /etc/cluster/cluster.conf --addsubservice yumdemo_pg fs ref="FS"
[root@172_16_13_203 cluster]# vi /etc/cluster/cluster.conf 
<cluster config_version="26" name="yumdemo">
  <fence_daemon post_fail_delay="6" post_join_delay="30"/>
  <clusternodes>
    <clusternode name="172.16.13.203" nodeid="1">
      <fence>
        <method name="ILO">
          <device action="reboot" name="fence_203"/>
        </method>
      </fence>
    </clusternode>
    <clusternode name="172.16.13.204" nodeid="2">
      <fence>
        <method name="ILO">
          <device action="reboot" name="fence_204"/>
        </method>
      </fence>
    </clusternode>
  </clusternodes>
  <cman expected_votes="1" two_node="1"/>
  <fencedevices>
    <fencedevice agent="fence_ilo" ipaddr="172.16.12.12" login="2010" name="fence_203" passwd="2010"/>
    <fencedevice agent="fence_ilo" ipaddr="172.16.12.13" login="2010" name="fence_204" passwd="2010"/>
  </fencedevices>
  <rm>
    <failoverdomains>
      <failoverdomain name="yumdemo_fd" nofailback="1" ordered="1" restricted="1">
        <failoverdomainnode name="172.16.13.203" priority="1"/>
        <failoverdomainnode name="172.16.13.204" priority="1"/>
      </failoverdomain>
    </failoverdomains>
    <resources>
      <ip address="172.16.13.156" monitor_link="1"/>
      <lvm lv_name="lv01" name="LVM" vg_name="shared_vg"/>
      <fs device="/dev/mapper/shared_vg-lv01" mountpoint="/data01" name="FS" options="lockproto=lock_dlm"/>
    </resources>
    <service autostart="0" domain="yumdemo_fd" name="yumdemo_pg">
      <ip ref="172.16.13.156"/>
      <lvm ref="LVM"/>
      <fs ref="FS"/>
    </service>
  </rm>
  <logging/>
</cluster>



把生成的/etc/cluster/cluster.conf内容拷贝到另一台服务器.
同时启动两台主机的cman服务.

[root@172_16_13_203 cluster]# service cman start
Starting cluster: 
   Checking if cluster has been disabled at boot... [  OK  ]
   Checking Network Manager... [  OK  ]
   Global setup... [  OK  ]
   Loading kernel modules... [  OK  ]
   Mounting configfs... [  OK  ]
   Starting cman... [  OK  ]
   Waiting for quorum... [  OK  ]
   Starting fenced... [  OK  ]
   Starting dlm_controld... [  OK  ]
   Tuning DLM kernel config... [  OK  ]
   Starting gfs_controld... [  OK  ]
   Unfencing self... [  OK  ]
   Joining fence domain... [  OK  ]
[root@172_16_13_204 ~]# service cman start
Starting cluster: 
   Checking if cluster has been disabled at boot... [  OK  ]
   Checking Network Manager... [  OK  ]
   Global setup... [  OK  ]
   Loading kernel modules... [  OK  ]
   Mounting configfs... [  OK  ]
   Starting cman... [  OK  ]
   Waiting for quorum... [  OK  ]
   Starting fenced... [  OK  ]
   Starting dlm_controld... [  OK  ]
   Tuning DLM kernel config... [  OK  ]
   Starting gfs_controld... [  OK  ]
   Unfencing self... [  OK  ]
   Joining fence domain... [  OK  ]
[root@172_16_13_204 ~]# clustat
Cluster Status for yumdemo @ Wed May 28 19:49:04 2014
Member Status: Quorate

 Member Name                                                     ID   Status
 ------ ----                                                     ---- ------
 172.16.13.203                                                     1 Online
 172.16.13.204                                                     2 Online, Local


启动clvmd, rgmanager服务.

[root@172_16_13_203 cluster]# service clvmd start
Starting clvmd: 
Activating VG(s):   1 logical volume(s) in volume group "shared_vg" now active
[  OK  ]
[root@172_16_13_203 cluster]# service rgmanager start
Starting Cluster Service Manager: [  OK  ]
[root@172_16_13_204 ~]# service clvmd start
Starting clvmd: 
Activating VG(s):   1 logical volume(s) in volume group "shared_vg" now active
  clvmd not running on node 172.16.13.203
[  OK  ]
[root@172_16_13_204 ~]# service rgmanager start
Starting Cluster Service Manager: [  OK  ]



启动yumdemo_pg服务, 自动加载文件系统.

[root@172_16_13_203 cluster]# clusvcadm -e yumdemo_pg
Local machine trying to enable service:yumdemo_pg...Success
service:yumdemo_pg is now running on 172.16.13.203
[root@172_16_13_203 cluster]# df -h
Filesystem                  Size  Used Avail Use% Mounted on
/dev/sda1                    32G  1.7G   29G   6% /
tmpfs                        12G   32M   12G   1% /dev/shm
/dev/sda3                    96G  206M   91G   1% /opt
/dev/mapper/shared_vg-lv01  100G  259M  100G   1% /data01



关闭服务, 自动卸载配置在服务中的文件系统.

[root@172_16_13_203 cluster]# clusvcadm -d yumdemo_pg
Local machine disabling service:yumdemo_pg...Success
[root@172_16_13_203 cluster]# df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1        32G  1.7G   29G   6% /
tmpfs            12G   26M   12G   1% /dev/shm
/dev/sda3        96G  206M   91G   1% /opt



接下来要做的是安装数据库软件, 在2台主机各自安装PostgreSQL 9.3.4软件.
软件安装在本地目录中,

[root@172_16_13_204 ~]# useradd postgres
[root@172_16_13_203 cluster]# useradd postgres
[root@172_16_13_203 cluster]# cd /opt
[root@172_16_13_203 opt]# mkdir soft_bak
cd soft_bak
wget http://ftp.postgresql.org/pub/source/v9.3.4/postgresql-9.3.4.tar.bz2
tar -jxvf postgresql-9.3.4.tar.bz2
cd postgresql-9.3.4
yum -y install lrzsz sysstat e4fsprogs ntp readline-devel zlib zlib-devel openssl openssl-devel pam-devel libxml2-devel libxslt-devel python-devel tcl-devel gcc make smartmontools flex bison perl perl-devel perl-ExtUtils* OpenIPMI-tools openldap*

./configure --prefix=/opt/pgsql9.3.4 --with-pgport=1921 --with-perl --with-tcl --with-python --with-openssl --with-pam --with-ldap --with-libxml --with-libxslt --enable-thread-safety && gmake world && gmake install-world
ln -s /opt/pgsql9.3.4 /opt/pgsql



安装完后, 启动集群服务, 挂载共享存储, 在任意节点初始化数据库到共享存储.

node A,B:
service cman start
node A,B:
service clvmd start
service rgmanager start
clusvcadmin -e yumdemo_pg
[root@172_16_13_204 postgresql-9.3.4]# df -h
Filesystem                  Size  Used Avail Use% Mounted on
/dev/sda1                    32G  1.7G   29G   6% /
tmpfs                        12G   32M   12G   1% /dev/shm
/dev/sda3                    96G  413M   90G   1% /opt
/dev/mapper/shared_vg-lv01  100G  259M  100G   1% /data01
[root@172_16_13_204 postgresql-9.3.4]# mkdir /data01/pgdata
[root@172_16_13_204 postgresql-9.3.4]# chown postgres:postgres /data01/pgdata



配置2个节点的postgres用户的.bash_profile

# vi /home/postgres/.bash_profile
# add by digoal
export PS1="$USER@`/bin/hostname -s`-> "
export PGPORT=1921
export PGDATA=/data01/pgdata/pg_root
export LANG=en_US.utf8
export PGHOME=/opt/pgsql
export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib:$LD_LIBRARY_PATH
export DATE=`date +"%Y%m%d%H%M"`
export PATH=$PGHOME/bin:$PATH:.
export MANPATH=$PGHOME/share/man:$MANPATH
export PGUSER=postgres
export PGHOST=$PGDATA
export PGDATABASE=postgres
alias rm='rm -i'
alias ll='ls -lh'



在任意节点初始化数据库, 数据目录使用共享存储.

su - postgres
postgres@172_16_13_204-> initdb -D $PGDATA -E UTF8 --locale=C -U postgres -W



配置pg_hba.conf, postgresql.conf

postgres@172_16_13_204-> cd $PGDATA
vi postgresql.conf
listen_addresses = '0.0.0.0'            # what IP address(es) to listen on;
max_connections = 1000                  # (change requires restart)
superuser_reserved_connections = 13     # (change requires restart)
unix_socket_directories = '.'   # comma-separated list of directories
unix_socket_permissions = 0700          # begin with 0 to use octal notation
password_encryption = on
tcp_keepalives_idle = 60                # TCP_KEEPIDLE, in seconds;
tcp_keepalives_interval = 10            # TCP_KEEPINTVL, in seconds;
tcp_keepalives_count = 10               # TCP_KEEPCNT;
shared_buffers = 2048MB                 # min 128kB
maintenance_work_mem = 512MB            # min 1MB
shared_preload_libraries = 'pg_stat_statements,auto_explain'            # (change requires restart)
vacuum_cost_delay = 10                  # 0-100 milliseconds
vacuum_cost_limit = 10000               # 1-10000 credits
bgwriter_delay = 10ms                   # 10-10000ms between rounds
wal_level = hot_standby                 # minimal, archive, or hot_standby
synchronous_commit = off                # synchronization level;
wal_buffers = 16384kB                   # min 32kB, -1 sets based on shared_buffers
wal_writer_delay = 10ms         # 1-10000 milliseconds
checkpoint_segments = 128               # in logfile segments, min 1, 16MB each
archive_mode = on               # allows archiving to be done
archive_command = '/bin/date'           # command to use to archive a logfile segment
max_wal_senders = 32            # max number of walsender processes
wal_keep_segments = 512         # in logfile segments, 16MB each; 0 disables
hot_standby = on                # "on" allows queries during recovery
max_standby_archive_delay = 300s        # max delay before canceling queries
max_standby_streaming_delay = 300s      # max delay before canceling queries
wal_receiver_status_interval = 1s       # send replies at least this often
hot_standby_feedback = on               # send info from standby to prevent
effective_cache_size = 24000MB
log_destination = 'csvlog'              # Valid values are combinations of
logging_collector = on          # Enable capturing of stderr and csvlog
log_directory = 'pg_log'                # directory where log files are written,
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log' # log file name pattern,
log_file_mode = 0600                    # creation mode for log files,
log_truncate_on_rotation = on           # If on, an existing log file with the
log_rotation_age = 1d                   # Automatic rotation of logfiles will
log_rotation_size = 10MB                # Automatic rotation of logfiles will
log_min_duration_statement = 1s # -1 is disabled, 0 logs all statements
log_checkpoints = on
log_connections = on
log_disconnections = on
log_error_verbosity = verbose           # terse, default, or verbose messages
log_lock_waits = on                     # log lock waits >= deadlock_timeout
log_statement = 'ddl'                   # none, ddl, mod, all
log_timezone = 'PRC'
track_activity_query_size = 2048        # (change requires restart)
autovacuum = on                 # Enable autovacuum subprocess?  'on'
log_autovacuum_min_duration = 0 # -1 disables, 0 logs all actions and
datestyle = 'iso, mdy'
timezone = 'PRC'
lc_messages = 'C'                       # locale for system error message
lc_monetary = 'C'                       # locale for monetary formatting
lc_numeric = 'C'                        # locale for number formatting
lc_time = 'C'                           # locale for time formatting
default_text_search_config = 'pg_catalog.english'
pg_stat_statements.max = 1000
pg_stat_statements.track = all
pg_stat_statements.save = on
auto_explain.log_min_duration = 1s
auto_explain.log_analyze = on
auto_explain.log_verbose = on
auto_explain.log_buffers = on
auto_explain.log_timing = on
auto_explain.log_nested_statements = on

vi pg_hba.conf
local   all             all                                     trust
host    all             all             127.0.0.1/32            trust
host    all             all             ::1/128                 trust
host all all 0.0.0.0/0 md5
host replication postgres 0.0.0.0/0 md5



编写启动数据库的脚本, 遵循LSB.

vi /usr/local/bin/pgctl.sh

#!/bin/bash

# environment.
# Get the aliases and functions
if [ -f ~/.bashrc ]; then
        . ~/.bashrc
fi

# User specific environment and startup programs

export PGHOME=/opt/pgsql
export PATH=$PGHOME/bin:$PATH
export PGDATA=/data01/pgdata/pg_root
export PGPORT=1921
export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib


start() {
su - postgres -c "$PGHOME/bin/pg_ctl start"
return 0
}

stop() {
su - postgres -c "$PGHOME/bin/pg_ctl stop -m fast"
return 0 
}

reload() {
su - postgres -c "$PGHOME/bin/pg_ctl reload"
return 0
}

status() {
su - postgres -c "$PGHOME/bin/pg_ctl status"
return $?
}

# See how we were called.
case "$1" in
  start)
        start
        exit $?
        ;;
  stop)
        stop
        exit $?
        ;;
  restart)
        stop
        start
        exit $?
        ;;
  reload)
        reload
        exit $?
        ;;
  status)
        status
        exit $?
        ;;
  *)
        echo $"Usage: $prog {start|stop|restart|reload|status}"
        exit 0
esac

chmod 500 /usr/local/bin/pgctl.sh



将脚本添加到/etc/cluster/cluster.conf

[root@172_16_13_203 ~]# ccs -f /etc/cluster/cluster.conf --lsresourceopts script
script - LSB-compliant init script as a clustered resource.
  Required Options:
    name: Name
    file: Path to script
  Optional Options:
    service_name: Inherit the service name.
    __independent_subtree: Treat this and all children as an independent subtree.
    __enforce_timeouts: Consider a timeout for operations as fatal.
    __max_failures: Maximum number of failures before returning a failure to a status check.
    __failure_expire_time: Amount of time before a failure is forgotten.
    __max_restarts: Maximum number restarts for an independent subtree before giving up.
    __restart_expire_time: Amount of time before a failure is forgotten for an independent subtree.

[root@172_16_13_203 ~]# ccs -f /etc/cluster/cluster.conf --addresource script name=pgctl file=/usr/local/bin/pgctl.sh
[root@172_16_13_203 ~]# ccs -f /etc/cluster/cluster.conf --addsubservice yumdemo_pg script ref=pgctl

[root@172_16_13_203 ~]# ccs -f /etc/cluster/cluster.conf --lsservices
service: name=yumdemo_pg, domain=yumdemo_fd, autostart=0
  ip: ref=172.16.13.156
  lvm: ref=LVM
  fs: ref=FS
  script: ref=pgctl
resources: 
  ip: monitor_link=1, address=172.16.13.156
  lvm: name=LVM, vg_name=shared_vg, lv_name=lv01
  fs: name=FS, device=/dev/mapper/shared_vg-lv01, mountpoint=/data01, options=lockproto=lock_dlm
  script: name=pgctl, file=/usr/local/bin/pgctl.sh


注意这里的subservice的顺序, 启动数据库前必须先启动FS, 如果顺序不对, 必须调整一下, 直接编辑/etc/cluster/cluster.conf即可. 编辑后记得加版本号和同步, 重启集群.

同步/etc/cluster/cluster.conf

[root@172_16_13_203 ~]# cat /etc/cluster/cluster.conf 
<cluster config_version="32" name="yumdemo">
  <fence_daemon post_fail_delay="6" post_join_delay="30"/>
  <clusternodes>
    <clusternode name="172.16.13.203" nodeid="1">
      <fence>
        <method name="ILO">
          <device action="reboot" name="fence_203"/>
        </method>
      </fence>
    </clusternode>
    <clusternode name="172.16.13.204" nodeid="2">
      <fence>
        <method name="ILO">
          <device action="reboot" name="fence_204"/>
        </method>
      </fence>
    </clusternode>
  </clusternodes>
  <cman expected_votes="1" two_node="1"/>
  <fencedevices>
    <fencedevice agent="fence_ilo" ipaddr="172.16.12.12" login="2010" name="fence_203" passwd="2010"/>
    <fencedevice agent="fence_ilo" ipaddr="172.16.12.13" login="2010" name="fence_204" passwd="2010"/>
  </fencedevices>
  <rm>
    <failoverdomains>
      <failoverdomain name="yumdemo_fd" nofailback="1" ordered="1" restricted="1">
        <failoverdomainnode name="172.16.13.203" priority="1"/>
        <failoverdomainnode name="172.16.13.204" priority="1"/>
      </failoverdomain>
    </failoverdomains>
    <resources>
      <ip address="172.16.13.156" monitor_link="1"/>
      <lvm lv_name="lv01" name="LVM" vg_name="shared_vg"/>
      <fs device="/dev/mapper/shared_vg-lv01" mountpoint="/data01" name="FS" options="lockproto=lock_dlm"/>
      <script file="/usr/local/bin/pgctl.sh" name="pgctl"/>
    </resources>
    <service autostart="0" domain="yumdemo_fd" name="yumdemo_pg">
      <ip ref="172.16.13.156"/>
      <lvm ref="LVM"/>
      <fs ref="FS"/>
      <script ref="pgctl"/>
    </service>
  </rm>
  <logging/>
</cluster>



重启集群的2个节点

# clusvcadm -d yumdemo_pg
# service clvmd stop
# service rgmanager stop
# service cman stop

node A,B : 
# service cman start
node A,B : 
# service clvmd start
# service rgmanager start


在任意节点启动集群服务.

node One :
# clusvcadm -e yumdemo_pg

[root@172_16_13_203 ~]# clustat
Cluster Status for yumdemo @ Wed May 28 21:51:19 2014
Member Status: Quorate

 Member Name                                                     ID   Status
 ------ ----                                                     ---- ------
 172.16.13.203                                                     1 Online, Local, rgmanager
 172.16.13.204                                                     2 Online, rgmanager

 Service Name                                             Owner (Last)                                             State         
 ------- ----                                             ----- ------                                             -----         
 service:yumdemo_pg                                       172.16.13.203                                          started       


查看服务所在节点的日志, 我们看到LSB模式的脚本status是用来检测服务状态的, 30秒检查一次.

[root@172_16_13_203 ~]# tail -f -n 1 /var/log/messages 
May 28 21:51:11 172_16_13_203 rgmanager[13987]: Service service:yumdemo_pg started
May 28 21:51:45 172_16_13_203 rgmanager[15614]: [script] Executing /usr/local/bin/pgctl.sh status
May 28 21:52:15 172_16_13_203 rgmanager[15985]: [script] Executing /usr/local/bin/pgctl.sh status
May 28 21:52:45 172_16_13_203 rgmanager[16408]: [script] Executing /usr/local/bin/pgctl.sh status



关闭数据库, 自动恢复, 因为我们脚本中已经定义了status返回异常.

$ pg_ctl stop -m fast

May 28 21:59:45 172_16_13_203 rgmanager[20418]: [script] script:pgctl: status of /usr/local/bin/pgctl.sh failed (returned 3)
May 28 21:59:45 172_16_13_203 rgmanager[13987]: status on script "pgctl" returned 1 (generic error)
May 28 21:59:45 172_16_13_203 rgmanager[13987]: Stopping service service:yumdemo_pg
May 28 21:59:46 172_16_13_203 rgmanager[20459]: [script] Executing /usr/local/bin/pgctl.sh stop
May 28 21:59:46 172_16_13_203 rgmanager[20541]: [ip] Removing IPv4 address 172.16.13.156/24 from eth0
May 28 21:59:56 172_16_13_203 rgmanager[20600]: [fs] unmounting /data01
May 28 21:59:56 172_16_13_203 multipathd: dm-2: remove map (uevent)
May 28 21:59:56 172_16_13_203 multipathd: dm-2: devmap not registered, can't remove
May 28 21:59:56 172_16_13_203 multipathd: dm-2: remove map (uevent)
May 28 21:59:56 172_16_13_203 multipathd: dm-2: devmap not registered, can't remove
May 28 21:59:56 172_16_13_203 rgmanager[13987]: Service service:yumdemo_pg is recovering
May 28 21:59:56 172_16_13_203 rgmanager[13987]: Recovering failed service service:yumdemo_pg
May 28 21:59:57 172_16_13_203 rgmanager[20744]: [fs] mounting /dev/dm-2 on /data01
May 28 21:59:57 172_16_13_203 kernel: GFS2: fsid=: Trying to join cluster "lock_dlm", "yumdemo:lv01"
May 28 21:59:57 172_16_13_203 kernel: GFS2: fsid=yumdemo:lv01.0: Joined cluster. Now mounting FS...
May 28 21:59:57 172_16_13_203 kernel: GFS2: fsid=yumdemo:lv01.0: jid=0, already locked for use
May 28 21:59:57 172_16_13_203 kernel: GFS2: fsid=yumdemo:lv01.0: jid=0: Looking at journal...
May 28 21:59:57 172_16_13_203 kernel: GFS2: fsid=yumdemo:lv01.0: jid=0: Done
May 28 21:59:57 172_16_13_203 kernel: GFS2: fsid=yumdemo:lv01.0: jid=1: Trying to acquire journal lock...
May 28 21:59:57 172_16_13_203 kernel: GFS2: fsid=yumdemo:lv01.0: jid=1: Looking at journal...
May 28 21:59:57 172_16_13_203 kernel: GFS2: fsid=yumdemo:lv01.0: jid=1: Done
May 28 21:59:57 172_16_13_203 rgmanager[20853]: [ip] Adding IPv4 address 172.16.13.156/24 to eth0
May 28 22:00:01 172_16_13_203 rgmanager[20936]: [script] Executing /usr/local/bin/pgctl.sh start
May 28 22:00:01 172_16_13_203 rgmanager[13987]: Service service:yumdemo_pg started



断开IP, 自动切换到另一台主机

[root@172_16_13_203 ~]# ifdown eth0


May 28 22:03:32 172_16_13_204 corosync[31497]:   [TOTEM ] A processor failed, forming new configuration.
May 28 22:03:34 172_16_13_204 corosync[31497]:   [QUORUM] Members[1]: 2
May 28 22:03:34 172_16_13_204 corosync[31497]:   [TOTEM ] A processor joined or left the membership and a new membership was formed.
May 28 22:03:34 172_16_13_204 corosync[31497]:   [CPG   ] chosen downlist: sender r(0) ip(172.16.13.204) ; members(old:2 left:1)
May 28 22:03:34 172_16_13_204 corosync[31497]:   [MAIN  ] Completed service synchronization, ready to provide service.
May 28 22:03:34 172_16_13_204 rgmanager[31785]: State change: 172.16.13.203 DOWN
May 28 22:03:35 172_16_13_204 kernel: dlm: closing connection to node 1
May 28 22:03:40 172_16_13_204 fenced[31552]: fencing node 172.16.13.203
May 28 22:03:53 172_16_13_204 fenced[31552]: fence 172.16.13.203 success
May 28 22:03:54 172_16_13_204 rgmanager[31785]: Taking over service service:yumdemo_pg from down member 172.16.13.203
May 28 22:03:57 172_16_13_204 rgmanager[973]: [fs] mounting /dev/dm-2 on /data01
May 28 22:03:58 172_16_13_204 kernel: GFS2: fsid=: Trying to join cluster "lock_dlm", "yumdemo:lv01"
May 28 22:03:58 172_16_13_204 kernel: GFS2: fsid=yumdemo:lv01.0: Joined cluster. Now mounting FS...
May 28 22:03:58 172_16_13_204 kernel: GFS2: fsid=yumdemo:lv01.0: jid=0, already locked for use
May 28 22:03:58 172_16_13_204 kernel: GFS2: fsid=yumdemo:lv01.0: jid=0: Looking at journal...
May 28 22:03:58 172_16_13_204 kernel: GFS2: fsid=yumdemo:lv01.0: jid=0: Done
May 28 22:03:58 172_16_13_204 kernel: GFS2: fsid=yumdemo:lv01.0: jid=1: Trying to acquire journal lock...
May 28 22:03:58 172_16_13_204 kernel: GFS2: fsid=yumdemo:lv01.0: jid=1: Looking at journal...
May 28 22:03:58 172_16_13_204 kernel: GFS2: fsid=yumdemo:lv01.0: jid=1: Done
May 28 22:03:58 172_16_13_204 rgmanager[1082]: [ip] Adding IPv4 address 172.16.13.156/24 to eth0
May 28 22:04:01 172_16_13_204 rgmanager[1176]: [script] Executing /usr/local/bin/pgctl.sh start
May 28 22:04:01 172_16_13_204 rgmanager[31785]: Service service:yumdemo_pg started



[其他]
1. 个人不建议将lvm , gfs的加载放到HA服务中来启动, 因为很不方便, 包括后面的排错, 如果你想在多个节点同时挂载共享磁盘文件系统的话, 放在HA服务启动就不行, 因为一个服务只能在1个节点启动.
所以建议lvm和gfs放在启动cman和clvmd后, 手工挂载. 
VIP和pg启动脚本建议放在一起.
2. 修改资源的检测间隔.
参考
https://fedorahosted.org/cluster/wiki/ResourceActions
或/etc/cluster/cluster.conf
例如把pgctl.sh的资源检测时间改成10秒一次.

# vi /etc/cluster/cluster.conf 
<cluster config_version="36" name="yumdemo">
...
    <resources>
      <ip address="192.168.173.156" monitor_link="1"/>
      <lvm lv_name="lv01" name="LVM" vg_name="shared_vg"/>
      <fs device="/dev/mapper/shared_vg-lv01" mountpoint="/data01" name="FS" options="lockproto=lock_dlm"/>
      <script file="/usr/local/bin/pgctl.sh" name="pgctl">
        <action name="status" interval="10"/>
      </script>
    </resources>
...


同步cluster.conf, 重启集群后, 生效.

[root@192_168_173_204 ~]# tail -f -n 1 /var/log/messages 
May 29 09:00:39 192_168_173_204 rgmanager[32323]: Service service:yumdemo_pg started
May 29 09:00:55 192_168_173_204 rgmanager[1317]: [script] Executing /usr/local/bin/pgctl.sh status
May 29 09:01:06 192_168_173_204 rgmanager[1550]: [script] Executing /usr/local/bin/pgctl.sh status
May 29 09:01:25 192_168_173_204 rgmanager[1755]: [script] Executing /usr/local/bin/pgctl.sh status
May 29 09:01:35 192_168_173_204 rgmanager[1875]: [script] Executing /usr/local/bin/pgctl.sh status
May 29 09:01:46 192_168_173_204 rgmanager[2136]: [script] Executing /usr/local/bin/pgctl.sh status



[参考]
1. http://blog.163.com/digoal@126/blog/static/1638770402014428985260/
2. https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Cluster_Administration/ap-ha-halvm-CA.html
3. man mkfs.gfs2
4. man mount.gfs2
5. https://access.redhat.com/site/articles/40051
6. https://fedorahosted.org/cluster/wiki/ResourceActions

Flag Counter
