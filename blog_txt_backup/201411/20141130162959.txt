PostgreSQL research

ceph GLOSSARY

2014-11-30 16:29:59   查看原文>>

ceph文档里术语较多, 为了方便理解, 最好先了解一下ceph的术语.
以下摘自ceph doc, 少了PG.

PG placement group
     PG, 存储 object 的逻辑组. PG存储在OSD中. OSD包含journal和data. 写完journal后返回ack确认数据安全性.
     一般journal使用SSD来存储, 需要高的响应速度(类型postgresql xlog)
     Ceph stores a client’s data as objects within storage pools. Using the CRUSH algorithm, Ceph calculates which placement group should contain the object, and further calculates which Ceph OSD Daemon should store the placement group. The CRUSH algorithm enables the Ceph Storage Cluster to scale, rebalance, and
recover dynamically.

CEPH GLOSSARY 

Ceph is growing rapidly. As firms deploy Ceph, the technical terms such as “RADOS”, “RBD,”“RGW” and so forth require corresponding marketing terms that explain what each component does. The terms in this glossary are intended to complement the existing technical terminology.

Sometimes more than one term applies to a definition. Generally, the first term reflects a term consistent with Ceph’s marketing, and secondary terms reflect either technical terms or legacy ways of referring to Ceph systems.

Ceph Project
    The aggregate term for the people, software, mission and infrastructure of Ceph.
cephx
    The Ceph authentication protocol. Cephx operates like Kerberos, but it has no single point of failure.
Ceph
    Ceph Platform
    All Ceph software, which includes any piece of code hosted at http://github.com/ceph.
Ceph System
    Ceph Stack
    A collection of two or more components of Ceph.
Ceph Node
    Node
    Host
    Any single machine or server in a Ceph System.
Ceph Storage Cluster
    Ceph Object Store
    RADOS
    RADOS Cluster
    Reliable Autonomic Distributed Object Store
    The core set of storage software which stores the user’s data (MON+OSD).
Ceph Cluster Map
    cluster map
    The set of maps comprising the monitor map, OSD map, PG map, MDS map and CRUSH map. See Cluster Map for details.
Ceph Object Storage
    The object storage “product”, service or capabilities, which consists essentially of a Ceph Storage Cluster and a Ceph Object Gateway.
Ceph Object Gateway
    RADOS Gateway
    RGW
    The S3/Swift gateway component of Ceph.
Ceph Block Device
    RBD
    The block storage component of Ceph.
Ceph Block Storage
    The block storage “product,” service or capabilities when used in conjunction with librbd, a hypervisor such as QEMU or Xen, and a hypervisor abstraction layer such as libvirt.
Ceph Filesystem
    CephFS
    Ceph FS
    The POSIX filesystem components of Ceph.
Cloud Platforms
    Cloud Stacks
    Third party cloud provisioning platforms such as OpenStack, CloudStack, OpenNebula, ProxMox, etc.
Object Storage Device
    OSD
    A physical or logical storage unit (e.g., LUN). Sometimes, Ceph users use the term “OSD” to refer to Ceph OSD Daemon, though the proper term is “Ceph OSD”.
    Object Storage Devices.
Ceph OSD Daemon
    Ceph OSD
    The Ceph OSD software, which interacts with a logical disk (OSD). Sometimes, Ceph users use the term “OSD” to refer to “Ceph OSD Daemon”, though the proper term is “Ceph OSD”.
Ceph Monitor
    MON
    The Ceph monitor software.
Ceph Metadata Server
    MDS
    The Ceph metadata software.
Ceph Clients
    Ceph Client
    The collection of Ceph components which can access a Ceph Storage Cluster. These include the Ceph Object Gateway, the Ceph Block Device, the Ceph Filesystem, and their corresponding libraries, kernel modules, and FUSEs.
Ceph Kernel Modules
    The collection of kernel modules which can be used to interact with the Ceph System (e.g,. ceph.ko, rbd.ko).
Ceph Client Libraries
    The collection of libraries that can be used to interact with components of the Ceph System.
Ceph Release
    Any distinct numbered version of Ceph.
Ceph Point Release
    Any ad-hoc release that includes only bug or security fixes.
Ceph Interim Release
    Versions of Ceph that have not yet been put through quality assurance testing, but may contain new features.
Ceph Release Candidate
    A major version of Ceph that has undergone initial quality assurance testing and is ready for beta testers.
Ceph Stable Release
    A major version of Ceph where all features from the preceding interim releases have been put through quality assurance testing successfully.
Ceph Test Framework
    Teuthology
    The collection of software that performs scripted tests on Ceph.
CRUSH
    Controlled Replication Under Scalable Hashing. It is the algorithm Ceph uses to compute object storage locations.
ruleset
    A set of CRUSH data placement rules that applies to a particular pool(s).
Pool
    Pools
    Pools are logical partitions for storing objects.


CLUSTER MAP 

Ceph depends upon Ceph Clients and Ceph OSD Daemons having knowledge of the cluster topology, which is inclusive of 5 maps collectively referred to as the “Cluster Map”:

 1. The Monitor Map: Contains the cluster fsid, the position, name address and port of each monitor. It also indicates the current epoch, when the map was created, and the last time it changed. To view a monitor map, execute ceph mon dump.
 2. The OSD Map: Contains the cluster fsid, when the map was created and last modified, a list of pools, replica sizes, PG numbers, a list of OSDs and their status (e.g., up, in). To view an OSD map, execute ceph osd dump.
 3. The PG Map: Contains the PG version, its time stamp, the last OSD map epoch, the full ratios, and details on each placement group such as the PG ID, the Up Set, the Acting Set, the state of the PG (e.g., active + clean), and data usage statistics for each pool.
 4. The CRUSH Map: Contains a list of storage devices, the failure domain hierarchy (e.g., device, host, rack, row, room, etc.), and rules for traversing the hierarchy when storing data. To view a CRUSH map, execute ceph osd getcrushmap -o {filename}; then, decompile it by executing crushtool -d 
    {comp-crushmap-filename} -o {decomp-crushmap-filename}. You can view the decompiled map in a text editor or with cat.
 5. The MDS Map: Contains the current MDS map epoch, when the map was created, and the last time it changed. It also contains the pool for storing metadata, a list of metadata servers, and which metadata servers are up and in. To view an MDS map, execute ceph mds dump.

Each map maintains an iterative history of its operating state changes. Ceph Monitors maintain a master copy of the cluster map including the cluster members, state, changes, and the overall health of the Ceph Storage Cluster.


[参考]
1. http://docs.ceph.com/docs/master/architecture/#cluster-map
2. http://ceph.com/
Flag Counter
