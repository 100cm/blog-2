PostgreSQL research

From bad sector to “damaged file” - did it for Linux/ext3

2014-02-15 21:32:28   查看原文>>

最近一位兄弟的greenplum 数据节点服务器上存储某扇区报错, 帮忙看了存储和链路都正常, 所以从扇区下手找到对应的文件.
首先看看dmesg中的报错信息, 涉及两个块设备sdb, sdf, 是多路径设备.
报错中打印了报错的扇区ID 1984117351. 后面我们使用debugfs从这个扇区获得对应的inode, 再从这个inode取得对应的文件信息.

qla2xxx 0000:07:00.1: scsi(1:0:0:2): Mid-layer underflow detected (40000 of 40000 bytes)...returning error status.
qla2xxx 0000:07:00.1: scsi(1:0:0:2): Mid-layer underflow detected (40000 of 40000 bytes)...returning error status.
qla2xxx 0000:07:00.1: scsi(1:0:0:2): Mid-layer underflow detected (40000 of 40000 bytes)...returning error status.
qla2xxx 0000:07:00.1: scsi(1:0:0:2): Mid-layer underflow detected (40000 of 40000 bytes)...returning error status.
qla2xxx 0000:07:00.1: scsi(1:0:0:2): Mid-layer underflow detected (40000 of 40000 bytes)...returning error status.
qla2xxx 0000:07:00.1: scsi(1:0:0:2): Mid-layer underflow detected (40000 of 40000 bytes)...returning error status.
sd 1:0:0:2: SCSI error: return code = 0x00070000
end_request: I/O error, dev sdf, sector 1984117351
device-mapper: multipath: Failing path 8:80.
sd 0:0:0:2: SCSI error: return code = 0x08000002
sdb: Current: sense key: Medium Error
    Add. Sense: Unrecovered read error
Info fld=0x0
end_request: I/O error, dev sdb, sector 1984117351
device-mapper: multipath: Failing path 8:16.


多路径信息如下 : 

# multipath -ll
msa3vd01vol02 (3600c0ff000103d755811394c01000000) dm-1 HP,MSA2312fc
[size=1.8T][features=1 queue_if_no_path][hwhandler=0][rw]
\_ round-robin 0 [prio=50][enabled]
 \_ 0:0:0:2 sdb 8:16  [failed][ready]
\_ round-robin 0 [prio=10][enabled]
 \_ 1:0:0:2 sdf 8:80  [failed][ready]


这个设备挂载的目录信息如下 : 

# df -h
/dev/mapper/msa3vd01vol02p1
                      1.8T  1.3T  417G  77% /database/gp_dmdir_3


接下来使用debugfs和扇区号找到对应的文件名.
首先要知道这个块设备的块大小.

# tune2fs -l /dev/sdb1 | grep Block\ size
Block size:               4096


从报错的扇区号和块大小来获取对应的块ID 248014668.

# bc -l
1984117351*512/4096
248014668.87500000000000000000


使用open, testb, icheck, ncheck得到文件名.

# debugfs
debugfs 1.39 (29-May-2006)
debugfs:  open /dev/sdb1
debugfs:  testb 248014668
Block 248014668 marked in use
debugfs:  icheck 248014668
Block   Inode number
248014668       103629311
debugfs:  ncheck 103629311
Inode   Pathname
103629311       /gp13/base/297158/624669617.1


首先进入到这个块设备挂载的目录, 因为从debugfs取得的是相对路径.

# cd /database/gp_dmdir_3
# echo 3 > /proc/sys/vm/drop_caches


清除缓存后, 遍历这个文件, 再次重现这个问题.

# cp gp13/base/297158/624669617.1 ~/
# dmesg
qla2xxx 0000:07:00.1: scsi(1:0:0:2): Mid-layer underflow detected (40000 of 40000 bytes)...returning error status.
qla2xxx 0000:07:00.1: scsi(1:0:0:2): Mid-layer underflow detected (40000 of 40000 bytes)...returning error status.
qla2xxx 0000:07:00.1: scsi(1:0:0:2): Mid-layer underflow detected (40000 of 40000 bytes)...returning error status.
qla2xxx 0000:07:00.1: scsi(1:0:0:2): Mid-layer underflow detected (40000 of 40000 bytes)...returning error status.
qla2xxx 0000:07:00.1: scsi(1:0:0:2): Mid-layer underflow detected (40000 of 40000 bytes)...returning error status.
qla2xxx 0000:07:00.1: scsi(1:0:0:2): Mid-layer underflow detected (40000 of 40000 bytes)...returning error status.
sd 1:0:0:2: SCSI error: return code = 0x00070000
end_request: I/O error, dev sdf, sector 1984117319
device-mapper: multipath: Failing path 8:80.
sd 0:0:0:2: SCSI error: return code = 0x08000002
sdb: Current: sense key: Medium Error
    Add. Sense: Unrecovered read error
Info fld=0x0
end_request: I/O error, dev sdb, sector 1984117319
device-mapper: multipath: Failing path 8:16.


问题重现.

# multipath -ll
msa3vd01vol02 (3600c0ff000103d755811394c01000000) dm-1 HP,MSA2312fc
[size=1.8T][features=1 queue_if_no_path][hwhandler=0][rw]
\_ round-robin 0 [prio=50][enabled]
 \_ 0:0:0:2 sdb 8:16  [failed][ready]
\_ round-robin 0 [prio=10][enabled]
 \_ 1:0:0:2 sdf 8:80  [failed][ready]


现在要解决这个问题, 首先要把这个文件重命名掉, 那么greenplum的这个节点不会扫描到这个文件. 也就不会触发重现这个问题.

# mv gp13/base/297158/624669617.1 gp13/base/297158/624669617.1.badsec


根据这个文件查到数据哪个数据库对象, 需要用到pg_class.relfilenode.
PostgreSQL 的文件节点存储在pg_class中, 所以通过查询这个系统表就可以找到对应的数据对象.

# select * from pg_class where relfilenode=624669617;
-[ RECORD 1 ]--+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
relname        | tbl_1_prt_p20130821
relnamespace   | 297163
reltype        | 624669618
relowner       | 16389
relam          | 0
relfilenode    | 624669617
reltablespace  | 0
relpages       | 0
reltuples      | 0
reltoastrelid  | 624669619
reltoastidxid  | 0
relaosegrelid  | 624669621
relaosegidxid  | 0
relhasindex    | f
relisshared    | f
relkind        | r
relstorage     | a
relnatts       | 21
relchecks      | 1
reltriggers    | 0
relukeys       | 0
relfkeys       | 0
relrefs        | 0
relhasoids     | f
relhaspkey     | f
relhasrules    | f
relhassubclass | f
relfrozenxid   | 53139347
relacl         | {digoal=arwdxt/digoal}
reloptions     | {appendonly=true,compresslevel=3}



greenplum中每个子节点的dbid和relfilenode应该都是一样的, 所以在其他节点应该也可以找到这个文件. 如下 : 

# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/cciss/c0d0p1      25G  3.7G   20G  16% /
/dev/cciss/c0d0p3      99G   81G   13G  87% /opt
tmpfs                  12G     0   12G   0% /dev/shm
/dev/cciss/c0d1p1     135G  109G   20G  86% /database/gp_subdir_0
/dev/cciss/c0d2p1     135G  109G   20G  85% /database/gp_subdir_1
/dev/mapper/msa2vd03vol01p1
                      1.8T  1.3T  421G  76% /database/gp_dmdir_0
/dev/mapper/msa2vd03vol02p1
                      1.8T  1.3T  423G  76% /database/gp_dmdir_1
/dev/mapper/msa2vd04vol01p1
                      1.8T  1.3T  427G  76% /database/gp_dmdir_2
/dev/mapper/msa2vd04vol02p1
                      1.8T  1.3T  414G  77% /database/gp_dmdir_3
# cd /database/gp_dmdir_3
# ll
total 1001012
-rw-r--r--  1 root      root      1024000000 Aug 10  2010 1Gb.file
drwx------ 14 greenplum greenplum       4096 Feb 15 20:34 gp3
drwx------  2 greenplum greenplum      16384 Jul 21  2010 lost+found
# ll gp3/base/297158/624669617
-rw------- 1 greenplum greenplum 0 Nov 18  2012 gp3/base/297158/624669617
# ll gp3/base/297158/624669617*
-rw------- 1 greenplum greenplum         0 Nov 18  2012 gp3/base/297158/624669617
-rw------- 1 greenplum greenplum 456241576 Aug 22 01:56 gp3/base/297158/624669617.1



后期处理, 因为我们处理扇区的问题把一个节点的624669617.1文件重命名, 所以在全表扫描时, 这部分数据将缺失, 如果使用索引扫描并且扫描到这部分缺失的数据将报错, 后期建议将 tbl_1_prt_p20130821这个表的数据删除, 从历史数据重新抽取.
坏块处理, 使用dd往坏块填充zero. 详见
http://smartmontools.sourceforge.net/badblockhowto.html
[root]# dd if=/dev/zero of=/dev/sdb1 bs=4096 count=1 seek=248014668
[root]# sync

扫描块设备的坏块: badblocks.

参考smartctl工具手册中使用debugfs取文件名的文章
http://smartmontools.sourceforge.net/badblockhowto.html

When a SMART check on a disk reports a bad sector, it is important to be able to identify the file that has the bad sector - and restore it from backups. Below, I show how I did this for my Linux/ext3 VMWARE server - but does anyone know if this can be done for Windows/NTFS?


Here's how I did it for Linux/ext3: I first asked the drive to do a hardware surface scan (below the OS level, with the on-drive SMART circuits):


vserver:~# smartctl -t long /dev/sdc

I looked at the results:

vserver:~# smartctl -a /dev/sdc
...
196 Reallocated_Event_Count 0x0032   100   100   000    Old_age   Always       -       1
197 Current_Pending_Sector  0x0012   100   100   000    Old_age   Always       -       9
...
Num  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error
# 1  Extended offline    Completed: read failure       90%     27679         591363172

So, one sector was already marked bad, 9 were marked for replacing from the "staging" sector space. More importantly, the first logical block address (LBA) that is unreadable, was 591363172.

I found the partition (and the offset inside it) that this number "translated" to:

vserver:~# fdisk -lu /dev/sdc
Device Boot      Start         End      Blocks   Id  System
/dev/sdc1           32   976773119   488386544   83  Linux

The partition started at sector 32. So, the bad sector was...

vserver:~# bc -l
591363172-32+1
591363141

...at an offset of 591363141 sectors from the beginning of the partition.

Now I could find which file was "hosed":

vserver:~# tune2fs -l /dev/sdc1 | grep Block\ size
Block size:               4096

The block size of this EXT3 filesystem was 4096 bytes, so the bad sector destroyed this block in the filesystem:

vserver:~# bc -l
591363141*512/4096
73920392.62500000000000000000

And the block number (73920392) corresponded into this file:

vserver:~# debugfs
debugfs 1.41.3 (12-Oct-2008)
debugfs:  open /dev/sdc1
testb 73920392
debugfs:  testb 73920392
Block 73920392 marked in use
debugfs:  icheck 73920392
Block           Inode number
73920392        18472967
debugfs:  ncheck 18472967
Inode           Pathname
18472967        /path/to/filewithbadsector

And I restored that file from my backups.

Is there an equivalent procedure I can follow for Windows/NTFS?

I know you have an NTFS FS, and run windows on that FS. I don't know if you "could" boot a live Linux to work on that driver or not.

If you can boot Linux from CD or USB, you can use ntfsprogs. look at -

ntfscluster

ntfsinfo

I believe ntfscluster tell you what file a particular cluster stores. I hope this puts you in the right direction.


[参考]
1. http://smartmontools.sourceforge.net/badblockhowto.html
2. http://serverfault.com/questions/311270/from-bad-sector-to-damaged-file-did-it-for-linux-ext3-can-i-do-it-for-windo
3. man debugfs
4. man badblocks
