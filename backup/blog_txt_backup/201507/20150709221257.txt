PostgreSQL research

PostgreSQL 9.5 new feature - Improve in-memory hash performance

2015-07-09 22:12:57   查看原文>>

Improve in-memory hash performance (Tomas Vondra, Robert Haas)
详见
1. http://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=8cce08f168481c5fc5be4e7e29b968e314f1b41e
2. http://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=45f6240a8fa9d35548eb2ef23dba2c11540aa02a
主要通过提高hashjointuple内存分配速度，以及搜索速度来提升HASHJOIN速度，具体提升效果如何，可以对比测试一下。


postgres=# create table t1(id int,info text);
CREATE TABLE
postgres=# create table t2(id int,info text);
CREATE TABLE
postgres=# insert into t1 select generate_series(1,10000000);
INSERT 0 10000000
postgres=# insert into t2 select generate_series(1,10000000);
INSERT 0 10000000
postgres=# analyze t1;
ANALYZE
postgres=# analyze t2;
ANALYZE



9.4的内存使用量更大，速度更慢。
PostgreSQL 9.4

postgres=# explain (analyze,timing,verbose,buffers,costs) select * from t1 join t2 using (id);
                                                              QUERY PLAN                                                            
  
------------------------------------------------------------------------------------------------------------------------------------
--
 Hash Join  (cost=236002.00..597004.00 rows=10000000 width=68) (actual time=5484.514..27850.011 rows=10000000 loops=1)
   Output: t1.id, t1.info, t2.info
   Hash Cond: (t1.id = t2.id)
   Buffers: shared hit=22004
   ->  Seq Scan on public.t1  (cost=0.00..111002.00 rows=10000000 width=36) (actual time=0.031..1474.728 rows=10000000 loops=1)
         Output: t1.id, t1.info
         Buffers: shared hit=11002
   ->  Hash  (cost=111002.00..111002.00 rows=10000000 width=36) (actual time=5480.140..5480.140 rows=10000000 loops=1)
         Output: t2.info, t2.id
         Buckets: 1048576  Batches: 1  Memory Usage: 351563kB
         Buffers: shared hit=11002
         ->  Seq Scan on public.t2  (cost=0.00..111002.00 rows=10000000 width=36) (actual time=0.034..1882.919 rows=10000000 loops=1
)
               Output: t2.info, t2.id
               Buffers: shared hit=11002
 Planning time: 0.295 ms
 Execution time: 28654.718 ms
(16 rows)



PostgreSQL 9.5

postgres=# explain (analyze,timing,verbose,buffers,costs) select * from t1 join t2 using (id);
                                                              QUERY PLAN                                                            
  
------------------------------------------------------------------------------------------------------------------------------------
--
 Hash Join  (cost=255534.00..675132.00 rows=10000000 width=68) (actual time=4911.462..14121.618 rows=10000000 loops=1)
   Output: t1.id, t1.info, t2.info
   Hash Cond: (t1.id = t2.id)
   Buffers: shared hit=22004, temp read=15302 written=14792
   ->  Seq Scan on public.t1  (cost=0.00..111002.00 rows=10000000 width=36) (actual time=0.038..1533.017 rows=10000000 loops=1)
         Output: t1.id, t1.info
         Buffers: shared hit=11002
   ->  Hash  (cost=111002.00..111002.00 rows=10000000 width=36) (actual time=4906.195..4906.195 rows=10000000 loops=1)
         Output: t2.info, t2.id
         Buckets: 65536  Batches: 256  Memory Usage: 1889kB
         Buffers: shared hit=11002, temp written=7141
         ->  Seq Scan on public.t2  (cost=0.00..111002.00 rows=10000000 width=36) (actual time=0.051..2108.836 rows=10000000 loops=1
)
               Output: t2.info, t2.id
               Buffers: shared hit=11002
 Planning time: 0.141 ms
 Execution time: 14947.637 ms
(16 rows)




Pack tuples in a hash join batch densely, to save memory.

Instead of palloc'ing each HashJoinTuple individually, allocate 32kB chunks
and pack the tuples densely in the chunks. This avoids the AllocChunk
header overhead, and the space wasted by standard allocator's habit of
rounding sizes up to the nearest power of two.

This doesn't contain any planner changes, because the planner's estimate of
memory usage ignores the palloc overhead. Now that the overhead is smaller,
the planner's estimates are in fact more accurate.


Change NTUP_PER_BUCKET to 1 to improve hash join lookup speed.

Since this makes the bucket headers use ~10x as much memory, properly
account for that memory when we figure out whether everything fits
in work_mem.  This might result in some cases that previously used
only a single batch getting split into multiple batches, but it's
unclear as yet whether we need defenses against that case, and if so,
what the shape of those defenses should be.

It's worth noting that even in these edge cases, users should still be
no worse off than they would have been last week, because commit
45f6240a8fa9d35548eb2ef23dba2c11540aa02a saved a big pile of memory
on exactly the same workloads.

[参考]
1. http://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=8cce08f168481c5fc5be4e7e29b968e314f1b41e
2. http://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=45f6240a8fa9d35548eb2ef23dba2c11540aa02a
3. src/include/executor/hashjoin.h

Flag Counter
