PostgreSQL research

PostgreSQL 9.5 new feature - Add information about buffer pins to pg_buffercache

2015-06-17 15:30:58   查看原文>>

pg_buffercache这个插件是用来输出shared buffer的内容的，PostgreSQL 9.5 新增了一列pinning_backends，反映这个buffer 块正在被多少个backend pin。见BufferDesc。

Table F-14. pg_buffercache Columns

┌────────────────┬────────┬─────────────────────┬───────────────────────────────────────────────────────────────────┐
│      Name      │  Type  │     References      │                            Description                            │
├────────────────┼────────┼─────────────────────┼───────────────────────────────────────────────────────────────────┤
│bufferid        │integer │                     │ID, in the range 1..shared_buffers                                 │
├────────────────┼────────┼─────────────────────┼───────────────────────────────────────────────────────────────────┤
│relfilenode     │oid     │pg_class.relfilenode │Filenode number of the relation                                    │
├────────────────┼────────┼─────────────────────┼───────────────────────────────────────────────────────────────────┤
│reltablespace   │oid     │pg_tablespace.oid    │Tablespace OID of the relation                                     │
├────────────────┼────────┼─────────────────────┼───────────────────────────────────────────────────────────────────┤
│reldatabase     │oid     │pg_database.oid      │Database OID of the relation                                       │
├────────────────┼────────┼─────────────────────┼───────────────────────────────────────────────────────────────────┤
│relforknumber   │smallint│                     │Fork number within the relation; see include/storage/relfilenode.h │
├────────────────┼────────┼─────────────────────┼───────────────────────────────────────────────────────────────────┤
│relblocknumber  │bigint  │                     │Page number within the relation                                    │
├────────────────┼────────┼─────────────────────┼───────────────────────────────────────────────────────────────────┤
│isdirty         │boolean │                     │Is the page dirty?                                                 │
├────────────────┼────────┼─────────────────────┼───────────────────────────────────────────────────────────────────┤
│usagecount      │smallint│                     │Clock-sweep access count                                           │
├────────────────┼────────┼─────────────────────┼───────────────────────────────────────────────────────────────────┤
│pinning_backends│integer │                     │Number of backends pinning this buffer                             │
└────────────────┴────────┴─────────────────────┴───────────────────────────────────────────────────────────────────┘
pinning_backends可以反映热块，表示当前有多少个process正在pin 这个块，正常情况下应该都是0，因为pined buffer是不能被replacement的，也就是无法驱逐出shared buffer，对一个buffer操作完后会立即释放，例如插入，更新，查询，删除都需要pin buffer，但是都会很快释放。什么情况下我们能观察到这个值大于0呢？
例如用户正在等待锁的时候，会观察到，但是获取到锁后立即就会消失，即使事务未退出。
例子：
会话1

postgres=# begin;
BEGIN
postgres=# select * from test for update;
 id 
----
  1
  1
  1
  1
  1
  1
(6 rows)


当前无pinning 

postgres=# select * from pg_buffercache where relfilenode=(select relfilenode from pg_class where relname='test');
 bufferid | relfilenode | reltablespace | reldatabase | relforknumber | relblocknumber | isdirty | usagecount | pinning_backends 
----------+-------------+---------------+-------------+---------------+----------------+---------+------------+------------------
      190 |       16480 |          1663 |       13181 |             0 |              0 | t       |          5 |                0
(1 row)


会话2，等待

postgres=# begin;
BEGIN
postgres=# select * from test for update;


有1个pinning backend

postgres=# select * from pg_buffercache where relfilenode=(select relfilenode from pg_class where relname='test');
 bufferid | relfilenode | reltablespace | reldatabase | relforknumber | relblocknumber | isdirty | usagecount | pinning_backends 
----------+-------------+---------------+-------------+---------------+----------------+---------+------------+------------------
      190 |       16480 |          1663 |       13181 |             0 |              0 | t       |          5 |                1
(1 row)


会话3，等待

postgres=# begin;
BEGIN
postgres=# select * from test for update;


有2个pinning backend

postgres=# select * from pg_buffercache where relfilenode=(select relfilenode from pg_class where relname='test');
 bufferid | relfilenode | reltablespace | reldatabase | relforknumber | relblocknumber | isdirty | usagecount | pinning_backends 
----------+-------------+---------------+-------------+---------------+----------------+---------+------------+------------------
      190 |       16480 |          1663 |       13181 |             0 |              0 | t       |          5 |                2
(1 row)


提交会话1，会话2获得锁，并释放pin

postgres=# end;
COMMIT


所以只有会话3在等待，即1个pinning backend

postgres=# select * from pg_buffercache where relfilenode=(select relfilenode from pg_class where relname='test');
 bufferid | relfilenode | reltablespace | reldatabase | relforknumber | relblocknumber | isdirty | usagecount | pinning_backends 
----------+-------------+---------------+-------------+---------------+----------------+---------+------------+------------------
      190 |       16480 |          1663 |       13181 |             0 |              0 | t       |          5 |                1
(1 row)


提交会话2，会话3获得锁，并释放pin

postgres=# end;
COMMIT


0个pinning backend

postgres=# select * from pg_buffercache where relfilenode=(select relfilenode from pg_class where relname='test');
 bufferid | relfilenode | reltablespace | reldatabase | relforknumber | relblocknumber | isdirty | usagecount | pinning_backends 
----------+-------------+---------------+-------------+---------------+----------------+---------+------------+------------------
      190 |       16480 |          1663 |       13181 |             0 |              0 | t       |          5 |                0
(1 row)



[参考]
1. src/include/storage/buf_internals.h

/*
 *      BufferDesc -- shared descriptor/state data for a single shared buffer.
 *
 * Note: buf_hdr_lock must be held to examine or change the tag, flags,
 * usage_count, refcount, or wait_backend_pid fields.  buf_id field never
 * changes after initialization, so does not need locking.  freeNext is
 * protected by the buffer_strategy_lock not buf_hdr_lock.  The LWLocks can take
 * care of themselves.  The buf_hdr_lock is *not* used to control access to
 * the data in the buffer!
 *
 * An exception is that if we have the buffer pinned, its tag can't change
 * underneath us, so we can examine the tag without locking the spinlock.
 * Also, in places we do one-time reads of the flags without bothering to
 * lock the spinlock; this is generally for situations where we don't expect
 * the flag bit being tested to be changing.
 *
 * We can't physically remove items from a disk page if another backend has
 * the buffer pinned.  Hence, a backend may need to wait for all other pins
 * to go away.  This is signaled by storing its own PID into
 * wait_backend_pid and setting flag bit BM_PIN_COUNT_WAITER.  At present,
 * there can be only one such waiter per buffer.
 *
 * We use this same struct for local buffer headers, but the lock fields
 * are not used and not all of the flag bits are useful either.
 */
typedef struct BufferDesc
{
        BufferTag       tag;                    /* ID of page contained in buffer */
        BufFlags        flags;                  /* see bit definitions above */
        uint16          usage_count;    /* usage counter for clock sweep code */
        unsigned        refcount;               /* # of backends holding pins on buffer */
        int                     wait_backend_pid;               /* backend PID of pin-count waiter */

        slock_t         buf_hdr_lock;   /* protects the above fields */

        int                     buf_id;                 /* buffer's index number (from 0) */
        int                     freeNext;               /* link in freelist chain */
        LWLock     *io_in_progress_lock;        /* to wait for I/O to complete */
        LWLock     *content_lock;       /* to lock access to buffer contents */
} BufferDesc;


2. contrib/pg_buffercache/pg_buffercache_pages.c

3. http://www.postgresql.org/docs/devel/static/pgbuffercache.html
4. src/backend/storage/buffer/bufmgr.c

/*
 * PinBuffer -- make buffer unavailable for replacement.
 *
 * For the default access strategy, the buffer's usage_count is incremented
 * when we first pin it; for other strategies we just make sure the usage_count
 * isn't zero.  (The idea of the latter is that we don't want synchronized
 * heap scans to inflate the count, but we need it to not be zero to discourage
 * other backends from stealing buffers from our ring.  As long as we cycle
 * through the ring faster than the global clock-sweep cycles, buffers in
 * our ring won't be chosen as victims for replacement by other backends.)
 *
 * This should be applied only to shared buffers, never local ones.
 *
 * Note that ResourceOwnerEnlargeBuffers must have been done already.
 *
 * Returns TRUE if buffer is BM_VALID, else FALSE.  This provision allows
 * some callers to avoid an extra spinlock cycle.
 */
static bool
PinBuffer(volatile BufferDesc *buf, BufferAccessStrategy strategy)
{
        int                     b = buf->buf_id;
        bool            result;
        PrivateRefCountEntry *ref;

        ref = GetPrivateRefCountEntry(b + 1, true);

        if (ref == NULL)
        {
                ReservePrivateRefCountEntry();
                ref = NewPrivateRefCountEntry(b + 1);

                LockBufHdr(buf);
                buf->refcount++;
                if (strategy == NULL)
                {
                        if (buf->usage_count < BM_MAX_USAGE_COUNT)
                                buf->usage_count++;
                }
                else
                {
                        if (buf->usage_count == 0)
                                buf->usage_count = 1;
                }
                result = (buf->flags & BM_VALID) != 0;
                UnlockBufHdr(buf);
        }
        else
        {
                /* If we previously pinned the buffer, it must surely be valid */
                result = true;
        }

        ref->refcount++;
        Assert(ref->refcount > 0);
        ResourceOwnerRememberBuffer(CurrentResourceOwner,
                                                                BufferDescriptorGetBuffer(buf));
        return result;
}

/*
 * PinBuffer_Locked -- as above, but caller already locked the buffer header.
 * The spinlock is released before return.
 *
 * As this function is called with the spinlock held, the caller has to
 * previously call ReservePrivateRefCountEntry().
 *
 * Currently, no callers of this function want to modify the buffer's
 * usage_count at all, so there's no need for a strategy parameter.
 * Also we don't bother with a BM_VALID test (the caller could check that for
 * itself).
 *
 * Also all callers only ever use this function when it's known that the
 * buffer can't have a preexisting pin by this backend. That allows us to skip
 * searching the private refcount array & hash, which is a boon, because the
 * spinlock is still held.
 *
 * Note: use of this routine is frequently mandatory, not just an optimization
 * to save a spin lock/unlock cycle, because we need to pin a buffer before
 * its state can change under us.
 */
static void
PinBuffer_Locked(volatile BufferDesc *buf)
{
        int                     b = buf->buf_id;
        PrivateRefCountEntry *ref;

        /*
         * As explained, We don't expect any preexisting pins. That allows us to
         * manipulate the PrivateRefCount after releasing the spinlock
         */
        Assert(GetPrivateRefCountEntry(b + 1, false) == NULL);

        buf->refcount++;
        UnlockBufHdr(buf);

        ref = NewPrivateRefCountEntry(b + 1);
        ref->refcount++;
        ResourceOwnerRememberBuffer(CurrentResourceOwner,
                                                                BufferDescriptorGetBuffer(buf));
}

/*
 * IncrBufferRefCount
 *              Increment the pin count on a buffer that we have *already* pinned
 *              at least once.
 *
 *              This function cannot be used on a buffer we do not have pinned,
 *              because it doesn't change the shared buffer state.
 */
void
IncrBufferRefCount(Buffer buffer)
{
        Assert(BufferIsPinned(buffer));
        ResourceOwnerEnlargeBuffers(CurrentResourceOwner);
        ResourceOwnerRememberBuffer(CurrentResourceOwner, buffer);
        if (BufferIsLocal(buffer))
                LocalRefCount[-buffer - 1]++;
        else
        {
                PrivateRefCountEntry *ref;

                ref = GetPrivateRefCountEntry(buffer, true);
                Assert(ref != NULL);
                ref->refcount++;
        }
}

/*
 * UnpinBuffer -- make buffer available for replacement.
 *
 * This should be applied only to shared buffers, never local ones.
 *
 * Most but not all callers want CurrentResourceOwner to be adjusted.
 * Those that don't should pass fixOwner = FALSE.
 */
static void
UnpinBuffer(volatile BufferDesc *buf, bool fixOwner)
{
        PrivateRefCountEntry *ref;

        /* not moving as we're likely deleting it soon anyway */
        ref = GetPrivateRefCountEntry(buf->buf_id + 1, false);
        Assert(ref != NULL);

        if (fixOwner)
                ResourceOwnerForgetBuffer(CurrentResourceOwner,
                                                                  BufferDescriptorGetBuffer(buf));

        Assert(ref->refcount > 0);
        ref->refcount--;
        if (ref->refcount == 0)
        {
                /* I'd better not still hold any locks on the buffer */
                Assert(!LWLockHeldByMe(buf->content_lock));
                Assert(!LWLockHeldByMe(buf->io_in_progress_lock));

                LockBufHdr(buf);

                /* Decrement the shared reference count */
                Assert(buf->refcount > 0);
                buf->refcount--;

                /* Support LockBufferForCleanup() */
                if ((buf->flags & BM_PIN_COUNT_WAITER) &&
                        buf->refcount == 1)
                {
                        /* we just released the last pin other than the waiter's */
                        int                     wait_backend_pid = buf->wait_backend_pid;

                        buf->flags &= ~BM_PIN_COUNT_WAITER;
                        UnlockBufHdr(buf);
                        ProcSendSignal(wait_backend_pid);
                }
                else
                        UnlockBufHdr(buf);

                ForgetPrivateRefCountEntry(ref);
        }
}




Flag Counter
