PostgreSQL research

PostgreSQL array range get && Redis LRANGE

2012-09-05 10:30:59   查看原文>>

某业务使用redis作为前端缓存, 存储list, ver, appid[] 这样的数据
例如:
LIST : 

1:1, "1,2,3,4,5,6,7,8,9,......10W"
1:2, "2,1,5,4,3,6,7,8,9,......10W"


分页取数据, 每次取20个appid : 

LRANGE 1:2 1 20
LRANGE 1:2 21 40


appid[]的数据是通过PostgreSQL里面的表生成的.
运营人员对list进行调整后将写入到redis数据库中.
如select list_id||':'||ver, appid from tbl_appid_info where list_id=? and ver=? order by download_cnt desc;
假设redis异常, 或者在redis中未找到相关的list:ver信息. 业务系统将需要从PostgreSQL取数据, 也就是需要调用
select list_id||':'||ver, appid from tbl_appid_info where list_id=? and ver=? order by download_cnt desc limit ? offset ?.
这种SQL如果并发请求很大的情况下, 数据库负载将急剧升高, 无法满足业务需求.

所以需要针对这种情况做出一定的优化.
在PostgreSQL中array类型同样可以处理range操作的请求, 如 :

postgres=# select a[3:6] from (select array[1,2,3,4,4,35,6,7,8,9] as a)t ;
     a      
------------
 {3,4,4,35}
(1 row)


所以只要在PostgreSQL中增加一个存储和Redis中存储的类似的信息表即可.

create table t1(appid int[]);
insert into t1 select array_agg(id) from (select generate_series(1, 150000) id)t;
create table digoal(list_id int, list_ver int, appid int[]);
create table digoal1 (like digoal including all);
insert into digoal select generate_series(1, 500), 1, appid from t1;
insert into digoal1 select * from digoal;
insert into digoal select list_id, generate_series(2, 20), appid from digoal1;
alter table digoal add constraint digoal_pkey primary key (list_id, list_ver);


-- 单个LIST(15W个appid)的存储大小585KB

postgres=> select pg_column_size(appid)/1024 from t1;
 ?column? 
----------
      585
(1 row)


-- 10W个LIST(含索引)共计5945MB.

postgres=> select pg_total_relation_size('digoal')/1024/1024;
 ?column? 
----------
     5945
(1 row)



测试脚本 : 

postgres@-> cat digoal.sql 
\setrandom appid1 1 149980
\set appid2 20 + :appid1
\setrandom list_id 1 500
\setrandom list_ver 1 20
select appid[:appid1: :appid2] from digoal where list_id=:list_id and list_ver=:list_ver;



PostgreSQL 9.2rc1版本测试结果 : 

postgres@-> pgbench -M prepared -f ./digoal.sql -c 8 -j 8 -n -r -T 30 -h $PGDATA -U skyaaa postgres
transaction type: Custom query
scaling factor: 1
query mode: prepared
number of clients: 8
number of threads: 8
duration: 30 s
number of transactions actually processed: 360385
tps = 12012.515402 (including connections establishing)
tps = 12014.450228 (excluding connections establishing)
statement latencies in milliseconds:
        0.002113        \setrandom appid1 1 149980
        0.000629        \set appid2 20 + :appid1
        0.000523        \setrandom list_id 1 500
        0.000543        \setrandom list_ver 1 20
        0.659888        select appid[:appid1: :appid2] from digoal where list_id=:list_id and list_ver=:list_ver;



所以如果redis挂掉, PostgreSQL数据库还能支撑每秒1.2W次这样的请求.

【redis测试结果】
redis 版本2.4.17.
配置 : 

[root@db-172-16-3-150 ~]# cat /etc/redis.conf |grep -v "^$"|grep -v "^#"
daemonize yes
pidfile /var/run/redis.pid
port 6379
bind 0.0.0.0
unixsocket /tmp/redis.sock
unixsocketperm 755
timeout 0
loglevel verbose
logfile stdout
databases 16
save 900 1
save 300 10
save 60 10000
rdbcompression yes
dbfilename dump.rdb
dir /redis
slave-serve-stale-data yes
slave-priority 100
maxclients 128
maxmemory 10240000000
appendonly no
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
slowlog-log-slower-than 10000
slowlog-max-len 128
vm-enabled no
vm-swap-file /tmp/redis.swap
vm-max-memory 0
vm-page-size 32
vm-pages 134217728
vm-max-threads 4
hash-max-zipmap-entries 512
hash-max-zipmap-value 64
list-max-ziplist-entries 512
list-max-ziplist-value 64
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
activerehashing yes



测试数据插入程序 : 

[root@db-172-16-3-150 zzz]# cat a.c
#include <stdlib.h>
#include <string.h>
#include <stdio.h>

int main() {
  int id;
  int ver;
  char command[500];
  char * command1 = "/usr/local/bin/redis-benchmark -n 150000 -d 4 lpush mylist" ;
  char * command2 = "ele:rand:00000000000000000" ;
  for (id=1; id<=500; id++) {
    for (ver=1; ver<=20; ver++) {
      sprintf(command, "%s%d:%d %s", command1, id, ver, command2);
      fprintf(stdout, "command:%s\n", command);
      system(command);
    }
  }
  return 0;
}
[root@db-172-16-3-150 zzz]# gcc -O3 -Wall -Wextra -Werror -g ./a.c -o a



先把redis-server中的数据清除 : 

[root@db-172-16-3-150 zzz]# redis-cli
redis 127.0.0.1:6379> flushdb
OK
(22.16s)
redis 127.0.0.1:6379> exit



开始插入测试数据 : 
[root@db-172-16-3-150 zzz]# ./a
写入速度 : 

command:/usr/local/bin/redis-benchmark -n 150000 -d 4 lpush mylist5:12 ele:rand:00000000000000000
====== lpush mylist5:12 ele:rand:00000000000000000 ======
  150000 requests completed in 2.33 seconds
  50 parallel clients
  4 bytes payload
  keep alive: 1

99.90% <= 1 milliseconds
100.00% <= 1 milliseconds
64488.39 requests per second



使用lrange取出的测试结果 : 
lrange速度 : 

[root@db-172-16-3-150 ~]# redis-benchmark -s /tmp/redis.sock -c 8 -n 1000000 -t lrange mylist1:20 100000 100020
====== mylist1:20 100000 100020 ======
  1000000 requests completed in 70.61 seconds
  8 parallel clients
  3 bytes payload
  keep alive: 1

99.72% <= 1 milliseconds
100.00% <= 2 milliseconds
100.00% <= 43 milliseconds
100.00% <= 43 milliseconds
14163.30 requests per second



【其他】
1. 如果有更复杂的信息在PostgreSQL中可以选择存储为json数组类型. json[]
2. range 取值范围前段和后段性能差别不大.

postgres=> select appid[120001:120020] from test where list_id=100 and list_ver=5;
                                                                     appid                                                          
           
------------------------------------------------------------------------------------------------------------------------------------
-----------
 {120001,120002,120003,120004,120005,120006,120007,120008,120009,120010,120011,120012,120013,120014,120015,120016,120017,120018,1200
19,120020}
(1 row)

Time: 0.591 ms
postgres=> select appid[1:20] from test where list_id=100 and list_ver=5;
                        appid                         
------------------------------------------------------
 {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20}
(1 row)

Time: 0.598 ms


3. 由于redis-server是线程的, 所以在处理连接数较多的场景时, 相比PG更优优势. 当然如果PG前段加个pgbouncer也可以起到很好的优化效果.

