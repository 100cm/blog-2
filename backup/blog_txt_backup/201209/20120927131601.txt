PostgreSQL research

PostgreSQL 9.1.6 & 9.2.1 Data Corruption Issue patch

2012-09-27 13:16:01   查看原文>>

9.1.6和9.2.1修复了一个非常严重的BUG, 这个BUG的问题描述如下 : 

Description of the Problem
  Versions 9.1 and 9.2 of PostgreSQL have a bug with flushing dirty blocks from memory, or "checkpointing", introduced accidentally as a side effect of performance optimizations and new features, mainly Unlogged Tables. This bug can cause data of certain types to not be written to disk if the database shuts down or restarts for any of the following reasons:
  1. PostgreSQL crash
  2. Server crash or power loss
  3. "immediate" shutdown (pg_ctl -m immediate)
  4. "kill -9" or Out-Of-Memory-Kill of the postmaster service
  5. database is a standby which was promoted to master
  Under these circumstances, the database can suffer from recoverable data corruption. The nature of this corruption is such that it can produce wrong, but seemingly valid, answers to queries, so it is critical that users who may have been affected by this corruption take steps to clean it up very soon.
  First, there is a low probability of corruption of BTREE and GIN indexes. Shutting down cleanly will limit the further spread of this issue. It's very likely that if corruption has occurred that it would be visible in the form of error messages when the index is used.
  Second, there is a significant probability of corruption of relation visibility maps (approaching 100% on standbys). This affects 9.1 very differently from 9.2, however. On PostgreSQL 9.1 the worst consequence is some transient inefficiency and/or failure to recover free space during VACUUM. On PostgreSQL 9.2, we use the visibility map during index only scans and so these are likely to produce wrong answers.
The PostgreSQL Global Development Group apologizes for the inconvenience caused by these issues.


  也就是说遇到这5种情况, 就可能触发这个BUG.
  而这个BUG最恐怖的地方应该是对于9.2 由于使用index only scan需要用到VM文件, 这个BUG可能导致走index only scan的执行计划和其他执行计划得到不一致的结果, 原因是vm文件flush 异常. 
  但是我在使用以上3,5这两种情况测试9.2.0的版本时并没有触发这个BUG. index only scan和其他执行计划得到的同样的结果.
  这个BUG的修复涉及到以下两个源码文件 : 

src/backend/access/transam/xlogutils.c


src/backend/storage/buffer/bufmgr.c


以下列举9.2.1对应这个BUG的修改如下 : 

1. Properly set relpersistence for fake relcache entries.

   This can result in buffers failing to be properly flushed at checkpoint time, leading to data loss.


PostgreSQL 9.1.6 9.2.1 Data Corruption Issue patch - 德哥@Digoal - The Heart,The World.
 
2. Fix bufmgr so CHECKPOINT_END_OF_RECOVERY behaves as a shutdown checkpoint.

   Recovery code documents clearly that a shutdown checkpoint is executed at end of recovery - a shutdown checkpoint WAL record is written but the buffer manager had been altered to treat end of recovery as a normal checkpoint. This bug exacerbates the bufmgr relpersistence bug.

/*
* BufferSync -- Write out all dirty buffers in the pool.
*
* This is called at checkpoint time to write out all dirty shared buffers.
* The checkpoint request flags should be passed in. If CHECKPOINT_IMMEDIATE
* is set, we disable delays between writes; if CHECKPOINT_IS_SHUTDOWN is
* set, we write even unlogged buffers, which are otherwise skipped. The
* remaining flags currently have no effect here.
*/


PostgreSQL 9.1.6 9.2.1 Data Corruption Issue patch - 德哥@Digoal - The Heart,The World.
 
我这边用到的再现BUG测试 : 
1. 单库
测试表 : 

digoal=> \d user_info
                Table "digoal.user_info"
  Column   |              Type              | Modifiers 
-----------+--------------------------------+-----------
 id        | integer                        | not null
 firstname | name                           | 
 lastname  | name                           | 
 age       | integer                        | 
 email     | text                           | 
 pwd       | text                           | 
 crt_time  | timestamp(0) without time zone | 
 mod_time  | timestamp(0) without time zone | 
Indexes:
    "user_info_pkey" PRIMARY KEY, btree (id)
    "idx_user_info_mod_time" btree (mod_time)


测试数据 : 

insert into user_info (id,firstname,lastname,age,email,pwd,crt_time) select generate_series(1,10000000),'zhou','digoal',29,'digoal@1
26.com','thisistest',now();


测试脚本 : 

pg9.2.0@db-172-16-3-150-> cat pgbench.sql 
\setrandom id 1 10000000
update user_info set mod_time=now() where id=:id;


测试使用pgbench : 

pg9.2.0@db-172-16-3-150-> pgbench -M prepared -c 4 -f ./pgbench.sql -j 4 -n -r -T 600 -U digoal digoal


加压过程中将数据库异常关闭,

pg_ctl stop -m immediate


重启数据库
查询数据 : 

pg9.2.0@db-172-16-3-150-> psql
psql (9.2.0)
Type "help" for help.

postgres=# \c digoal digoal
You are now connected to database "digoal" as user "digoal".
digoal=> explain select count(*) from user_info where mod_time >= '2012-09-27 08:10:00' and mod_time <='2012-09-27 13:20:50';
                                                                         QUERY PLAN                                                 
                         
------------------------------------------------------------------------------------------------------------------------------------
-------------------------
 Aggregate  (cost=270743.89..270743.90 rows=1 width=0)
   ->  Index Only Scan using idx_user_info_mod_time on user_info  (cost=0.00..267655.92 rows=1235188 width=0)
         Index Cond: ((mod_time >= '2012-09-27 08:10:00'::timestamp without time zone) AND (mod_time <= '2012-09-27 13:20:50'::times
tamp without time zone))
(3 rows)
digoal=> select count(*) from user_info where mod_time >= '2012-09-27 08:10:00' and mod_time <='2012-09-27 13:20:50';
  count  
---------
 6446993
(1 row)

digoal=> set enable_indexonlyscan=off;
SET
digoal=> set enable_indexscan=off;
SET
digoal=> set enable_bitmapscan=off;
SET
digoal=> explain select count(*) from user_info where mod_time >= '2012-09-27 08:10:00' and mod_time <='2012-09-27 13:20:50';
                                                                       QUERY PLAN                                                   
                     
------------------------------------------------------------------------------------------------------------------------------------
---------------------
 Aggregate  (cost=426642.09..426642.10 rows=1 width=0)
   ->  Seq Scan on user_info  (cost=0.00..422820.99 rows=1528437 width=0)
         Filter: ((mod_time >= '2012-09-27 08:10:00'::timestamp without time zone) AND (mod_time <= '2012-09-27 13:20:50'::timestamp
 without time zone))
(3 rows)

digoal=> select count(*) from user_info where mod_time >= '2012-09-27 08:10:00' and mod_time <='2012-09-27 13:20:50';
  count  
---------
 6446993
(1 row)



2. standby 库(与以上相同, 只是测试的是standby 库.)
2.1 pgbench在primary库上执行.
2.2 在standby库上执行pg_ctl promote
2.3 在standby库上查询以上SQL.
使用indexonlyscan和seqscan得到的结果一致.
所以我在测试中用到的是BUG PATCH中提到的3和5点, 但是都没有能够触发这个BUG.

【修复】
修复这个BUG, 9.1和9.2的操作略有区别, 摘录如下 : 
Steps for Users of PostgreSQL 9.1

If you are running 9.1, and suspect that you may be vulnerable to database corruption because your database has shut down unexpectedly or failed over during the last few months:
Download new 9.1.6 packages
Do a clean shutdown of PostgreSQL, using one of the following mechanisms:
init script or service manager
pg_ctl -m smart stop
pg_ctl -m fast stop
Install 9.1.6
Restart the database system
Gradually rebuild all of your BTree and GIN indexes (see below)
Schedule a manual vacuum of the whole database during a convenient slow period (see below)
If you are planning to upgrade to PostgreSQL 9.2 using pg_upgrade, it is critical for you to run the full database VACUUM first.



Steps for Users of PostgreSQL 9.2

If you are running 9.2.0, and suspect that you may be vulnerable to database corruption because your database has shut down unexpectedly or failed over during the last two weeks:
Download new 9.2.1 packages
Do a clean shutdown of PostgreSQL, using one of the following mechanisms:
init script or service manager
pg_ctl -m smart stop
pg_ctl -m fast stop
Install 9.2.1
Restart the database system
VACUUM all tables in your database immediately
Gradually rebuild all of your BTree and GIN indexes (see below)


How to VACUUM All Tables

To correct corruption of the visibility map, users should run a vacuum and force a scan of all database blocks in order to reset the entire map. Since this means effectively scanning the entire database, it will generate considerable IO and take significant time to execute for large databases. One way to ameliorate the impact on concurrently running database load is to use cost delay to spread out the vacuum:
   SET vacuum_cost_delay = 50;


Interactive VACUUM

For each database, you should:
log in to psql as the Postgres superuser
set vacuum_cost_delay, if doing so
run "VACUUM ( FREEZE, VERBOSE, ANALYZE );" (ANALYZE is optional)
This will produce a lot of output, allowing you to track progress of the full-database vacuum.
You can also VACUUM one table at a time instead of doing them all one after the other, provided that you have some way to track which tables have and have not been vacuumed.


vacuumdb

If you have multiple databases to vacuum, you may find it convenient to use the vacuumdb utility instead. This would work by:
set vacuum_cost_delay in postgresql.conf, if doing so (and reload database)
run "vacuumdb -F -v -z -a" as the postgres superuser
Note that you may need to give vacuumdb additional parameters in order to connect with the database server. The -z (analyze) and -v (verbose) options are optional.


Rebuild BTree/GIN Indexes

It is likely that any indexes which are corrupted because of the issues fixed in this update release will display error messages when accessed, and can be easily identified. However, it is possible (though unlikely) that a few indexes may be corrupted so that they return incorrect answers without errors.
The VACUUM FREEZE recommended above will correct some types of index corruption. However, users who have strong data integrity concerns, or feel they are especially at risk due to multiple crashes or failovers in their server history, should take the extra step of rebuilding indexes in order to eliminate any possible corruption.


Rebuilding an Individual Index

Whether you are being precautionary, or because you have found an index corruption error, you can rebuild indexes one at a time. The simplest way is via REINDEX.
   REINDEX TABLE <tablename>;
or for a single index:
   REINDEX INDEX <indexname>;
You may want to increase the RAM available to REINDEX, by increasing maintenance_work_mem, up to 1/8 of your available RAM (up to a maximum of 2GB). REINDEX takes a full table write lock, however, and depending on the size of the table, can take a considerable time to run. In order to rebuild indexes while under concurrent database load, use CREATE INDEX CONCURRENTLY:
   CREATE INDEX CONCURRENTLY <indexname>_tmp <index_definition>;
   BEGIN;
   DROP INDEX <indexname>;
   ALTER INDEX <indexname>_tmp RENAME TO <indexname>;
   END;
This locks the table only during the final drop and rename stage. It is, however, more complex.
Either approach will generate considerable IO while running on large tables.


Getting a List of Btree and GIN Indexes

Regardless of your approach towards rebuilding your indexes, you may want to get a list of all BTree and GIN indexes in the database. BTree is the most common type of index, so this will include most of the indexes in your database. Given that GiST indexes can be quite large, though, you may want to omit them from rebuilding.
Use this query:
   SELECT tablename, indexname, indexdef
   FROM pg_indexes
   WHERE ( indexdef ILIKE '%USING btree%'
     OR indexdef ILIKE '%USING GIN%' )
     AND schemaname <> 'pg_catalog'
   ORDER BY tablename, indexname;


Reindexing Everything

If you can afford the required downtime, and want to be absolutely certain that you've prevented all corruption, you can reindex every index in your database using the reindexdb utility. Note that this will cause GiST indexes to be rebuilt as well, even though they are not in danger of corruption.
Run the following as the postgres superuser to reindex one database:
   reindexdb <databasename>
Or to reindex all databases:
   reindexdb -a
Additional options may be required for reindexdb to connect to your database. Since reindexdb will take a lock on entire tables in your installation, one at a time, this is best done during a downtime.



【参考】
http://wiki.postgresql.org/wiki/20120924updaterelease
https://github.com/postgres/postgres/commit/eb6e9b5ea4a5e733da705c053906f3aff47c9bf5
https://github.com/postgres/postgres/commit/9c855045809362f2cdabc110afd6ab8a4b250507
