PostgreSQL research

【摘录】把文件永久cache到内存的工具filecache

2012-03-19 8:23:39   查看原文>>

相比pgfincore, filecache不仅仅适合于PostgreSQL, 对其他数据库同样适用.
filecache比较适合数据文件比较小, 同时又被频繁被访问的文件(例如例子中提到的数据字典) .
【pgfincore介绍】
http://blog.163.com/digoal@126/blog/static/163877040201062944945126/
【filecache原文】
http://blog.osdba.net/?post=78 
程序解决的问题：
　　我们知道在PostgreSQL数据库下，可以使用pgfincore工具把数据文件cache到文件系统内存中，但如果有其它文件被频繁访问，
　　被cache的数据文件还是有可能被挤出内存，导致cache失效。我写的这个工具可以保证文件永久被cache到内存中，不被交换出去。

程序的原理说明：
   此程序通过打开一个文件，把文件的内容映射到内存中，然后把这些内存lock住。只要这个程序不退出，这个文件就被永久的cache住了。
   其它程序访问这个文件时，也一样不会在从物理磁盘上读了，而是直接从这块共享内存中读取了。

程序参数说明：
  -f　<cfgfile>
     指定一个配置文件，在配置文件中需要cache的文件占一行，这样filecache就可以同时cache很多个文件。
  -c  <百分比>
     指定cache文件的百分比，一般可以取110，意思是110%。cache文件时，如果这个值设置为100%时，
     当文件变大时，变大的部分并不会被永远cache在内存中，于是增加了这个参数，设置为110%时，
     当文件增加大小到原先的110%时，后面增大的10%的空间仍然会被cache中内存中。
  -p  <pid file>
     指定一个pid文件，会把自己的进程号写到这个文件中，如果不指定，会生成filecache.pid的文件。
     
  -M <最大的文件尺寸>
     大于这值的文件，就不会被cache，防止太大的文件把内存给撑爆。

使用说明:
    1. 在Linux下使用时，需要设置max locked memory大于本程序cache文件所需要的内存大小。使用之前使用通过ulimit -a检查：
    postgres@mydb:/home/postgres/filecache>ulimit -a
    .....
    max locked memory       (kbytes, -l) unlimited
    .....
    .....

    2. 在solaris下使用，请使用下面的命令设置可以cache住内存的最大大小：
    projadd -n -U postgres -G postgres -K "project.max-locked-memory=(priv,4096MB,deny)" user.postgres
    使用下面命令对用户postgres用户加权限，让postgres可以锁定内存：
    usermod -K defaultpriv=basic,priv_proc_lock_memory postgres
    
    ３. 此工具后指定一个存有需要cache文件的列表文件，这个列表可以自行写一个脚本来生成。cachedict.sh脚本就是一个这样的脚本，
    此脚本把所有的数据字典表的文件和索引的文件都生成到cachedict.lst文件就，然后运行filecache，这样就把PostgreSQL的所有
    字典表都永久的cache到内存中的，如果你需要cache其它表，可以改一改这个脚本，就可以实现你的功能，当然你也可以写一个全新的脚
    本来实现你的功能。
    

Flag Counter
