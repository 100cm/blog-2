PostgreSQL research

PostgreSQL 9.4 patch: extensible TOAST compression support

2013-07-29 17:00:38   查看原文>>

PostgreSQL 9.4 可能会新增的一个补丁, 支持TOAST表的压缩算法扩展. 
PostgreSQL 存储变长字段时可以选择基表存储或者TOAST存储, 当使用TOAST存储时, 可以支持压缩或不压缩存储, 目前压缩算法仅仅支持pglz算法. 
TOAST存储的字段最大可以存储1G的长度. 字段长度使用4字节来表示, 只用到30个比特位, 多出2个比特位. 00表示不压缩, 01表示压缩. 如果要支持更多的压缩算法需要用到这2个比特位. 另外该补丁扩展了1个字节用来支持更多的压缩算法.

* add lz4 compression algorithm (2 clause bsd)
* move compression algorithms into own subdirectory
* clean up compression/decompression functions
* allow 258 compression algorithms, uses 1byte extra for any but the
  first three
* don't pass a varlena to pg_lzcompress.c anymore, but data directly
* add pglz_long as a test fourth compression method that uses the +1
  byte encoding
* us postgres' endian detection in snappy for compatibility with osx


目前补丁新增了3个压缩算法, snappy, lz4, pglz_long
对TOAST有兴趣的朋友可以同时阅读如下BLOG : 
http://blog.163.com/digoal@126/blog/static/16387704020136264471312/

下面测试一下这个补丁.
1. 下载一个9.4 开发版源码.

http://git.postgresql.org/gitweb/?p=postgresql.git;a=snapshot;h=129759d6a539059cde85d0dad19992ff45da3bb4;sf=tgz


2. 编译安装

cd postgresql-129759d
./configure --prefix=/home/pg94/pgsql9.4devel --with-pgport=1921 --with-perl --with-tcl --with-python --with-openssl --with-pam --without-ldap --with-libxml --with-libxslt --enable-thread-safety --with-wal-blocksize=16 && gmake && gmake install


注意, 如果是直接打补丁然后编译安装, 可能会报错utils/errcodes.h不存在, 如果你遇到这个错误, 请先不要打补丁, 编译安装完后再打补丁.

3. 下载补丁

cd postgresql-129759d
wget http://www.postgresql.org/message-id/attachment/29333/0001-Add-snappy-compression-algorithm-to-contrib.patch
wget http://www.postgresql.org/message-id/attachment/29334/0002-Add-lz4-compression-algorithm-to-contrib.patch
wget http://www.postgresql.org/message-id/attachment/29335/0003-Introduce-pluggable-compression.patch



4. 打补丁

patch -p1 < ./0001-Add-snappy-compression-algorithm-to-contrib.patch 
patch -p1 < ./0002-Add-lz4-compression-algorithm-to-contrib.patch 
patch -p1 < ./0003-Introduce-pluggable-compression.patch



5. 编译安装
报错

../../src/common/libpgcommon_srv.a(snappy_srv.o): In function `snappy_uncompress':
snappy.c:(.text+0x531): undefined reference to `le32toh'
snappy.c:(.text+0x782): undefined reference to `le32toh'
../../src/common/libpgcommon_srv.a(snappy_srv.o): In function `snappy_compress':
snappy.c:(.text+0xdb2): undefined reference to `htole16'
snappy.c:(.text+0xdec): undefined reference to `htole16'
snappy.c:(.text+0xf6f): undefined reference to `htole16'
collect2: ld returned 1 exit status
gmake[2]: *** [postgres] Error 1
gmake[2]: Leaving directory `/home/pg94/postgresql-129759d/src/backend'
gmake[1]: *** [all-backend-recurse] Error 2
gmake[1]: Leaving directory `/home/pg94/postgresql-129759d/src'
gmake: *** [all-src-recurse] Error 2



修复错误, 修改snappy.c, 把snappy-compat.h中le32toh和htole16的定义拷贝到snappy.c 如下 : 

vi src/common/snappy/snappy.c
#include "snappy-compat.h"

#if    __DARWIN_BYTE_ORDER == __DARWIN_LITTLE_ENDIAN
#  define       htole16(x) (x)
#  define       le32toh(x) (x)
#elif  __DARWIN_BYTE_ORDER == __DARWIN_BIG_ENDIAN
#  define       htole16(x) __DARWIN_OSSwapInt16(x)
#  define       le32toh(x) __DARWIN_OSSwapInt32(x)
#else
#  error "Endianness is undefined"
#endif



6. 初始化数据库

initdb -D $PGDATA -E UTF8 --locale=C -W -U postgres


7. 启动数据库, 测试
查看新增的一个参数toast_compression_algo.
支持4个值, 0,1,2,3. 分别表示压缩算法0: pglz, 1: snappy, 2: lz4, 3: pglz_long

digoal=# select * from pg_settings where name ~ 'toast_compression_algo';
-[ RECORD 1 ]----------------------------------------------------------------------
name       | toast_compression_algo
setting    | 2
unit       | 
category   | Client Connection Defaults / Statement Behavior
short_desc | chooses the compression algo: 0: pglz, 1: snappy, 2: lz4, 3: pglz_long
extra_desc | 
context    | user
vartype    | integer
source     | default
min_val    | 0
max_val    | 3
enumvals   | 
boot_val   | 2
reset_val  | 2
sourcefile | 
sourceline | 


生成1份大数据.

digoal=# copy (select repeat(md5(random()::text),10000) from generate_series(1,10000)) to '/home/pg94/toast.dmp';
COPY 10000
Time: 29034.168 ms
大小3GB
-rw-r--r-- 1 pg94 pg94 3.0G Jul 29 16:39 toast.dmp


7.1 导入到pglz数据表

digoal=# set toast_compression_algo=0;
SET
Time: 0.323 ms
digoal=# create table test_pglz(info text);
CREATE TABLE
Time: 225.286 ms
digoal=# copy test_pglz from  '/home/pg94/toast.dmp' ;
COPY 10000
Time: 65924.167 ms


确认数据在toast表中, 查看toast表和基表大小

digoal=# select * from pg_class where relname='test_pglz';
-[ RECORD 1 ]--+----------
relname        | test_pglz
relnamespace   | 2200
reltype        | 16393
reloftype      | 0
relowner       | 10
relam          | 0
relfilenode    | 16391
reltablespace  | 0
relpages       | 64
reltuples      | 10000
relallvisible  | 0
reltoastrelid  | 16394
reltoastidxid  | 0
relhasindex    | f
relisshared    | f
relpersistence | p
relkind        | r
relnatts       | 1
relchecks      | 0
relhasoids     | f
relhaspkey     | f
relhasrules    | f
relhastriggers | f
relhassubclass | f
relispopulated | t
relfrozenxid   | 1687
relminmxid     | 1
relacl         | 
reloptions     | 
digoal=# select pg_relation_size('16394'::regclass)/1024/1024;
-[ RECORD 1 ]
?column? | 39
digoal=# select pg_relation_size('test_pglz'::regclass)/1024/1024.0;
-[ RECORD 1 ]--------------------
?column? | 0.50000000000000000000


7.2 导入到snappy数据表 : 

digoal=# set toast_compression_algo=1;
SET
Time: 0.291 ms
digoal=# create table test_snappy(info text);
CREATE TABLE
Time: 27.662 ms
digoal=# copy test_snappy from  '/home/pg94/toast.dmp' ;
COPY 10000
Time: 42846.571 ms
查看表大小
digoal=# select reltoastrelid from pg_class where relname='test_snappy';
-[ RECORD 1 ]-+------
reltoastrelid | 26400
digoal=# select pg_relation_size('test_snappy'::regclass)/1024/1024.0;
-[ RECORD 1 ]--------------------
?column? | 0.50000000000000000000
digoal=# select pg_relation_size('26400'::regclass)/1024/1024.0;
-[ RECORD 1 ]------------------
?column? | 156.2500000000000000


7.3 导入到lz4数据表 :
lz4压缩比最高, 压缩后未达到TOAST_TUPLE_THRESHOLD阈值,  所以未存储到TOAST中.

digoal=# set toast_compression_algo=2;
SET
Time: 0.200 ms
digoal=# create table test_lz4(info text);
CREATE TABLE
Time: 33.929 ms
digoal=# copy test_lz4 from  '/home/pg94/toast.dmp' ;
COPY 10000
Time: 33985.440 ms
digoal=# select reltoastrelid from pg_class where relname='test_lz4';
-[ RECORD 1 ]-+------
reltoastrelid | 36406
digoal=# select pg_relation_size('test_lz4'::regclass)/1024/1024.0;
-[ RECORD 1 ]-----------------
?column? | 13.0234375000000000
digoal=# select pg_relation_size('36406'::regclass)/1024/1024.0;
-[ RECORD 1 ]--------------------
?column? | 0.00000000000000000000



7.4 导入到pglz_long数据表 :

digoal=# set toast_compression_algo=3;
SET
Time: 0.278 ms
digoal=# create table test_pglz_long(info text);
CREATE TABLE
Time: 25.622 ms
digoal=# copy test_pglz_long from  '/home/pg94/toast.dmp' ;
COPY 10000
Time: 72135.470 ms
digoal=# select reltoastrelid from pg_class where relname='test_pglz_long';
-[ RECORD 1 ]-+------
reltoastrelid | 36412
digoal=# select pg_relation_size('test_pglz_long'::regclass)/1024/1024.0;
-[ RECORD 1 ]--------------------
?column? | 0.50000000000000000000
digoal=# select pg_relation_size('36412'::regclass)/1024/1024.0;
-[ RECORD 1 ]-----------------
?column? | 39.0625000000000000


解压缩速度测试 : 

digoal=# copy test_pglz to '/dev/null' with binary;
COPY 10000
Time: 7978.786 ms
digoal=# copy test_snappy to '/dev/null' with binary;
COPY 10000
Time: 3401.034 ms
digoal=# copy test_lz4 to '/dev/null' with binary;
COPY 10000
Time: 2533.736 ms
digoal=# copy test_pglz_long to '/dev/null' with binary;
COPY 10000
Time: 7994.396 ms


最后测试一下不压缩存储

digoal=# alter table test_uncompress_toast alter column info set storage external;
ALTER TABLE
Time: 0.695 ms
digoal=# copy test_uncompress_toast from '/home/pg94/toast.dmp' ;
COPY 10000
Time: 128210.037 ms
digoal=# copy test_uncompress_toast to '/dev/null' with binary;
COPY 10000
Time: 5090.905 ms



[小结]
1. 本文用到的测试数据显示lz4压缩算法压缩比最高, 压缩和解压缩速度最快.
2. 除了使用数据库本身的数据压缩, 还可以考虑文件系统层面的压缩, 例如ZFS文件系统. 
http://www.citusdata.com/blog/64-zfs-compression

[参考]
1. http://blog.163.com/digoal@126/blog/static/16387704020136264471312/
2. http://www.postgresql.org/message-id/flat/20130530114225.GI4201@awork2.anarazel.de#20130530114225.GI4201@awork2.anarazel.de
3. http://www.postgresql.org/message-id/flat/20130216164231.GA15069@awork2.anarazel.de#20130216164231.GA15069@awork2.anarazel.de
4. http://www.postgresql.org/message-id/flat/20130605150144.GD28067@alap2.anarazel.de#20130605150144.GD28067@alap2.anarazel.de
5. 

On 2013-06-15 12:20:28 +0200, Andres Freund wrote:
> On 2013-06-14 21:56:52 -0400, Robert Haas wrote:
> > I don't think we need it.  I think what we need is to decide is which
> > algorithm is legally OK to use.  And then put it in.
> > 
> > In the past, we've had a great deal of speculation about that legal
> > question from people who are not lawyers.  Maybe it would be valuable
> > to get some opinions from people who ARE lawyers.  Tom and Heikki both
> > work for real big companies which, I'm guessing, have substantial
> > legal departments; perhaps they could pursue getting the algorithms of
> > possible interest vetted.  Or, I could try to find out whether it's
> > possible do something similar through EnterpriseDB.
> 
> I personally don't think the legal arguments holds all that much water
> for snappy and lz4. But then the opinion of a european non-lawyer doesn't
> hold much either.
> Both are widely used by a large number open and closed projects, some of
> which have patent grant clauses in their licenses. E.g. hadoop,
> cassandra use lz4, and I'd be surprised if the companies behind those
> have opened themselves to litigation.
> 
> I think we should preliminarily decide which algorithm to use before we
> get lawyers involved. I'd surprised if they can make such a analysis
> faster than we can rule out one of them via benchmarks.
> 
> Will post an updated patch that includes lz4 as well.

Attached.

Changes:
* add lz4 compression algorithm (2 clause bsd)
* move compression algorithms into own subdirectory
* clean up compression/decompression functions
* allow 258 compression algorithms, uses 1byte extra for any but the
  first three
* don't pass a varlena to pg_lzcompress.c anymore, but data directly
* add pglz_long as a test fourth compression method that uses the +1
  byte encoding
* us postgres' endian detection in snappy for compatibility with osx

Based on the benchmarks I think we should go with lz4 only for now. The
patch provides the infrastructure should somebody else want to add more
or even proper configurability.

Todo:
* windows build support
* remove toast_compression_algo guc
* remove either snappy or lz4 support
* remove pglz_long support (just there for testing)

New benchmarks:

Table size:
                          List of relations
 Schema |        Name        | Type  | Owner  |  Size  | Description 
--------+--------------------+-------+--------+--------+-------------
 public | messages_pglz      | table | andres | 526 MB | 
 public | messages_snappy    | table | andres | 523 MB | 
 public | messages_lz4       | table | andres | 522 MB | 
 public | messages_pglz_long | table | andres | 527 MB | 
(4 rows)

Workstation (2xE5520, enough s_b for everything):

Data load:
pglz:           36643.384 ms
snappy:         24626.894 ms
lz4:            23871.421 ms
pglz_long:      37097.681 ms

COPY messages_* TO '/dev/null' WITH BINARY;
pglz:           3116.083 ms
snappy:         2524.388 ms
lz4:            2349.396 ms
pglz_long:      3104.134 ms

COPY (SELECT rawtxt FROM messages_*) TO '/dev/null' WITH BINARY;
pglz:           1609.969 ms
snappy:         1031.696 ms
lz4:             886.782 ms
pglz_long:      1606.803 ms


On my elderly laptop (core 2 duo), too load shared buffers:

Data load:
pglz:           39968.381 ms
snappy:         26952.330 ms
lz4:            29225.472 ms
pglz_long:      39929.568 ms

COPY messages_* TO '/dev/null' WITH BINARY;
pglz:           3920.588 ms
snappy:         3421.938 ms
lz4:            3311.540 ms
pglz_long:      3885.920 ms

COPY (SELECT rawtxt FROM messages_*) TO '/dev/null' WITH BINARY;
pglz:           2238.145 ms
snappy:         1753.403 ms
lz4:            1638.092 ms
pglz_long:      2227.804 ms


Greetings,

Andres Freund

-- 
 Andres Freund                     http://www.2ndQuadrant.com
 PostgreSQL Development, 24x7 Support, Training & Services


6. 

+ * toast_get_compress_size_algo -
+ *
+ * get metadata about compressed Datum
+ *
+ *
+ * How to disambiguate between compression strategies:
+ *
+ * For historical reasons the first 4 bytes of a compressed Datum are used to
+ * store the raw size of the datum as an unsigned integer. Since the length
+ * cannot be more than 1GB due to general toast limitations we have the 2 high
+ * bits to disambiguate whether the Datum has been compressed with the legacy
+ * pglz or something else. We cannot change the meaning of Datums with the
+ * first 2 bits unset since we need to support the old ondisk format.
+ *
+ * Since earlier our only compression format was pglz, which stored two 0 bits
+ * we can use those two bits to discern different formats. If the compresion
+ * format we use has a higher numeric value than 2 we store b11/3 in the high
+ * bits and use an extra byte for storing the numeric id - 2 in an extra byte.
+ *
+ * So storage looks like:
+ * 1) [4 byte varlena header]
+ * 2) [4 byte uncompressed length, 2 high bits for algorithm]
+ * 3) [1 optional byte of algorithm id - 2]
+ * 4) [compressed data ...]
+ *
+ * On little endian the storage for 2) looks like:
+ * [1st length byte][3rd length byte][2nd length byte][6 bit length][2 bit algorithm]
+ *
+ * Due to the 2 high bits only being in the 4th bit we cannot store the
+ * algorithm in a convenient format if we need more than two bits to represent
+ * it.
+ * ----
+ */


