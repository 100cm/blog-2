PostgreSQL research

Configure NFS over GFS or GFS2 Consideration

2010-09-28 14:03:36   查看原文>>

[From REDHAT's Advise]
NFS通常用在共享存储的环境，是一种廉价的共享存储的解决方案。于此类似的还有分布式文件系统共享解决方案。
如果要实现透明的FAILOVER，NFS目前是做不到的（使用RHCS+GFS+NFS+VIP也是没有办法做到的，并且也不推荐这么做，后面会有红帽的官方言论）。
在REDHAT平台，同一台主机如果要做廉价的FAILOVER的共享存储。可以考虑GNBD，这是网络块设备，可以配置DM MP。不过也会有莫名其妙的问题。稍微有点钱的话比较安全一点的做法是使用ISCSI设备进行共享，再加上逻辑卷管理中的MIRROR功能可以实现底层存储的FAILOVER。很有钱的话最好还是使用SAN存储，管理方便，性能也是比较好的。而在FAILOVER这块
没钱就找软件解决方案，有钱就直接上硬件解决方案，例如一些高端存储的存储复制。
另外，STEELEYE,DRBD是一种介于底层硬件和文件系统之间的一种数据同步方案。在某些场合也可以考虑使用。
撤远了，下面来说一下NFS OVER GFS，
1. 文件系统锁的建议
Warning
If the GFS or GFS2 file system is NFS exported, and NFS client applications use POSIX locks, then you must mount the file system with the localflocks option. The intended effect of this is to force POSIX locks from each server to be local: i.e., non-clustered, independent of each other. (A number of problems exist if
GFS/GFS2 attempts to implement POSIX locks from NFS across the nodes of a cluster.) For applications running on NFS clients, localized POSIX locks means that two clients can hold the same lock concurrently if the two clients are mounting from different servers. If all clients mount NFS from one server, then the
problem of separate servers granting the same locks independently goes away.
If you are unsure whether an application uses fcntl()/POSIX locks, for safety's sake you should mount your file system with the localflocks option.
在NFS服务器也就是集群服务器上挂载被EXPORT的GFS文件系统必须小心，建议使用本地文件锁来MOUNT。因为LOCK_DLM挂载的话可能有意想不到的后果。
2. 其他
In addition to the locking considerations, you should take the following into account when configuring an NFS service over a GFS or GFS2 file system.

  • Red Hat supports only Red Hat Cluster Suite configurations using NFSv3 with locking in an active/passive configuration with the following characteristics:
      □ The backend file system is a GFS or GFS2 file system running on a 2 to 16 node cluster.
      □ An NFSv3 server is defined as a service exporting the entire GFS/GFS2 file system from a single cluster node at a time.
      □ The NFS server can fail over from one cluster node to another (active/passive configuration).
      □ No access to the GFS/GFS2 file system is allowed except through the NFS server. This includes both local GFS/GFS2 file system access as well as access through Samba or Clustered Samba.
      □ There is no NFS quota support on the system.
    This configuration provides HA for the file system and reduces system downtime since a failed node does not result in the requirement to execute the fsck command when failing the NFS server from one node to another.
  • The fsid= NFS option is mandatory for NFS exports of GFS/GFS2.
  • There is currently an issue with failover and failback when using NFSv3 over GFS with TCP when the following scenario comes into play:
     1. Client A mounts from server 1.
     2. The system administrator moves NFS service from server 1 to server 2.
     3. The client resumes I/O operations.
     4. The system administrator moves NFS service from server 2 to server 1.
    In this situation, the NFS service on server 1 does not get shut down because this would render other NFS services inoperable.
    Should this situation arise, you should move all NFS services off of server 1 and run the service nfs restart. After this you can safely migrate your NFS services back to server 1.
  • If problems arise with your cluster (for example, the cluster becomes inquorate and fencing is not successful), the clustered logical volumes and the GFS/GFS2 file system will be frozen and no access is possible until the cluster is quorate. You should consider this possibility when determining whether a simple
    failover solution such as the one defined in this procedure is the most appropriate for your system.
    看来看去就一个好处，当一个节点挂了另一个节点起来之后不用做FSCK。并且需要人工干预NFS的服务和客户端。


