<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=gbk">
<title>PostgreSQL research</title>
<style type="text/css">
.blogcnt{line-height:160%;font-size:14px;text-align:left;word-wrap:break-word;}
.blogcnt *{line-height:160%;}
.blogcnt p{margin:0 0 10px;}
.blogcnt ul,.nbw-blog ol{margin:5px 0 5px 40px;padding:0}
.blogcnt em{font-style:italic;}
.blogcnt blockquote{font-size:1em;margin:auto 0 auto 35px;}
.blogcnt img{border:0;max-width:100%;}
</style>
</head>
<body style="color:#444444;">
<h1 id="blog-Title"><a href="index.html">PostgreSQL research</a></h1>
<div id="" style="padding:0 20px;">
	<h2 id="">PostgreSQL 9.5 new feature - Provides Infrastructure for launching parallel workers</h2>
	<h5 id="">2015-05-28 10:17:48&nbsp;&nbsp;&nbsp;<a href="http://blog.163.com/digoal@126/blog/static/163877040201542894752785/" target="_blank">查看原文&gt;&gt;</a></h5>
	<div class="" id="" style="padding:0 20px;">
		<div class="blogcnt" style="width:800px;"><div>PostgreSQL 9.5 离并行又更近一步了，此前已经提供了并行必须的动态fork work process, 动态创建共享内存段。</div><div>现在又增加了一些方便用户自定义并行操作的接口函数，PostgreSQL的做法是一个比较开放的做法，提供必要的API，让用户自己玩去，前期可以减轻PG社区开发压力，同时又能够调动社会力量一起来填充PG的并行库，从PG的发展历史来看，很多功能也是社会贡献的，然后合并到社区版本中。</div><div>9.5开放的自定义访问路径，自定义采样方法等动作，也是这样的路数。</div><div><br></div><div>首先创建一个共享内存段，包含内容：用于接收并行工作进程日志的消息队列，同步初始进程状态，其他共享上下文数据。</div><div>一个简单的demo如下，开始并行操作前<span style="line-height: 28px;"   >首先要调用EnterParallelMode()，所有并行操作结束后调用</span>ExitParallelMode()。</div><div><pre class="prettyprint"   ><div style="line-height: 28px;"   ><br></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; EnterParallelMode(); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;/* prohibit unsafe state changes */</font></div><div style="line-height: 28px;"   ><font size="2"   ><br style="line-height: 28px;"   ></font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; pcxt = CreateParallelContext(entrypoint, nworkers);</font></div><div style="line-height: 28px;"   ><font size="2"   ><br style="line-height: 28px;"   ></font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; /* Allow space for application-specific data here. */</font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; shm_toc_estimate_chunk(&amp;pcxt-&gt;estimator, size);</font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; shm_toc_estimate_keys(&amp;pcxt-&gt;estimator, keys);</font></div><div style="line-height: 28px;"   ><font size="2"   ><br style="line-height: 28px;"   ></font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; InitializeParallelDSM(pcxt); &nbsp; &nbsp;/* create DSM and copy state to it */</font></div><div style="line-height: 28px;"   ><font size="2"   ><br style="line-height: 28px;"   ></font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; /* Store the data for which we reserved space. */</font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; space = shm_toc_allocate(pcxt-&gt;toc, size);</font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; shm_toc_insert(pcxt-&gt;toc, key, space);</font></div><div style="line-height: 28px;"   ><font size="2"   ><br style="line-height: 28px;"   ></font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; LaunchParallelWorkers(pcxt);</font></div><div style="line-height: 28px;"   ><font size="2"   ><br style="line-height: 28px;"   ></font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; /* do parallel stuff */</font></div><div style="line-height: 28px;"   ><font size="2"   ><br style="line-height: 28px;"   ></font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; WaitForParallelWorkersToFinish(pcxt);</font></div><div style="line-height: 28px;"   ><font size="2"   ><br style="line-height: 28px;"   ></font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; /* read any final results from dynamic shared memory */</font></div><div style="line-height: 28px;"   ><font size="2"   ><br style="line-height: 28px;"   ></font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; DestroyParallelContext(pcxt);</font></div><div style="line-height: 28px;"   ><font size="2"   ><br style="line-height: 28px;"   ></font></div><div style="line-height: 28px;"   ><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; ExitParallelMode();</font></div><div style="line-height: 28px;"   ><font size="2"   ><br></font></div><div style="line-height: 28px;"   ><font size="2"   >相关数据结构</font></div><div><font size="2"   >typedef struct ParallelWorkerInfo<br>{<br>        BackgroundWorkerHandle *bgwhandle;<br>        shm_mq_handle *error_mqh;<br>        int32           pid;<br>} ParallelWorkerInfo;<br><br>typedef struct ParallelContext<br>{<br>        dlist_node      node;<br>        SubTransactionId subid;<br>        int                     nworkers;<br>        parallel_worker_main_type entrypoint;<br>        char       *library_name;<br>        char       *function_name;<br>        ErrorContextCallback *error_context_stack;<br>        shm_toc_estimator estimator;<br>        dsm_segment *seg;<br>        void       *private_memory;<br>        shm_toc    *toc;<br>        ParallelWorkerInfo *worker;<br>} ParallelContext;</font></div><p style="line-height: 28px;"   ></p></pre></div><div style="line-height: 28px;"   >下面引用README.parallel的内容：</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >Overview</font></div><div><font size="2"   >========</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >PostgreSQL provides some simple facilities to make writing parallel algorithms</font></div><div><font size="2"   >easier. &nbsp;Using a data structure called a ParallelContext, you can arrange to</font></div><div><font size="2"   >launch background worker processes, initialize their state to match that of</font></div><div><font size="2"   >the backend which initiated parallelism, communicate with them via dynamic</font></div><div><font size="2"   >shared memory, and write reasonably complex code that can run either in the</font></div><div><font size="2"   >user backend or in one of the parallel workers without needing to be aware of</font></div><div><font size="2"   >where it's running.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >The backend which starts a parallel operation (hereafter, the initiating</font></div><div><font size="2"   >backend) starts by creating a dynamic shared memory segment which will last</font></div><div><font size="2"   >for the lifetime of the parallel operation. &nbsp;This dynamic shared memory segment</font></div><div><font size="2"   >will contain (1) a shm_mq that can be used to transport errors (and other</font></div><div><font size="2"   >messages reported via elog/ereport) from the worker back to the initiating</font></div><div><font size="2"   >backend; (2) serialized representations of the initiating backend's private</font></div><div><font size="2"   >state, so that the worker can synchronize its state with of the initiating</font></div><div><font size="2"   >backend; and (3) any other data structures which a particular user of the</font></div><div><font size="2"   >ParallelContext data structure may wish to add for its own purposes. &nbsp;Once</font></div><div><font size="2"   >the initiating backend has initialized the dynamic shared memory segment, it</font></div><div><font size="2"   >asks the postmaster to launch the appropriate number of parallel workers.</font></div><div><font size="2"   >These workers then connect to the dynamic shared memory segment, initiate</font></div><div><font size="2"   >their state, and then invoke the appropriate entrypoint, as further detailed</font></div><div><font size="2"   >below.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Error Reporting</font></div><div><font size="2"   >===============</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >When started, each parallel worker begins by attaching the dynamic shared</font></div><div><font size="2"   >memory segment and locating the shm_mq to be used for error reporting; it</font></div><div><font size="2"   >redirects all of its protocol messages to this shm_mq. &nbsp;Prior to this point,</font></div><div><font size="2"   >any failure of the background worker will not be reported to the initiating</font></div><div><font size="2"   >backend; from the point of view of the initiating backend, the worker simply</font></div><div><font size="2"   >failed to start. &nbsp;The initiating backend must anyway be prepared to cope</font></div><div><font size="2"   >with fewer parallel workers than it originally requested, so catering to</font></div><div><font size="2"   >this case imposes no additional burden.</font></div></div><div><div><font size="2"   >Whenever a new message (or partial message; very large messages may wrap) is</font></div><div><font size="2"   >sent to the error-reporting queue, PROCSIG_PARALLEL_MESSAGE is sent to the</font></div><div><font size="2"   >initiating backend. &nbsp;This causes the next CHECK_FOR_INTERRUPTS() in the</font></div><div><font size="2"   >initiating backend to read and rethrow the message. &nbsp;For the most part, this</font></div><div><font size="2"   >makes error reporting in parallel mode "just work". &nbsp;Of course, to work</font></div><div><font size="2"   >properly, it is important that the code the initiating backend is executing</font></div><div><font size="2"   >CHECK_FOR_INTERRUPTS() regularly and avoid blocking interrupt processing for</font></div><div><font size="2"   >long periods of time, but those are good things to do anyway.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >(A currently-unsolved problem is that some messages may get written to the</font></div><div><font size="2"   >system log twice, once in the backend where the report was originally</font></div><div><font size="2"   >generated, and again when the initiating backend rethrows the message. &nbsp;If</font></div><div><font size="2"   >we decide to suppress one of these reports, it should probably be second one;</font></div><div><font size="2"   >otherwise, if the worker is for some reason unable to propagate the message</font></div><div><font size="2"   >back to the initiating backend, the message will be lost altogether.)</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >State Sharing</font></div><div><font size="2"   >=============</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >It's possible to write C code which works correctly without parallelism, but</font></div><div><font size="2"   >which fails when parallelism is used. &nbsp;No parallel infrastructure can</font></div><div><font size="2"   >completely eliminate this problem, because any global variable is a risk.</font></div><div><font size="2"   >There's no general mechanism for ensuring that every global variable in the</font></div><div><font size="2"   >worker will have the same value that it does in the initiating backend; even</font></div><div><font size="2"   >if we could ensure that, some function we're calling could update the variable</font></div><div><font size="2"   >after each call, and only the backend where that update is performed will see</font></div><div><font size="2"   >the new value. &nbsp;Similar problems can arise with any more-complex data</font></div><div><font size="2"   >structure we might choose to use. &nbsp;For example, a pseudo-random number</font></div><div><font size="2"   >generator should, given a particular seed value, produce the same predictable</font></div><div><font size="2"   >series of values every time. &nbsp;But it does this by relying on some private</font></div><div><font size="2"   >state which won't automatically be shared between cooperating backends. &nbsp;A</font></div><div><font size="2"   >parallel-safe PRNG would need to store its state in dynamic shared memory, and</font></div><div><font size="2"   >would require locking. &nbsp;The parallelism infrastructure has no way of knowing</font></div><div><font size="2"   >whether the user intends to call code that has this sort of problem, and can't</font></div><div><font size="2"   >do anything about it anyway.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Instead, we take a more pragmatic approach. First, we try to make as many of</font></div><div><font size="2"   >the operations that are safe outside of parallel mode work correctly in</font></div><div><div><font size="2"   >parallel mode as well. &nbsp;Second, we try to prohibit common unsafe operations</font></div><div><font size="2"   >via suitable error checks. &nbsp;These checks are intended to catch 100% of</font></div><div><font size="2"   >unsafe things that a user might do from the SQL interface, but code written</font></div><div><font size="2"   >in C can do unsafe things that won't trigger these checks. &nbsp;The error checks</font></div><div><font size="2"   >are engaged via EnterParallelMode(), which should be called before creating</font></div><div><font size="2"   >a parallel context, and disarmed via ExitParallelMode(), which should be</font></div><div><font size="2"   >called after all parallel contexts have been destroyed. &nbsp;The most</font></div><div><font size="2"   >significant restriction imposed by parallel mode is that all operations must</font></div><div><font size="2"   >be strictly read-only; we allow no writes to the database and no DDL. &nbsp;We</font></div><div><font size="2"   >might try to relax these restrictions in the future.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >To make as many operations as possible safe in parallel mode, we try to copy</font></div><div><font size="2"   >the most important pieces of state from the initiating backend to each parallel</font></div><div><font size="2"   >worker. &nbsp;This includes:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; - The set of libraries dynamically loaded by dfmgr.c.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; - The authenticated user ID and current database. &nbsp;Each parallel worker</font></div><div><font size="2"   >&nbsp; &nbsp; will connect to the same database as the initiating backend, using the</font></div><div><font size="2"   >&nbsp; &nbsp; same user ID.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; - The values of all GUCs. &nbsp;Accordingly, permanent changes to the value of</font></div><div><font size="2"   >&nbsp; &nbsp; any GUC are forbidden while in parallel mode; but temporary changes,</font></div><div><font size="2"   >&nbsp; &nbsp; such as entering a function with non-NULL proconfig, are OK.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; - The current subtransaction's XID, the top-level transaction's XID, and</font></div><div><font size="2"   >&nbsp; &nbsp; the list of XIDs considered current (that is, they are in-progress or</font></div><div><font size="2"   >&nbsp; &nbsp; subcommitted). &nbsp;This information is needed to ensure that tuple visibility</font></div><div><font size="2"   >&nbsp; &nbsp; checks return the same results in the worker as they do in the</font></div><div><font size="2"   >&nbsp; &nbsp; initiating backend. &nbsp;See also the section Transaction Integration, below.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; - The combo CID mappings. &nbsp;This is needed to ensure consistent answers to</font></div><div><font size="2"   >&nbsp; &nbsp; tuple visibility checks. &nbsp;The need to synchronize this data structure is</font></div><div><font size="2"   >&nbsp; &nbsp; a major reason why we can't support writes in parallel mode: such writes</font></div><div><font size="2"   >&nbsp; &nbsp; might create new combo CIDs, and we have no way to let other workers</font></div><div><font size="2"   >&nbsp; &nbsp; (or the initiating backend) know about them.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; - The transaction snapshot.</font></div></div></div><div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; - The active snapshot, which might be different from the transaction</font></div><div><font size="2"   >&nbsp; &nbsp; snapshot.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; - The currently active user ID and security context. &nbsp;Note that this is</font></div><div><font size="2"   >&nbsp; &nbsp; the fourth user ID we restore: the initial step of binding to the correct</font></div><div><font size="2"   >&nbsp; &nbsp; database also involves restoring the authenticated user ID. &nbsp;When GUC</font></div><div><font size="2"   >&nbsp; &nbsp; values are restored, this incidentally sets SessionUserId and OuterUserId</font></div><div><font size="2"   >&nbsp; &nbsp; to the correct values. &nbsp;This final step restores CurrentUserId.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >To prevent undetected or unprincipled deadlocks when running in parallel mode,</font></div><div><font size="2"   >this could should eventually handle heavyweight locks in some way. &nbsp;This is</font></div><div><font size="2"   >not implemented yet.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Transaction Integration</font></div><div><font size="2"   >=======================</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Regardless of what the TransactionState stack looks like in the parallel</font></div><div><font size="2"   >leader, each parallel worker ends up with a stack of depth 1. &nbsp;This stack</font></div><div><font size="2"   >entry is marked with the special transaction block state</font></div><div><font size="2"   >TBLOCK_PARALLEL_INPROGRESS so that it's not confused with an ordinary</font></div><div><font size="2"   >toplevel transaction. &nbsp;The XID of this TransactionState is set to the XID of</font></div><div><font size="2"   >the innermost currently-active subtransaction in the initiating backend. &nbsp;The</font></div><div><font size="2"   >initiating backend's toplevel XID, and the XIDs of all current (in-progress</font></div><div><font size="2"   >or subcommitted) XIDs are stored separately from the TransactionState stack,</font></div><div><font size="2"   >but in such a way that GetTopTransactionId(), GetTopTransactionIdIfAny(), and</font></div><div><font size="2"   >TransactionIdIsCurrentTransactionId() return the same values that they would</font></div><div><font size="2"   >in the initiating backend. &nbsp;We could copy the entire transaction state stack,</font></div><div><font size="2"   >but most of it would be useless: for example, you can't roll back to a</font></div><div><font size="2"   >savepoint from within a parallel worker, and there are no resources to</font></div><div><font size="2"   >associated with the memory contexts or resource owners of intermediate</font></div><div><font size="2"   >subtransactions.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >No meaningful change to the transaction state can be made while in parallel</font></div><div><font size="2"   >mode. &nbsp;No XIDs can be assigned, and no subtransactions can start or end,</font></div><div><font size="2"   >because we have no way of communicating these state changes to cooperating</font></div><div><font size="2"   >backends, or of synchronizing them. &nbsp;It's clearly unworkable for the initiating</font></div><div><font size="2"   >backend to exit any transaction or subtransaction that was in progress when</font></div></div><div><div><font size="2"   >parallelism was started before all parallel workers have exited; and it's even</font></div><div><font size="2"   >more clearly crazy for a parallel worker to try to subcommit or subabort the</font></div><div><font size="2"   >current subtransaction and execute in some other transaction context than was</font></div><div><font size="2"   >present in the initiating backend. &nbsp;It might be practical to allow internal</font></div><div><font size="2"   >sub-transactions (e.g. to implement a PL/pgsql EXCEPTION block) to be used in</font></div><div><font size="2"   >parallel mode, provided that they are XID-less, because other backends</font></div><div><font size="2"   >wouldn't really need to know about those transactions or do anything</font></div><div><font size="2"   >differently because of them. &nbsp;Right now, we don't even allow that.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >At the end of a parallel operation, which can happen either because it</font></div><div><font size="2"   >completed successfully or because it was interrupted by an error, parallel</font></div><div><font size="2"   >workers associated with that operation exit. &nbsp;In the error case, transaction</font></div><div><font size="2"   >abort processing in the parallel leader kills of any remaining workers, and</font></div><div><font size="2"   >the parallel leader then waits for them to die. &nbsp;In the case of a successful</font></div><div><font size="2"   >parallel operation, the parallel leader does not send any signals, but must</font></div><div><font size="2"   >wait for workers to complete and exit of their own volition. &nbsp;In either</font></div><div><font size="2"   >case, it is very important that all workers actually exit before the</font></div><div><font size="2"   >parallel leader cleans up the (sub)transaction in which they were created;</font></div><div><font size="2"   >otherwise, chaos can ensue. &nbsp;For example, if the leader is rolling back the</font></div><div><font size="2"   >transaction that created the relation being scanned by a worker, the</font></div><div><font size="2"   >relation could disappear while the worker is still busy scanning it. &nbsp;That's</font></div><div><font size="2"   >not safe.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Generally, the cleanup performed by each worker at this point is similar to</font></div><div><font size="2"   >top-level commit or abort. &nbsp;Each backend has its own resource owners: buffer</font></div><div><font size="2"   >pins, catcache or relcache reference counts, tuple descriptors, and so on</font></div><div><font size="2"   >are managed separately by each backend, and must free them before exiting.</font></div><div><font size="2"   >There are, however, some important differences between parallel worker</font></div><div><font size="2"   >commit or abort and a real top-level transaction commit or abort. &nbsp;Most</font></div><div><font size="2"   >importantly:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; - No commit or abort record is written; the initiating backend is</font></div><div><font size="2"   >&nbsp; &nbsp; responsible for this.</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; - Cleanup of pg_temp namespaces is not done. &nbsp;Parallel workers cannot</font></div><div><font size="2"   >&nbsp; &nbsp; safely access the initiating backend's pg_temp namespace, and should</font></div><div><font size="2"   >&nbsp; &nbsp; not create one of their own.</font></div></div><div><div><font size="2"   ><br></font></div><div><font size="2"   >Coding Conventions</font></div><div><font size="2"   >===================</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >Before beginning any parallel operation, call EnterParallelMode(); after all</font></div><div><font size="2"   >parallel operations are completed, call ExitParallelMode(). &nbsp;To actually</font></div><div><font size="2"   >parallelize a particular operation, use a ParallelContext. &nbsp;The basic coding</font></div><div><font size="2"   >pattern looks like this:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; EnterParallelMode(); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;/* prohibit unsafe state changes */</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; pcxt = CreateParallelContext(entrypoint, nworkers);</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; /* Allow space for application-specific data here. */</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; shm_toc_estimate_chunk(&amp;pcxt-&gt;estimator, size);</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; shm_toc_estimate_keys(&amp;pcxt-&gt;estimator, keys);</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; InitializeParallelDSM(pcxt); &nbsp; &nbsp;/* create DSM and copy state to it */</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; /* Store the data for which we reserved space. */</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; space = shm_toc_allocate(pcxt-&gt;toc, size);</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; shm_toc_insert(pcxt-&gt;toc, key, space);</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; LaunchParallelWorkers(pcxt);</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; /* do parallel stuff */</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; WaitForParallelWorkersToFinish(pcxt);</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; /* read any final results from dynamic shared memory */</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; DestroyParallelContext(pcxt);</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; ExitParallelMode();</font></div></div><p></p></pre></div><div><br></div><div><br></div>[参考]<wbr><div>1.&nbsp;src/backend/access/transam/parallel.c</div><div>2.&nbsp;src/backend/access/transam/README.parallel</div><div>3.&nbsp;src/include/access/parallel.h</div>
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"   ><img title="PostgreSQL 9.5 new feature - Provides Infrastructure for launching parallel workers - 德哥@Digoal - PostgreSQL research"   src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"   alt="Flag Counter"   border="0"   ></a></div>
	</div>
</div>
</body>
</html>