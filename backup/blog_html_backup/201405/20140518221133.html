<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=gbk">
<title>PostgreSQL research</title>
<style type="text/css">
.blogcnt{line-height:160%;font-size:14px;text-align:left;word-wrap:break-word;}
.blogcnt *{line-height:160%;}
.blogcnt p{margin:0 0 10px;}
.blogcnt ul,.nbw-blog ol{margin:5px 0 5px 40px;padding:0}
.blogcnt em{font-style:italic;}
.blogcnt blockquote{font-size:1em;margin:auto 0 auto 35px;}
.blogcnt img{border:0;max-width:100%;}
</style>
</head>
<body style="color:#444444;">
<h1 id="blog-Title"><a href="index.html">PostgreSQL research</a></h1>
<div id="" style="padding:0 20px;">
	<h2 id="">zfs pool self healing and scrub and pre-replace "bad"-disks</h2>
	<h5 id="">2014-05-18 22:11:33&nbsp;&nbsp;&nbsp;<a href="http://blog.163.com/digoal@126/blog/static/163877040201441893635546/" target="_blank">查看原文&gt;&gt;</a></h5>
	<div class="" id="" style="padding:0 20px;">
		<div class="blogcnt" style="width:800px;"><div>ZFS的又一个强大之处, 支持坏块的自愈 (如果使用了冗余的话,如raidz1, raidz2, raidze, ... 并且正确的块可通过ECC重新计算出的话.).&nbsp;</div><div>同时ZFS具备类似ECC DIMM的校验功能, 默认使用SHA-256 checksum.</div><div>使用scrub来检测ZPOOL底层的块设备是否健康, 对于SAS或FC硬盘, 可以一个月检测一次, 而对于低端的SATA, SCSI设备则最好1周检测一次.</div><div>这些可以放在定时任务中执行, 例如每天0点1分开始执行一次scrub.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >crontab -e</font></div><div><font size="2"   >1 0 * * * /opt/zfs0.6.2/sbin/zpool scrub zptest</font></div><p></p></pre></div><div>对于检测到的指标不好的盘, 可以提前更换(使用zpool replace).</div><div>指标 :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >The rows in the "zpool status" command give you vital information about the pool, most of which are self-explanatory. They are defined as follows:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >pool- The name of the pool.</font></div><div><font size="2"   >state- The current health of the pool. This information refers only to the ability of the pool to provide the necessary replication level.</font></div><div><font size="2"   >status- A description of what is wrong with the pool. This field is omitted if no problems are found.</font></div><div><font size="2"   >action- A recommended action for repairing the errors. This field is an abbreviated form directing the user to one of the following sections. This field is omitted if no problems are found.</font></div><div><font size="2"   >see- A reference to a knowledge article containing detailed repair information. Online articles are updated more often than this guide can be updated, and should always be referenced for the most up-to-date repair procedures. This field is omitted if no problems are found.</font></div><div><font size="2"   >scrub- Identifies the current status of a scrub operation, which might include the date and time that the last scrub was completed, a scrub in progress, or if no scrubbing was requested.</font></div><div><font size="2"   >errors- Identifies known data errors or the absence of known data errors.</font></div><div><font size="2"   >config- Describes the configuration layout of the devices comprising the pool, as well as their state and any errors generated from the devices. The state can be one of the following: ONLINE, FAULTED, DEGRADED, UNAVAILABLE, or OFFLINE. If the state is anything but ONLINE, the fault tolerance of the pool has been compromised.</font></div><div><font size="2"   >The columns in the status output, "READ", "WRITE" and "CHKSUM" are defined as follows:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >NAME- The name of each VDEV in the pool, presented in a nested order.</font></div><div><font size="2"   >STATE- The state of each VDEV in the pool. The state can be any of the states found in "config" above.</font></div><div><font size="2"   >READ- I/O errors occurred while issuing a read request.</font></div><div><font size="2"   >WRITE- I/O errors occurred while issuing a write request.</font></div><div><font size="2"   >CHKSUM- Checksum errors. The device returned corrupted data as the result of a read request.</font></div><p></p></pre></div><div><br></div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >Scrubbing ZFS storage pools is not something that happens automatically. You need to do it manually, and it's highly recommended that you do it on a regularly scheduled interval. The recommended frequency at which you should scrub the data depends on the quality of the underlying disks. If you have SAS or FC disks, then once per month should be sufficient. If you have consumer grade SATA or SCSI, you should do once per week. You can schedule a scrub easily with the following command:</font></div><div><font size="2"   ># zpool scrub tank</font></div><div><font size="2"   ># zpool status tank</font></div><div><font size="2"   >&nbsp; pool: tank</font></div><div><font size="2"   >&nbsp;state: ONLINE</font></div><div><font size="2"   >&nbsp;scan: scrub in progress since Sat Dec &nbsp;8 08:06:36 2012</font></div><div><font size="2"   >&nbsp; &nbsp; 32.0M scanned out of 48.5M at 16.0M/s, 0h0m to go</font></div><div><font size="2"   >&nbsp; &nbsp; 0 repaired, 65.99% done</font></div><div><font size="2"   >config:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; NAME &nbsp; &nbsp; &nbsp; &nbsp;STATE &nbsp; &nbsp; READ WRITE CKSUM</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; tank &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mirror-0 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sde &nbsp; &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sdf &nbsp; &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mirror-1 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sdg &nbsp; &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sdh &nbsp; &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mirror-2 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sdi &nbsp; &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sdj &nbsp; &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >errors: No known data errors</font></div><p></p></pre></div><div><br></div><div>例如, 使用raidz1冗余, 创建一个zp pool.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@spark01 ~]# zpool create zp raidz1 /home/digoal/zfs.disk1 /home/digoal/zfs.disk2 /home/digoal/zfs.disk3 /home/digoal/zfs.disk4 log mirror /home/digoal/zfs.log1 /home/digoal/zfs.log2</font></div><div><font size="2"   ><br></font></div><div><div><font size="2"   >[root@spark01 ~]# zpool status</font></div><div><font size="2"   >&nbsp; pool: zp</font></div><div><font size="2"   >&nbsp;state: ONLINE</font></div><div><font size="2"   >&nbsp; scan: none requested</font></div><div><font size="2"   >config:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;STATE &nbsp; &nbsp; READ WRITE CKSUM</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; zp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; raidz1-0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk1 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk2 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk3 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk4 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; logs</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mirror-1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.log1 &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.log2 &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >errors: No known data errors</font></div></div><p></p></pre></div><div>拷贝一些文件到dataset.</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >[root@spark01 ~]# cd /home/digoal</font></div><div><font size="2"   >[root@spark01 digoal]# ll</font></div><div><font size="2"   >total 10575000</font></div><div><font size="2"   >drwxr-xr-x. &nbsp;9 digoal digoal &nbsp; &nbsp; &nbsp; 4096 Mar 31 17:15 hadoop-2.4.0</font></div><div><font size="2"   >-rw-rw-r--. &nbsp;1 digoal digoal &nbsp;138943699 Mar 31 17:16 hadoop-2.4.0.tar.gz</font></div><div><font size="2"   >drwxr-xr-x. 10 &nbsp; 7900 &nbsp; 7900 &nbsp; &nbsp; &nbsp; 4096 May 19 01:24 spl-0.6.2</font></div><div><font size="2"   >-rw-r--r--. &nbsp;1 root &nbsp; root &nbsp; &nbsp; &nbsp; 565277 Aug 24 &nbsp;2013 spl-0.6.2.tar.gz</font></div><div><font size="2"   >drwxr-xr-x. 13 &nbsp; 7900 &nbsp; 7900 &nbsp; &nbsp; &nbsp; 4096 May 19 01:28 zfs-0.6.2</font></div><div><font size="2"   >-rw-r--r--. &nbsp;1 root &nbsp; root &nbsp; &nbsp; &nbsp;2158948 Aug 24 &nbsp;2013 zfs-0.6.2.tar.gz</font></div><div><font size="2"   >-rw-r--r--. &nbsp;1 root &nbsp; root &nbsp; 2147483648 May 19 05:54 zfs.disk1</font></div><div><font size="2"   >-rw-r--r--. &nbsp;1 root &nbsp; root &nbsp; 2147483648 May 19 05:54 zfs.disk2</font></div><div><font size="2"   >-rw-r--r--. &nbsp;1 root &nbsp; root &nbsp; 2147483648 May 19 05:54 zfs.disk3</font></div><div><font size="2"   >-rw-r--r--. &nbsp;1 root &nbsp; root &nbsp; 2147483648 May 19 05:54 zfs.disk4</font></div><div><font size="2"   >-rw-r--r--. &nbsp;1 root &nbsp; root &nbsp; 1048576000 May 19 05:54 zfs.log1</font></div><div><font size="2"   >-rw-r--r--. &nbsp;1 root &nbsp; root &nbsp; 1048576000 May 19 05:54 zfs.log2</font></div></div><div><div><font size="2"   >[root@spark01 digoal]# zfs create zp/test</font></div><div><font size="2"   >[root@spark01 digoal]# cp -r spl-0.6.2* zfs-0.6.2* hadoop-2.4.0* /zp/test/</font></div></div><div><font size="2"   ><br></font></div><div><div><font size="2"   >[root@spark01 digoal]# df -h</font></div><div><font size="2"   >Filesystem &nbsp; &nbsp; &nbsp;Size &nbsp;Used Avail Use% Mounted on</font></div><div><font size="2"   >/dev/sda1 &nbsp; &nbsp; &nbsp; &nbsp;31G &nbsp;1.2G &nbsp; 29G &nbsp; 5% /</font></div><div><font size="2"   >tmpfs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;12G &nbsp; &nbsp; 0 &nbsp; 12G &nbsp; 0% /dev/shm</font></div><div><font size="2"   >/dev/sda3 &nbsp; &nbsp; &nbsp; &nbsp;89G &nbsp; 11G &nbsp; 74G &nbsp;13% /home</font></div><div><font size="2"   >zp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5.4G &nbsp; &nbsp; 0 &nbsp;5.4G &nbsp; 0% /zp</font></div><div><font size="2"   >zp/test &nbsp; &nbsp; &nbsp; &nbsp; 5.9G &nbsp;535M &nbsp;5.4G &nbsp; 9% /zp/test</font></div></div><p></p></pre></div><div><br></div><div>使用zpool scrub检查这个pool.</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@spark01 digoal]# zpool scrub zp</font></div><div><font size="2"   >[root@spark01 digoal]# zpool status</font></div><div><font size="2"   >&nbsp; pool: zp</font></div><div><font size="2"   >&nbsp;state: ONLINE</font></div><div><font size="2"   >&nbsp; scan: scrub repaired 0 in 0h0m with 0 errors on Mon May 19 05:56:17 2014</font></div><div><font size="2"   >config:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;STATE &nbsp; &nbsp; READ WRITE CKSUM</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; zp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; raidz1-0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk1 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk2 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk3 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk4 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; logs</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mirror-1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.log1 &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.log2 &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >errors: No known data errors</font></div><p></p></pre></div><div><br></div><div><div>关闭一个正在执行的scrub任务 :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@spark01 test]# zpool scrub -s zp</font></div><div><font size="2"   >cannot cancel scrubbing zp: there is no active scrub</font></div><p></p></pre></div></div><div>接下来要测试一下在线替换scrub检查到问题的块设备, 我这里使用删除一个zfs.disk来模拟坏盘.</div><div><pre class="prettyprint"   ><p></p><div><span style="line-height: 28px;"   ><font size="2"   >[root@spark01 digoal]# rm -f zfs.disk1</font></span></div><div><div><font size="2"   >[root@spark01 digoal]# zpool scrub zp &nbsp; &nbsp;#使用scrub没有检测到删除的盘.</font></div><div><font size="2"   >[root@spark01 digoal]# zpool status</font></div><div><font size="2"   >&nbsp; pool: zp</font></div><div><font size="2"   >&nbsp;state: ONLINE</font></div><div><font size="2"   >&nbsp; scan: scrub repaired 0 in 0h0m with 0 errors on Mon May 19 05:56:44 2014</font></div><div><font size="2"   >config:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;STATE &nbsp; &nbsp; READ WRITE CKSUM</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; zp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; raidz1-0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk1 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk2 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk3 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk4 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; logs</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mirror-1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.log1 &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.log2 &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >errors: No known data errors</font></div></div><p></p></pre></div><div>但是因为使用了raidz1, 所以删除disk1后还能查询. (从校验数据中计算出原始数据. raidz1允许坏1块盘)</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@spark01 digoal]# cd /zp/test</font></div><div><font size="2"   >[root@spark01 test]# ll</font></div><div><font size="2"   >total 138651</font></div><div><font size="2"   >drwxr-xr-x. &nbsp;9 root root &nbsp; &nbsp; &nbsp; &nbsp;12 May 19 05:55 hadoop-2.4.0</font></div><div><font size="2"   >-rw-r--r--. &nbsp;1 root root 138943699 May 19 05:56 hadoop-2.4.0.tar.gz</font></div><div><font size="2"   >drwxr-xr-x. 10 root root &nbsp; &nbsp; &nbsp; &nbsp;30 May 19 05:55 spl-0.6.2</font></div><div><font size="2"   >-rw-r--r--. &nbsp;1 root root &nbsp; &nbsp;565277 May 19 05:55 spl-0.6.2.tar.gz</font></div><div><font size="2"   >drwxr-xr-x. 13 root root &nbsp; &nbsp; &nbsp; &nbsp;37 May 19 05:55 zfs-0.6.2</font></div><div><font size="2"   >-rw-r--r--. &nbsp;1 root root &nbsp; 2158948 May 19 05:55 zfs-0.6.2.tar.gz</font></div><div><font size="2"   >[root@spark01 test]# du -sh *</font></div><div><font size="2"   >250M &nbsp; &nbsp;hadoop-2.4.0</font></div><div><font size="2"   >133M &nbsp; &nbsp;hadoop-2.4.0.tar.gz</font></div><div><font size="2"   >39M &nbsp; &nbsp; spl-0.6.2</font></div><div><font size="2"   >643K &nbsp; &nbsp;spl-0.6.2.tar.gz</font></div><div><font size="2"   >193M &nbsp; &nbsp;zfs-0.6.2</font></div><div><font size="2"   >2.2M &nbsp; &nbsp;zfs-0.6.2.tar.gz</font></div><p></p></pre></div><div><br></div><div>新建一个文件, 用来替换被我删掉的zfs.disk1文件, 新增的这个文件可以与zfs.disk1同名, 也可以不同名.&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@spark01 test]# cd /home/digoal/</font></div><div><div><font size="2"   >[root@spark01 digoal]# dd if=/dev/zero of=./zfs.disk1 bs=1024k count=2048</font></div><div><font size="2"   >2048+0 records in</font></div><div><font size="2"   >2048+0 records out</font></div><div><font size="2"   >2147483648 bytes (2.1 GB) copied, 1.29587 s, 1.7 GB/s</font></div></div><p></p></pre></div><div>使用zpool replace替换坏盘 :&nbsp;</div><div><pre class="prettyprint"   ><p></p><div><div><font size="2"   >[root@spark01 digoal]# zpool replace -h</font></div><div><font size="2"   >usage:</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; replace [-f] &lt;pool&gt; &lt;device&gt; [new-device]</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >[root@spark01 digoal]# zpool replace zp /home/digoal/zfs.disk1 /home/digoal/zfs.disk1</font></div></div><div><div><font size="2"   >[root@spark01 digoal]# zpool scrub zp</font></div><div><font size="2"   >[root@spark01 digoal]# zpool status zp</font></div><div><font size="2"   >&nbsp; pool: zp</font></div><div><font size="2"   >&nbsp;state: ONLINE</font></div><div><font size="2"   >&nbsp; scan: scrub repaired 0 in 0h0m with 0 errors on Mon May 19 06:01:19 2014</font></div><div><font size="2"   >config:</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;STATE &nbsp; &nbsp; READ WRITE CKSUM</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; zp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; raidz1-0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk1 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk2 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk3 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.disk4 &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; logs</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mirror-1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.log1 &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /home/digoal/zfs.log2 &nbsp; ONLINE &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0</font></div><div><font size="2"   ><br></font></div><div><font size="2"   >errors: No known data errors</font></div></div><p></p></pre></div><div><div>使用status -x参数查看pool的健康状态</div><div><pre class="prettyprint"   ><p></p><div><font size="2"   >[root@spark01 digoal]# zpool status zp -x</font></div><div><font size="2"   >pool 'zp' is healthy</font></div><p></p></pre></div></div><div><br></div><div>注意如果是真实环境中的硬盘替换的话, 支持热插拔的硬盘可以直接替换硬盘, 然后使用zpool replace替换.</div><div>对于不能热插拔的硬盘, 需要关机替换硬盘, 再使用zpool replace替换掉坏盘.</div><div>查看坏盘对应的设备号(或序列号, 因为更换硬盘时需要拔下硬盘后现场对比一下序列号, 以免弄错).</div><div>hdparm -I, 对比zpool status中的设备名.</div><div><br></div><div>[参考]</div>1.&nbsp;<wbr><a target="_blank" rel="nofollow" href="http://docs.oracle.com/cd/E26502_01/pdf/E29007.pdf"   >http://docs.oracle.com/cd/E26502_01/pdf/E29007.pdf</a><div>2.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="http://www.root.cz/clanky/suborovy-system-zfs-konzistentnost-dat/"   >http://www.root.cz/clanky/suborovy-system-zfs-konzistentnost-dat/</a></div><div>3.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="https://pthree.org/2012/12/11/zfs-administration-part-vi-scrub-and-resilver/"   >https://pthree.org/2012/12/11/zfs-administration-part-vi-scrub-and-resilver/</a></div><div>4.&nbsp;<a style="line-height: 28px;" target="_blank" rel="nofollow" href="https://pthree.org/2012/12/05/zfs-administration-part-ii-raidz/"   >https://pthree.org/2012/12/05/zfs-administration-part-ii-raidz/</a></div>
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"   ><img title="zfs pool self healing and scrub - 德哥@Digoal - PostgreSQL"   src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"   alt="Flag Counter"   border="0"   ></a></div>
	</div>
	<h3>评论</h3>
	<div class="" id="" style="padding:0 20px;">
			<div id="">
				<h5 id="">xmarker - 2014-05-29 16:55:44</h5>
				<div>德哥，我创建好zfs文件系统后，为啥一重启操作系统，zpool就不见了呢，我只能用import来导入，有什么办法能解决这个问题么<br><br></div>
			</div>
			<div style="padding-left:40px;">
				<h5 id="">德哥@Digoal 回复 xmarker - 2014-05-29 16:55:44</h5>
				<div style="width:600px;">配置文件设置一下</div>
			</div>
	</div>
</div>
</body>
</html>