<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=gbk">
<title>PostgreSQL research</title>
<style type="text/css">
.blogcnt{line-height:160%;font-size:14px;text-align:left;word-wrap:break-word;}
.blogcnt *{line-height:160%;}
.blogcnt p{margin:0 0 10px;}
.blogcnt ul,.nbw-blog ol{margin:5px 0 5px 40px;padding:0}
.blogcnt em{font-style:italic;}
.blogcnt blockquote{font-size:1em;margin:auto 0 auto 35px;}
.blogcnt img{border:0;max-width:100%;}
</style>
</head>
<body style="color:#444444;">
<h1 id="blog-Title"><a href="index.html">PostgreSQL research</a></h1>
<div id="" style="padding:0 20px;">
	<h2 id="">RHCS abnormal case: network problem between cluster nodes</h2>
	<h5 id="">2012-04-09 23:52:27&nbsp;&nbsp;&nbsp;<a href="http://blog.163.com/digoal@126/blog/static/163877040201239112223275/" target="_blank">查看原文&gt;&gt;</a></h5>
	<div class="" id="" style="padding:0 20px;">
		<div class="blogcnt" style="width:800px;">  <p>一个RHCS的异常案例，节点是redhat 5.5 64位的系统。集群由两个节点组成。</p>  <p>两个节点之间的通讯网络异常，导致双方都检测不到对方心跳，发出了fence对方的指令，而fence网络也出了问题，因此双方都在不停的fence对方。</p>  <p>就在FENCE对方的过程当中，还发现了一个问题。</p>  <p>因为我们在这个集群中只用到了GFS文件系统，在FENCE的过程中，我们发现两个节点的GFS文件系统都不可访问。</p>  <p>为什么呢?</p>  <p>host_2的日志是这样的 : </p>  <p>Apr&nbsp; 9 22:18:18 host_2 kernel: NETDEV WATCHDOG: eth0: transmit timed out<br>Apr&nbsp; 9 22:18:19 host_2 kernel: bnx2: eth0 NIC Copper Link is Down<br>Apr&nbsp; 9 22:18:28 host_2 openais[9376]: [TOTEM] The token was lost in the OPERATIONAL state. <br>Apr&nbsp; 9 22:18:28 host_2 openais[9376]: [TOTEM] Receive multicast socket recv buffer size (320000 bytes). <br>Apr&nbsp; 9 22:18:28 host_2 openais[9376]: [TOTEM] Transmit multicast socket send buffer size (320000 bytes). <br>Apr&nbsp; 9 22:18:28 host_2 openais[9376]: [TOTEM] entering GATHER state from 2. <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [TOTEM] entering GATHER state from 0. <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [TOTEM] Creating commit token because I am the rep. <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [TOTEM] Saving state aru c349 high seq received c349 <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [TOTEM] Storing new sequence id for ring 884 <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [TOTEM] entering COMMIT state. <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [TOTEM] entering RECOVERY state. <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [TOTEM] position [0] member host_2: <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [TOTEM] previous ring seq 2176 rep host_1 <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [TOTEM] aru c349 high delivered c349 received flag 1 <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [TOTEM] Did not need to originate any messages in recovery. <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [TOTEM] Sending initial ORF token <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [CLM&nbsp; ] CLM CONFIGURATION CHANGE <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [CLM&nbsp; ] New Configuration: <br>Apr&nbsp; 9 22:18:48 host_2 kernel: dlm: closing connection to node 1<br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [CLM&nbsp; ]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r(0) ip(host_2)&nbsp; <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [CLM&nbsp; ] Members Left: <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [CLM&nbsp; ]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r(0) ip(host_1)&nbsp; <br>Apr&nbsp; 9 22:18:48 host_2 openais[9376]: [CLM&nbsp; ] Members Joined: <br>Apr&nbsp; 9 22:18:49 host_2 openais[9376]: [CLM&nbsp; ] CLM CONFIGURATION CHANGE <br>Apr&nbsp; 9 22:18:49 host_2 openais[9376]: [CLM&nbsp; ] New Configuration: <br>Apr&nbsp; 9 22:18:49 host_2 openais[9376]: [CLM&nbsp; ]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r(0) ip(host_2)&nbsp; <br>Apr&nbsp; 9 22:18:49 host_2 openais[9376]: [CLM&nbsp; ] Members Left: <br>Apr&nbsp; 9 22:18:49 host_2 openais[9376]: [CLM&nbsp; ] Members Joined: <br>Apr&nbsp; 9 22:18:49 host_2 openais[9376]: [SYNC ] This node is within the primary component and will provide service. <br>Apr&nbsp; 9 22:18:49 host_2 openais[9376]: [TOTEM] entering OPERATIONAL state. <br>Apr&nbsp; 9 22:18:49 host_2 openais[9376]: [CLM&nbsp; ] got nodejoin message host_2 <br>Apr&nbsp; 9 22:18:49 host_2 openais[9376]: [CPG&nbsp; ] got joinlist message from node 2 <br>Apr&nbsp; 9 22:19:18 host_2 fenced[9579]: host_1 not a cluster member after 30 sec post_fail_delay<br>Apr&nbsp; 9 22:19:18 host_2 fenced[9579]: fencing node "host_1"<br>Apr&nbsp; 9 22:19:29 host_2 fenced[9579]: agent "fence_ilo" reports: Unable to connect/login to fencing device</p>  <p>省略部分</p>  <p>Apr&nbsp; 9 22:25:38 host_2 kernel: INFO: task gfs_glockd:10447 blocked for more than 120 seconds.<br>Apr&nbsp; 9 22:25:38 host_2 kernel: "echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs" disables this message.<br>Apr&nbsp; 9 22:25:38 host_2 kernel: gfs_glockd&nbsp;&nbsp;&nbsp; D ffff8102faa6b860&nbsp;&nbsp;&nbsp;&nbsp; 0 10447&nbsp;&nbsp;&nbsp; 231&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10455 10446 (L-T<br>LB)<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; ffff81030f4add40 0000000000000046 ffff81030f4add10 000000000000000e<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; ffff81030d57ba20 000000000000000a ffff81061df63100 ffff8102faa6b860<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; 00296e817f38ba31 0000000000002424 ffff81061df632e8 0000000400000080<br>Apr&nbsp; 9 22:25:38 host_2 kernel: Call Trace:<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff800484d0&gt;] pagevec_lookup_tag+0x1a/0x21<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff8004a66a&gt;] wait_on_page_writeback_range+0xd6/0x12e<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff800a198c&gt;] keventd_create_kthread+0x0/0xc4<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff800656ac&gt;] __down_read+0x7a/0x92<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff8859cea3&gt;] :dlm:dlm_unlock+0x37/0xcd<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff8865d802&gt;] :gfs:ail_empty_gl+0x21/0xef<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff800a198c&gt;] keventd_create_kthread+0x0/0xc4<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff88671d3e&gt;] :gfs:gdlm_do_unlock+0x3b/0x85<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff886691c6&gt;] :gfs:gfs_glock_drop_th+0x116/0x187<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff88667543&gt;] :gfs:run_queue+0x12e/0x35e<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff800a198c&gt;] keventd_create_kthread+0x0/0xc4<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff886680c8&gt;] :gfs:unlock_on_glock+0x1b/0x24<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff88667389&gt;] :gfs:gfs_reclaim_glock+0xed/0x179<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff8865d218&gt;] :gfs:gfs_glockd+0x0/0xd1<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff8865d22d&gt;] :gfs:gfs_glockd+0x15/0xd1<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff800a1ba4&gt;] autoremove_wake_function+0x0/0x2e<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff800a198c&gt;] keventd_create_kthread+0x0/0xc4<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff80032bdc&gt;] kthread+0xfe/0x132<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff8005efb1&gt;] child_rip+0xa/0x11<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff800a198c&gt;] keventd_create_kthread+0x0/0xc4<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff80032ade&gt;] kthread+0x0/0x132<br>Apr&nbsp; 9 22:25:38 host_2 kernel:&nbsp; [&lt;ffffffff8005efa7&gt;] child_rip+0x0/0x11</p>  <p>后面每隔2分钟会报一次以上的gfs相关内核信息, 略去中间部分, </p>  <p>Apr&nbsp; 9 22:27:38 host_2 kernel: INFO: task gfs_glockd:10447 blocked for more than 120 seconds.</p>  <p>略<br>Apr&nbsp; 9 22:29:38 host_2 kernel: INFO: task gfs_glockd:10447 blocked for more than 120 seconds.</p>  <p>略<br>Apr&nbsp; 9 22:31:38 host_2 kernel: INFO: task gfs_glockd:10447 blocked for more than 120 seconds.<br>Apr&nbsp; 9 22:33:38 host_2 kernel: INFO: task gfs_glockd:10447 blocked for more than 120 seconds.<br>Apr&nbsp; 9 22:35:38 host_2 kernel: INFO: task gfs_glockd:10447 blocked for more than 120 seconds.<br>Apr&nbsp; 9 22:37:38 host_2 kernel: INFO: task gfs_glockd:10447 blocked for more than 120 seconds.<br>Apr&nbsp; 9 22:39:38 host_2 kernel: INFO: task gfs_glockd:10447 blocked for more than 120 seconds.<br>Apr&nbsp; 9 22:41:38 host_2 kernel: INFO: task gfs_glockd:10447 blocked for more than 120 seconds.<br>Apr&nbsp; 9 22:43:38 host_2 kernel: INFO: task gfs_glockd:10447 blocked for more than 120 seconds.</p>  <p>&nbsp;</p>  <p>host_1的日志 : </p>  <p>Apr&nbsp; 9 22:18:22 host_1 openais[9265]: [TOTEM] The token was lost in the OPERATIONAL state. <br>Apr&nbsp; 9 22:18:22 host_1 openais[9265]: [TOTEM] Receive multicast socket recv buffer size (320000 bytes). <br>Apr&nbsp; 9 22:18:22 host_1 openais[9265]: [TOTEM] Transmit multicast socket send buffer size (320000 bytes). <br>Apr&nbsp; 9 22:18:22 host_1 openais[9265]: [TOTEM] entering GATHER state from 2. <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [TOTEM] entering GATHER state from 0. <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [TOTEM] Creating commit token because I am the rep. <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [TOTEM] Saving state aru c349 high seq received c349 <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [TOTEM] Storing new sequence id for ring 884 <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [TOTEM] entering COMMIT state. <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [TOTEM] entering RECOVERY state. <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [TOTEM] position [0] member host_1: <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [TOTEM] previous ring seq 2176 rep host_1 <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [TOTEM] aru c349 high delivered c349 received flag 1 <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [TOTEM] Did not need to originate any messages in recovery. <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [TOTEM] Sending initial ORF token <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [CLM&nbsp; ] CLM CONFIGURATION CHANGE <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [CLM&nbsp; ] New Configuration: <br>Apr&nbsp; 9 22:18:42 host_1 kernel: dlm: closing connection to node 2<br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [CLM&nbsp; ]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r(0) ip(host_1)&nbsp; <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [CLM&nbsp; ] Members Left: <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [CLM&nbsp; ]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r(0) ip(host_2)&nbsp; <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [CLM&nbsp; ] Members Joined: <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [CLM&nbsp; ] CLM CONFIGURATION CHANGE <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [CLM&nbsp; ] New Configuration: <br>Apr&nbsp; 9 22:18:42 host_1 openais[9265]: [CLM&nbsp; ]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r(0) ip(host_1)&nbsp; <br>Apr&nbsp; 9 22:18:43 host_1 openais[9265]: [CLM&nbsp; ] Members Left: <br>Apr&nbsp; 9 22:18:43 host_1 openais[9265]: [CLM&nbsp; ] Members Joined: <br>Apr&nbsp; 9 22:18:43 host_1 openais[9265]: [SYNC ] This node is within the primary component and will provide service. <br>Apr&nbsp; 9 22:18:43 host_1 openais[9265]: [TOTEM] entering OPERATIONAL state. <br>Apr&nbsp; 9 22:18:43 host_1 openais[9265]: [CLM&nbsp; ] got nodejoin message host_1 <br>Apr&nbsp; 9 22:18:43 host_1 openais[9265]: [CPG&nbsp; ] got joinlist message from node 1 <br>Apr&nbsp; 9 22:19:12 host_1 fenced[9382]: host_2 not a cluster member after 30 sec post_fail_delay<br>Apr&nbsp; 9 22:19:12 host_1 fenced[9382]: fencing node "host_2"<br>Apr&nbsp; 9 22:19:18 host_1 fenced[9382]: agent "fence_ilo" reports: Unable to connect/login to fencing device</p>  <p>&nbsp;在host_1节点未出现gfs的异常信息. 但是对应的GFS设备还是无法访问.</p>  <p>GFS文件系统无法访问原因是GFS的锁进程出了问题，我猜测导致两个节点死锁了. </p>  <p>GFS挂载信息 : 使用的是分布式锁.</p>  <p>&nbsp; SB lock proto = "lock_dlm"<br>&nbsp; SB lock table = "略"<br>&nbsp; SB ondisk format = 1309<br>&nbsp; SB multihost format = 1401<br>&nbsp; Block size = 4096<br>&nbsp; Journals = 2<br>&nbsp; Resource Groups = 188<br>&nbsp; Mounted lock proto = "lock_dlm"<br>&nbsp; Mounted lock table = "略"<br>&nbsp; Mounted host data = "jid=0:id=131073:first=0"<br>&nbsp; Journal number = 0<br>&nbsp; Lock module flags = 0<br>&nbsp; Local flocks = FALSE<br>&nbsp; Local caching = FALSE<br>&nbsp; Oopses OK = FALSE</p>  <p>&nbsp; Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total Blocks&nbsp;&nbsp; Used Blocks&nbsp;&nbsp;&nbsp; Free Blocks&nbsp;&nbsp;&nbsp; use%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>&nbsp; ------------------------------------------------------------------------<br>&nbsp; inodes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 132&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 132&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 100%<br>&nbsp; metadata&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 205254&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 51282&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 153972&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 25%<br>&nbsp; data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 98026214&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 25613842&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 72412372&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 26%</p>  <p>对于可以避免使用GFS的环境还是要尽量避免使用GFS。例如ORACLE的RAC节点如果仅仅是拿GFS来放归档文件的话就没必要了, 备份归档时可以考虑所有节点同时备份，清除操作也一样，所有节点都清除。这样可以有效的避免类似情况发生。</p>  <p>而出现本次故障的原因是HOST_2的网络异常, 并且fence的网络异常，相互都无法FENCE对方。</p><p>eth0网络异常可能是网卡驱动BUG造成的, 见<a rel="nofollow" href="http://lists.us.dell.com/pipermail/linux-poweredge/2009-December/040770.html"  >http://lists.us.dell.com/pipermail/linux-poweredge/2009-December/040770.html</a></p>  <p>由于无法获得机器的ROOT密码，只能在网络联通的情况下通过堡垒机正常登陆，最后故障排除是通过ILO重启了节点2，把节点2重新加入到集群后加载GFS，节点1的GFS文件系统重新挂载。</p></div>
	</div>
</div>
</body>
</html>