## PostgreSQL 11 1万亿 tpcb 性能测试 on 阿里云ECS + ESSD + zfs/lvm2条带   
                                                                 
### 作者                                   
digoal                                   
                                            
### 日期                                                                 
2018-09-19                                  
                                                                 
### 标签                                                                 
PostgreSQL , pgbench , tpcb , tpcc , tpch , lvm2 , zfs , 条带     
                                                                 
----                                                                 
                                                                 
## 背景         
最近的几个PostgreSQL OLTP与OLAP的测试：  
  
[《PostgreSQL 11 tpcc 测试(103万tpmC on ECS) - use sysbench-tpcc by Percona-Lab》](../201809/20180913_01.md)    
  
[《(TPC-H测试 SF=10,SF=200) PostgreSQL 11 vs 10 vs Deepgreen》](../201808/20180823_01.md)    
  
[《PostgreSQL 100亿 tpcb 性能 on ECS》](../201809/20180916_01.md)    
  
[《[未完待续] PostgreSQL on 阿里云ECS+ESSD - 1000亿 tpcb、1000W tpcc 测试》](../201809/20180917_01.md)    
  
覆盖面：  
  
1、SF=10， SF=200 TPCH  
  
2、1000W TPCC  
  
3、100亿 TPCB  
  
4、1000亿 TPCB  
  
5、1万亿 TPCB（本文要测试的）  
  
本文使用的是16块ESSD云盘，测试时，使用了两套文件系统，ZFS与EXT4，都使用到了条带。  
  
## 环境  
1、ecs，CentOS 7.4 x64  
  
2、CPU  
  
```  
lscpu  
  
Architecture:          x86_64  
CPU op-mode(s):        32-bit, 64-bit  
Byte Order:            Little Endian  
CPU(s):                64  
On-line CPU(s) list:   0-63  
Thread(s) per core:    2  
Core(s) per socket:    32  
Socket(s):             1  
NUMA node(s):          1  
Vendor ID:             GenuineIntel  
CPU family:            6  
Model:                 85  
Model name:            Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz  
Stepping:              4  
CPU MHz:               2499.996  
BogoMIPS:              4999.99  
Hypervisor vendor:     KVM  
Virtualization type:   full  
L1d cache:             32K  
L1i cache:             32K  
L2 cache:              1024K  
L3 cache:              33792K  
NUMA node0 CPU(s):     0-63  
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1  
```  
  
3、内核  
  
```  
uname -a  
Linux pg11-320tb-zfs 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux  
```  
  
4、内存  
  
```  
free -g  
              total        used        free      shared  buff/cache   available  
Mem:            503           2         500           0           0         498  
Swap:             0           0           0  
```  
  
5、时钟  
  
```  
echo tsc > /sys/devices/system/clocksource/clocksource0/current_clocksource  
```  
  
6、块设备  
  
```  
lsblk  
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT  
vda    253:0    0  200G  0 disk   
└─vda1 253:1    0  200G  0 part /  
vdb    253:16   0   20T  0 disk   
vdc    253:32   0   20T  0 disk   
vdd    253:48   0   20T  0 disk   
vde    253:64   0   20T  0 disk   
vdf    253:80   0   20T  0 disk   
vdg    253:96   0   20T  0 disk   
vdh    253:112  0   20T  0 disk   
vdi    253:128  0   20T  0 disk   
vdj    253:144  0   20T  0 disk   
vdk    253:160  0   20T  0 disk   
vdl    253:176  0   20T  0 disk   
vdm    253:192  0   20T  0 disk   
vdn    253:208  0   20T  0 disk   
vdo    253:224  0   20T  0 disk   
vdp    253:240  0   20T  0 disk   
vdq    253:256  0   20T  0 disk   
```  
  
## 配置ECS虚拟机OS参数    
1、内核参数    
    
```    
vi /etc/sysctl.conf    
    
# add by digoal.zhou        
fs.aio-max-nr = 1048576        
fs.file-max = 76724600        
        
# 可选：kernel.core_pattern = /data01/corefiles/core_%e_%u_%t_%s.%p                 
# /data01/corefiles 事先建好，权限777，如果是软链接，对应的目录修改为777        
        
kernel.sem = 4096 2147483647 2147483646 512000            
# 信号量, ipcs -l 或 -u 查看，每16个进程一组，每组信号量需要17个信号量。        
        
kernel.shmall = 107374182              
# 所有共享内存段相加大小限制 (建议内存的80%)，单位为页。        
kernel.shmmax = 274877906944           
# 最大单个共享内存段大小 (建议为内存一半), >9.2的版本已大幅降低共享内存的使用，单位为字节。        
kernel.shmmni = 819200                 
# 一共能生成多少共享内存段，每个PG数据库集群至少2个共享内存段        
        
net.core.netdev_max_backlog = 10000        
net.core.rmem_default = 262144               
# The default setting of the socket receive buffer in bytes.        
net.core.rmem_max = 4194304                  
# The maximum receive socket buffer size in bytes        
net.core.wmem_default = 262144               
# The default setting (in bytes) of the socket send buffer.        
net.core.wmem_max = 4194304                  
# The maximum send socket buffer size in bytes.        
net.core.somaxconn = 4096        
net.ipv4.tcp_max_syn_backlog = 4096        
net.ipv4.tcp_keepalive_intvl = 20        
net.ipv4.tcp_keepalive_probes = 3        
net.ipv4.tcp_keepalive_time = 60        
net.ipv4.tcp_mem = 8388608 12582912 16777216        
net.ipv4.tcp_fin_timeout = 5        
net.ipv4.tcp_synack_retries = 2        
net.ipv4.tcp_syncookies = 1            
# 开启SYN Cookies。当出现SYN等待队列溢出时，启用cookie来处理，可防范少量的SYN攻击        
net.ipv4.tcp_timestamps = 1            
# 减少time_wait        
net.ipv4.tcp_tw_recycle = 0            
# 如果=1则开启TCP连接中TIME-WAIT套接字的快速回收，但是NAT环境可能导致连接失败，建议服务端关闭它        
net.ipv4.tcp_tw_reuse = 1              
# 开启重用。允许将TIME-WAIT套接字重新用于新的TCP连接        
net.ipv4.tcp_max_tw_buckets = 262144        
net.ipv4.tcp_rmem = 8192 87380 16777216        
net.ipv4.tcp_wmem = 8192 65536 16777216        
        
net.nf_conntrack_max = 1200000        
net.netfilter.nf_conntrack_max = 1200000        
        
vm.dirty_background_bytes = 409600000               
#  系统脏页到达这个值，系统后台刷脏页调度进程 pdflush（或其他） 自动将(dirty_expire_centisecs/100）秒前的脏页刷到磁盘        
#  默认为10%，大内存机器建议调整为直接指定多少字节        
        
vm.dirty_expire_centisecs = 3000                     
#  比这个值老的脏页，将被刷到磁盘。3000表示30秒。        
vm.dirty_ratio = 95                                  
#  如果系统进程刷脏页太慢，使得系统脏页超过内存 95 % 时，则用户进程如果有写磁盘的操作（如fsync, fdatasync等调用），则需要主动把系统脏页刷出。        
#  有效防止用户进程刷脏页，在单机多实例，并且使用CGROUP限制单实例IOPS的情况下非常有效。          
        
vm.dirty_writeback_centisecs = 100                    
#  pdflush（或其他）后台刷脏页进程的唤醒间隔， 100表示1秒。        
        
vm.swappiness = 0        
#  不使用交换分区        
        
vm.mmap_min_addr = 65536        
vm.overcommit_memory = 0             
#  在分配内存时，允许少量over malloc, 如果设置为 1, 则认为总是有足够的内存，内存较少的测试环境可以使用 1 .          
        
vm.overcommit_ratio = 90             
#  当overcommit_memory = 2 时，用于参与计算允许指派的内存大小。        
vm.swappiness = 0                    
#  关闭交换分区        
vm.zone_reclaim_mode = 0             
# 禁用 numa, 或者在vmlinux中禁止.         
net.ipv4.ip_local_port_range = 40000 65535            
# 本地自动分配的TCP, UDP端口号范围        
fs.nr_open=20480000        
# 单个进程允许打开的文件句柄上限        
        
# 以下参数请注意        
# vm.extra_free_kbytes = 4096000        
# vm.min_free_kbytes = 2097152        
# 如果是小内存机器，以上两个值不建议设置        
# vm.nr_hugepages = 66536            
#  建议shared buffer设置超过64GB时 使用大页，页大小 /proc/meminfo Hugepagesize        
# vm.lowmem_reserve_ratio = 1 1 1        
# 对于内存大于64G时，建议设置，否则建议默认值 256 256 32    
```    
    
2、资源限制    
    
```    
vi /etc/security/limits.conf    
    
# nofile超过1048576的话，一定要先将sysctl的fs.nr_open设置为更大的值，并生效后才能继续设置nofile.        
        
* soft    nofile  1024000        
* hard    nofile  1024000        
* soft    nproc   unlimited        
* hard    nproc   unlimited        
* soft    core    unlimited        
* hard    core    unlimited        
* soft    memlock unlimited        
* hard    memlock unlimited       
```    
    
3、关闭透明大页，使用精准时钟（可选）    
    
```    
vi /etc/rc.local    
    
touch /var/lock/subsys/local    
         
if test -f /sys/kernel/mm/transparent_hugepage/enabled; then        
   echo never > /sys/kernel/mm/transparent_hugepage/enabled        
fi      
  
# 时钟  
echo tsc > /sys/devices/system/clocksource/clocksource0/current_clocksource  
  
su - postgres -c "pg_ctl start"      
```    
  
  
## 部署 PostgreSQL 11  
  
```  
rpm -ivh https://download.postgresql.org/pub/repos/yum/11/redhat/rhel-7-x86_64/pgdg-centos11-11-2.noarch.rpm    
    
rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm     
    
yum -y install coreutils glib2 lrzsz dstat sysstat e4fsprogs xfsprogs ntp readline-devel zlib-devel openssl-devel pam-devel libxml2-devel libxslt-devel python-devel tcl-devel gcc gcc-c++ make smartmontools flex bison perl-devel perl-ExtUtils* openldap-devel jadetex  openjade bzip2 git iotop lvm2 perf      
    
yum install -y postgresql11*    
```  
  
# 块设备部署策略1 - zfs  
  
## zfsonlinux  
  
[《[未完待续] PostgreSQL on ECS 高效率持续备份设计 - By ZFS on Linux》](../201711/20171104_02.md)    
  
[《PostgreSQL OLTP on ZFS 性能优化》](../201512/20151229_01.md)    
  
1、zfs yum配置  
  
```  
yum install -y http://download.zfsonlinux.org/epel/zfs-release.el7_4.noarch.rpm  
```  
  
2、当前内核对应的kernel-devel    
  
http://vault.centos.org   
  
```  
uname -a  
Linux pg11-320tb-zfs 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux  
```  
  
```  
rpm -ivh http://vault.centos.org/7.4.1708/updates/x86_64/Packages/kernel-devel-3.10.0-693.2.2.el7.x86_64.rpm  
```  
  
3、安装zfs  
  
```  
yum install -y zfs   
```  
  
查看日志，是否有报错，正常情况下没有报错   
  
  
测试是否可用  
  
```  
modprobe zfs  
  
zfs get -o all  
```  
  
系统启动将自动加载zfs  
  
## 创建zpool  
  
essd底层三副本，无需再使用zfs的RAID功能。  
  
```  
parted -a optimal -s /dev/vdb mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdc mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdd mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vde mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdf mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdg mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdh mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdi mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdj mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdk mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdl mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdm mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdn mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdo mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdp mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdq mklabel gpt mkpart primary 1MiB 100%FREE  
```  
  
```  
zpool create zp1 -f -o ashift=13 vdb1 vdc1 vdd1 vde1 vdf1 vdg1 vdh1 vdi1 vdj1 vdk1 vdl1 vdm1 vdn1 vdo1 vdp1 vdq1  
```  
  
```  
zpool get all zp1  
NAME  PROPERTY                       VALUE                          SOURCE  
zp1   size                           318T                           -  
zp1   capacity                       0%                             -  
zp1   altroot                        -                              default  
zp1   health                         ONLINE                         -  
zp1   guid                           12407519490197584982           -  
zp1   version                        -                              default  
zp1   bootfs                         -                              default  
zp1   delegation                     on                             default  
zp1   autoreplace                    off                            default  
zp1   cachefile                      -                              default  
zp1   failmode                       wait                           default  
zp1   listsnapshots                  off                            default  
zp1   autoexpand                     off                            default  
zp1   dedupditto                     0                              default  
zp1   dedupratio                     1.00x                          -  
zp1   free                           318T                           -  
zp1   allocated                      960K                           -  
zp1   readonly                       off                            -  
zp1   ashift                         13                             local  
zp1   comment                        -                              default  
zp1   expandsize                     -                              -  
zp1   freeing                        0                              -  
zp1   fragmentation                  0%                             -  
zp1   leaked                         0                              -  
zp1   multihost                      off                            default  
zp1   feature@async_destroy          enabled                        local  
zp1   feature@empty_bpobj            enabled                        local  
zp1   feature@lz4_compress           active                         local  
zp1   feature@multi_vdev_crash_dump  enabled                        local  
zp1   feature@spacemap_histogram     active                         local  
zp1   feature@enabled_txg            active                         local  
zp1   feature@hole_birth             active                         local  
zp1   feature@extensible_dataset     active                         local  
zp1   feature@embedded_data          active                         local  
zp1   feature@bookmarks              enabled                        local  
zp1   feature@filesystem_limits      enabled                        local  
zp1   feature@large_blocks           enabled                        local  
zp1   feature@large_dnode            enabled                        local  
zp1   feature@sha512                 enabled                        local  
zp1   feature@skein                  enabled                        local  
zp1   feature@edonr                  enabled                        local  
zp1   feature@userobj_accounting     active                         local  
```  
  
## 创建zfs  
  
```  
zfs create -o mountpoint=/data01 -o recordsize=8K -o atime=off -o primarycache=metadata -o logbias=throughput -o secondarycache=none zp1/data01  
  
  
zfs set canmount=off zp1  
```  
  
优化两个参数  
  
```  
cd /sys/module/zfs/parameters/  
echo 1 > zfs_prefetch_disable  
echo 15 > zfs_arc_shrink_shift   
```  
  
查看当前参数  
  
```  
zfs get all zp1/data01  
NAME        PROPERTY              VALUE                  SOURCE  
zp1/data01  type                  filesystem             -  
zp1/data01  creation              Wed Sep 19 10:26 2018  -  
zp1/data01  used                  192K                   -  
zp1/data01  available             308T                   -  
zp1/data01  referenced            192K                   -  
zp1/data01  compressratio         1.00x                  -  
zp1/data01  mounted               yes                    -  
zp1/data01  quota                 none                   default  
zp1/data01  reservation           none                   default  
zp1/data01  recordsize            8K                     local  
zp1/data01  mountpoint            /data01                local  
zp1/data01  sharenfs              off                    default  
zp1/data01  checksum              on                     default  
zp1/data01  compression           off                    default  
zp1/data01  atime                 off                    local  
zp1/data01  devices               on                     default  
zp1/data01  exec                  on                     default  
zp1/data01  setuid                on                     default  
zp1/data01  readonly              off                    default  
zp1/data01  zoned                 off                    default  
zp1/data01  snapdir               hidden                 default  
zp1/data01  aclinherit            restricted             default  
zp1/data01  createtxg             81                     -  
zp1/data01  canmount              on                     default  
zp1/data01  xattr                 on                     default  
zp1/data01  copies                1                      default  
zp1/data01  version               5                      -  
zp1/data01  utf8only              off                    -  
zp1/data01  normalization         none                   -  
zp1/data01  casesensitivity       sensitive              -  
zp1/data01  vscan                 off                    default  
zp1/data01  nbmand                off                    default  
zp1/data01  sharesmb              off                    default  
zp1/data01  refquota              none                   default  
zp1/data01  refreservation        none                   default  
zp1/data01  guid                  3373300831209850945    -  
zp1/data01  primarycache          metadata               local  
zp1/data01  secondarycache        none                   default  
zp1/data01  usedbysnapshots       0B                     -  
zp1/data01  usedbydataset         192K                   -  
zp1/data01  usedbychildren        0B                     -  
zp1/data01  usedbyrefreservation  0B                     -  
zp1/data01  logbias               throughput             local  
zp1/data01  dedup                 off                    default  
zp1/data01  mlslabel              none                   default  
zp1/data01  sync                  standard               default  
zp1/data01  dnodesize             legacy                 default  
zp1/data01  refcompressratio      1.00x                  -  
zp1/data01  written               192K                   -  
zp1/data01  logicalused           76K                    -  
zp1/data01  logicalreferenced     76K                    -  
zp1/data01  volmode               default                default  
zp1/data01  filesystem_limit      none                   default  
zp1/data01  snapshot_limit        none                   default  
zp1/data01  filesystem_count      none                   default  
zp1/data01  snapshot_count        none                   default  
zp1/data01  snapdev               hidden                 default  
zp1/data01  acltype               off                    default  
zp1/data01  context               none                   default  
zp1/data01  fscontext             none                   default  
zp1/data01  defcontext            none                   default  
zp1/data01  rootcontext           none                   default  
zp1/data01  relatime              off                    default  
zp1/data01  redundant_metadata    all                    default  
zp1/data01  overlay               off                    default  
```  
  
## 初始化数据库  
  
  
1、目录  
  
```  
mkdir /data01/pg11  
  
chown postgres:postgres /data01/pg11  
```  
  
2、环境变量  
  
```  
su - postgres  
  
vi .bash_profile  
  
export PS1="$USER@`/bin/hostname -s`-> "        
export PGPORT=1921        
export PGDATA=/data01/pg11/pg_root$PGPORT        
export LANG=en_US.utf8        
export PGHOME=/usr/pgsql-11      
export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib:$LD_LIBRARY_PATH        
export DATE=`date +"%Y%m%d%H%M"`      
export PATH=$PGHOME/bin:$PATH:.        
export MANPATH=$PGHOME/share/man:$MANPATH        
export PGHOST=$PGDATA        
export PGUSER=postgres        
export PGDATABASE=postgres        
alias rm='rm -i'        
alias ll='ls -lh'        
unalias vi   
```  
  
3、初始化  
  
```  
initdb -D $PGDATA -U postgres -E SQL_ASCII --lc-collate=C --lc-ctype=en_US.utf8 --wal-segsize=1024   
```  
  
## huge page配置  
zfs 可以绕过文件系统cache，所以建议一配置较大postgresql shared buffer，并使用huge page  
  
[《PostgreSQL Huge Page 使用建议 - 大内存主机、实例注意》](../201803/20180325_02.md)    
  
```  
300GB/2MB=153600  
```  
  
```  
sysctl -w vm.nr_hugepages=159600     
  
echo "vm.nr_hugepages=159600" >> /etc/sysctl.conf  
```  
  
## postgresql.auto.conf  
  
```  
listen_addresses = '0.0.0.0'    
port = 1921    
max_connections = 2000    
superuser_reserved_connections = 3    
unix_socket_directories = '., /var/run/postgresql, /tmp'    
tcp_keepalives_idle = 60    
tcp_keepalives_interval = 10    
tcp_keepalives_count = 10    
huge_pages = on  # 使用huge page  
shared_buffers = 300GB    
max_prepared_transactions = 2000    
work_mem = 32MB    
maintenance_work_mem = 2GB    
dynamic_shared_memory_type = posix    
vacuum_cost_delay = 0    
bgwriter_delay = 10ms    
bgwriter_lru_maxpages = 1000    
bgwriter_lru_multiplier = 10.0    
effective_io_concurrency = 0    
max_worker_processes = 128    
max_parallel_workers = 64    
max_parallel_maintenance_workers = 64   
max_parallel_workers_per_gather = 0    
parallel_leader_participation = on    
min_parallel_table_scan_size=0  
min_parallel_index_scan_size=0  
parallel_setup_cost=0  
parallel_tuple_cost=0  
wal_level = minimal      
synchronous_commit = off    
full_page_writes=off       #  zfs内置了checksum，cow. 关闭 fpw . 如果BLOCKDEV能保证8K原子写时，也可以关闭   
wal_writer_delay = 10ms    
checkpoint_timeout = 30min    
max_wal_size = 600GB    
min_wal_size = 150GB    
checkpoint_completion_target = 0.1    
max_wal_senders = 0    
effective_cache_size = 200GB    
log_destination = 'csvlog'    
logging_collector = on    
log_directory = 'log'    
log_filename = 'postgresql-%a.log'    
log_truncate_on_rotation = on    
log_rotation_age = 1d    
log_rotation_size = 0    
log_checkpoints = on     
log_connections = on    
log_disconnections = on    
log_error_verbosity = verbose     
log_line_prefix = '%m [%p] '    
log_timezone = 'PRC'    
log_autovacuum_min_duration = 0    
autovacuum_max_workers = 16    
autovacuum_freeze_max_age = 1200000000    
autovacuum_multixact_freeze_max_age = 1400000000    
autovacuum_vacuum_cost_delay = 0ms    
vacuum_freeze_table_age = 1150000000    
vacuum_multixact_freeze_table_age = 1150000000    
datestyle = 'iso, mdy'    
timezone = 'PRC'    
lc_messages = 'C'    
lc_monetary = 'C'    
lc_numeric = 'C'    
lc_time = 'C'    
default_text_search_config = 'pg_catalog.english'    
jit = off    
cpu_tuple_cost=0.00018884145574257426      
cpu_index_tuple_cost = 0.00433497085216479990      
cpu_operator_cost = 0.00216748542608239995      
seq_page_cost=0.014329      
random_page_cost = 0.016   
  
```  
  
restart 数据库  
  
  
```  
pg_ctl restart -m fast  
```  
  
## zfs on linux 性能问题  
本例测试时，可能磁盘太大，计算得出的配置有地方也许不对。  
  
调用fsync接口特别慢。(vacuum 时就可以看出问题 ```pgbench -i -s 1000``` 可复现)  
  
```  
sync=standard|always|disabled  
  Controls the behavior of synchronous requests (e.g. fsync, O_DSYNC).    
    
  standard is the POSIX specified behavior of ensuring all synchronous requests are written to stable   
  storage and all devices are flushed to ensure data is  
  not cached by device controllers (this is the default).    
    
  always causes every file system transaction to be written and flushed before its system call returns.    
  This has a large performance penalty.    
    
  disabled disables synchronous requests.  File system transactions are only committed to stable storage periodically.    
  This option will give the highest performance.    
  However, it is very dangerous as ZFS would be ignoring the synchronous transaction demands of applications such as databases or NFS.    
  Administrators should only use this option when the risks are understood.  
```  
  
# 块设备部署策略1 - lvm2 , ext4  
  
1、停库，消除zfs  
  
```  
pg_ctl stop -m immediate  
  
zfs destroy zp1/data01  
zpool destroy zp1  
```  
  
2、清理块设备头信息  
  
```  
wipefs -f -a /dev/vd[b-q]  
  
  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdb  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdc  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdd  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vde  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdf  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdg  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdh  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdi  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdj  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdk  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdl  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdm  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdn  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdo  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdp  
dd bs=1024 count=1000 if=/dev/zero of=/dev/vdq  
  
  
parted -a optimal -s /dev/vdb mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdc mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdd mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vde mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdf mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdg mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdh mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdi mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdj mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdk mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdl mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdm mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdn mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdo mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdp mklabel gpt mkpart primary 1MiB 100%FREE  
parted -a optimal -s /dev/vdq mklabel gpt mkpart primary 1MiB 100%FREE  
  
  
wipefs -f -a /dev/vd[b-q]1  
```  
  
3、创建PV  
  
```  
pvcreate /dev/vd[b-q]1  
```  
  
4、创建vg  
  
```  
vgcreate -A y -s 128M vgdata01 /dev/vd[b-q]1    
```  
  
5、创建逻辑卷，配置条带  
  
```  
lvcreate -A y -i 16 -I 8 -L 4TiB -n lv03 vgdata01    
lvcreate -A y -i 16 -I 8 -L 220TiB -n lv01 vgdata01    
lvcreate -A y -i 16 -I 8 -l 100%FREE -n lv02 vgdata01    
```  
  
6、查看  
  
```  
[root@pg11-320tb-zfs ~]# pvs  
  PV         VG       Fmt  Attr PSize   PFree  
  /dev/vdb   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdc   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdd   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vde   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdf   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdg   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdh   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdi   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdj   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdk   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdl   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdm   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdn   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdo   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdp   vgdata01 lvm2 a--  <20.00t    0   
  /dev/vdq   vgdata01 lvm2 a--  <20.00t    0   
[root@pg11-320tb-zfs ~]# vgs  
  VG       #PV #LV #SN Attr   VSize    VFree  
  vgdata01  16   3   0 wz--n- <320.00t    0   
[root@pg11-320tb-zfs ~]# lvs  
  LV   VG       Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert  
  lv01 vgdata01 -wi-a----- 220.00t                                                      
  lv02 vgdata01 -wi-a----- <96.00t                                                      
  lv03 vgdata01 -wi-a-----   4.00t   
```  
  
7、创建ext4文件系统，配置条带  
  
```  
mkfs.ext4 /dev/mapper/vgdata01-lv01 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stride=2,stripe_width=32 -b 4096 -T largefile -L lv01    
mkfs.ext4 /dev/mapper/vgdata01-lv02 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stride=2,stripe_width=32 -b 4096 -T largefile -L lv02    
mkfs.ext4 /dev/mapper/vgdata01-lv03 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stride=2,stripe_width=32 -b 4096 -T largefile -L lv03    
```  
  
8、配置挂载  
  
```  
vi /etc/fstab     
LABEL=lv01 /data01 ext4 defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback 0 0  
LABEL=lv02 /data02 ext4 defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback 0 0  
LABEL=lv03 /data03 ext4 defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback 0 0  
    
    
mkdir /data01    
mkdir /data02    
mkdir /data03    
    
    
mount -a    
```  
  
```  
[root@pg11-320tb-zfs ~]# df -h  
Filesystem                 Size  Used Avail Use% Mounted on  
/dev/vda1                  197G  2.1G  185G   2% /  
devtmpfs                   252G     0  252G   0% /dev  
tmpfs                      252G     0  252G   0% /dev/shm  
tmpfs                      252G  596K  252G   1% /run  
tmpfs                      252G     0  252G   0% /sys/fs/cgroup  
tmpfs                       51G     0   51G   0% /run/user/0  
/dev/mapper/vgdata01-lv01  220T   20K  220T   1% /data01  
/dev/mapper/vgdata01-lv02   96T   20K   96T   1% /data02  
/dev/mapper/vgdata01-lv03  4.0T   89M  4.0T   1% /data03  
```  
  
9、创建数据库数据、表空间、WAL日志目录  
  
```  
[root@pg11-320tb-zfs ~]# mkdir /data01/pg11  
[root@pg11-320tb-zfs ~]# mkdir /data02/pg11  
[root@pg11-320tb-zfs ~]# mkdir /data03/pg11  
[root@pg11-320tb-zfs ~]# chown postgres:postgres /data0*/pg11  
```  
  
10、配置环境变量  
  
```  
su - postgres  
  
export PS1="$USER@`/bin/hostname -s`-> "  
export PGPORT=1921  
export PGDATA=/data01/pg11/pg_root$PGPORT  
export LANG=en_US.utf8  
export PGHOME=/usr/pgsql-11  
export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib:$LD_LIBRARY_PATH  
export DATE=`date +"%Y%m%d%H%M"`  
export PATH=$PGHOME/bin:$PATH:.  
export MANPATH=$PGHOME/share/man:$MANPATH  
export PGHOST=$PGDATA  
export PGUSER=postgres  
export PGDATABASE=postgres  
alias rm='rm -i'  
alias ll='ls -lh'  
unalias vi  
```  
  
11、初始化数据库  
  
```  
initdb -D $PGDATA -X /data03/pg11/pg_wal1921 -U postgres -E SQL_ASCII --lc-collate=C --lc-ctype=en_US.utf8 --wal-segsize=1024   
```  
  
12、创建表空间  
  
```  
mkdir /data01/pg11/tbs1  
mkdir /data02/pg11/tbs2  
  
  
create tablespace tbs1 location '/data01/pg11/tbs1';  
create tablespace tbs2 location '/data02/pg11/tbs2';  
```  
  
  
# 1万亿 tpcb test  
  
## 初始化数据  
nohup pgbench -i -s 10000000 -I dtg -n --tablespace=tbs1 >./init.log 2>&1 &  
  
## 创建索引  
加载初始化数据结束后，创建索引  
  
1、修改并行度  
  
```  
psql  
  
analyze;  
alter table pgbench_accounts set (parallel_workers=64);  
alter table pgbench_tellers set (parallel_workers=64);  
alter table pgbench_branches set (parallel_workers=64);  
```  
  
2、创建索引  
  
```  
nohup pgbench -i -s 10000000 -I p -n --index-tablespace=tbs2 >./init_pkey.log 2>&1 &  
```  
  
  
## 1万亿 tpcb 只读测试 - 所有数据活跃  
  
  
## 1万亿 tpcb 读写测试 - 所有数据活跃  
  
  
## 1万亿 tpcb 只读测试 - 10亿数据活跃  
  
  
## 1万亿 tpcb 读写测试 - 10亿数据活跃  
  
  
  
# 附录 - pgbench_accounts 分区, 并行加载测试数据, 动态查询  
## 1万亿单表，会带来什么问题？  
1、单表125TB，创建索引耗时增加。PG 11 引入并行创建索引，解决。  
  
2、单表125TB，垃圾回收时间拉长。PG 12 使用zheap引擎彻底杜绝。  
  
3、单表125TB，FREEZE耗时拉长，甚至可能无法在20亿个事务内完成。PG未来版本，使用超过32位的XID，彻底解决。  
  
4、单表125TB，必须放在单个目录下，可能导致文件系统上限（INODE，容量等上限）。   
  
5、单表125TB，要做一些数据清理时不方便，如果有时间维度老化概念，用分区表，可以更好的管理冷热数据  
  
## pgbench转换为分区表。  
1、建议使用pg_pathman，性能损失低。内置分区功能，目前还有性能问题。  
  
[《PostgreSQL 9.x, 10, 11 hash分区表 用法举例》](../201805/20180524_05.md)    
  
[《PostgreSQL 10 内置分区 vs pg_pathman perf profiling》](../201710/20171015_01.md)    
  
[《分区表锁粒度差异 - pg_pathman VS native partition table》](../201802/20180206_01.md)    
  
[《PostgreSQL 查询涉及分区表过多导致的性能问题 - 性能诊断与优化(大量BIND, spin lock, SLEEP进程)》](../201801/20180124_01.md)    
  
使用内部分区，建议使用动态SQL，避免BIND问题。  
  
## 分区demo  
[《PostgreSQL pgbench tpcb 数据生成与SQL部分源码解读》](../201809/20180919_03.md)    
  
[《PostgreSQL pgbench tpcb 海量数据库测试 - 分区表测试优化》](../201809/20180919_04.md)    
  
### 装载数据  
1、表  
  
```  
pgbench -i -I dt --tablespace=tbs1 -s 10000000  
```  
  
2、分区  
  
```  
create table p (like pgbench_accounts) partition by RANGE ( aid ) tablespace tbs1;  
  
do language plpgsql $$                                                           
declare  
  i_rows_perpartition int8 := 244140625;  
begin  
  for i in 0..4096 loop  
    execute format ('create table pgbench_accounts%s partition of p for values from (%s) to (%s) tablespace tbs1', i, i*i_rows_perpartition, (i+1)*i_rows_perpartition);  
  end loop;  
end;  
$$;  
  
drop table pgbench_accounts;  
  
alter table p rename to pgbench_accounts;  
  
-- alter table pgbench_accounts add constraint pk_pgbench_accounts_aid primary key (aid) using index tablespace tbs2;  
```  
  
3、加载任务  
  
```  
drop table task;  
create table task(id int primary key);  
insert into task select i from generate_series(0,4095) t(i);  
```  
  
4、初始化记录  
  
```  
create table init_accounts(aid int8);  
insert into init_accounts select generate_series(0,244140624);  
```  
  
5、并行状态UDF  
  
```  
create or replace function tpcb_init_accounts() returns void as $$  
declare  
  v_id int;  
begin  
  with tmp as (select * from task limit 1 for update skip locked),  
    tmp1 as (delete from task using tmp where task.id=tmp.id)  
    select id into v_id from tmp;  
    
  if found then  
    execute format ('insert into pgbench_accounts%s select aid+%s*244140625::int8, ((aid+%s*244140625::int8)-1)/100000 + 1, 0 from init_accounts on conflict do nothing', v_id, v_id, v_id);  
  end if;  
end;  
$$ language plpgsql strict;  
```  
  
6、并行装载数据  
  
```  
vi test.sql  
select tpcb_init_accounts();  
  
nohup pgbench -M prepared -n -r -f ./test.sql -c 64 -j 64 -t 100 >./init.log 2>&1 &  
```  
  
### 初始化索引  
1、任务表  
  
```  
drop table task;  
create table task(id int primary key);  
insert into task select i from generate_series(0,4095) t(i);  
```  
  
2、并行创建索引UDF  
  
```  
create or replace function tpcb_init_accounts_pkey() returns void as $$  
declare  
  v_id int;  
begin  
  with tmp as (select * from task limit 1 for update skip locked),  
    tmp1 as (delete from task using tmp where task.id=tmp.id)  
    select id into v_id from tmp;  
    
  if found then  
    execute format ('analyze pgbench_accounts%s', v_id);  
    execute format ('alter table pgbench_accounts%s add constraint pk_pgbench_accounts%s_aid primary key (aid) using index tablespace tbs2', v_id, v_id);  
  end if;  
end;  
$$ language plpgsql strict;  
```  
  
3、并行创建索引  
  
```  
vi test.sql  
select tpcb_init_accounts_pkey();  
  
nohup pgbench -M prepared -n -r -f ./test.sql -c 64 -j 64 -t 100 >./init.log 2>&1 &  
```  
  
## 小结  
  
  
  
  
## 参考  
[《PostgreSQL 11 tpcc 测试(103万tpmC on ECS) - use sysbench-tpcc by Percona-Lab》](../201809/20180913_01.md)    
  
[《(TPC-H测试 SF=10,SF=200) PostgreSQL 11 vs 10 vs Deepgreen》](../201808/20180823_01.md)    
  
[《PostgreSQL 100亿 tpcb 性能 on ECS》](../201809/20180916_01.md)    
  
[《[未完待续] PostgreSQL on 阿里云ECS+ESSD - 1000亿 tpcb、1000W tpcc 测试》](../201809/20180917_01.md)    
  
  
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"  ><img src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"  alt="Flag Counter"  border="0"  ></a>  
  
  
## [digoal's 大量PostgreSQL文章入口](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
